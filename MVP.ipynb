{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MVP.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPYPs+C9nJMHX3geVIftoJQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shadyvhs/NBA2020/blob/master/MVP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHUnxKFSppeL",
        "colab_type": "text"
      },
      "source": [
        "The aim of this notebook is to use NBA 2K data and regular season player statistics from the previous two years to determine who the best player in the league is in the current 2020 season while simultaneously exploring some interesting statistcs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjvnzTZaO3f0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSAT1H2UPBMW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1=pd.read_csv('nba18.csv', index_col=0)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xe7KKcOAPGeE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df2=pd.read_csv('nba19.csv',index_col=0)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsmO1fCuPMD5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "8e780fca-efd9-4b2f-b3d6-e44d4cfae591"
      },
      "source": [
        "df1.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Team</th>\n",
              "      <th>Pos</th>\n",
              "      <th>Age</th>\n",
              "      <th>GP</th>\n",
              "      <th>MPG</th>\n",
              "      <th>Minpercent</th>\n",
              "      <th>USG</th>\n",
              "      <th>TOR</th>\n",
              "      <th>FTA</th>\n",
              "      <th>FTpercent</th>\n",
              "      <th>2PA</th>\n",
              "      <th>2Ppercent</th>\n",
              "      <th>3PA</th>\n",
              "      <th>3Ppercent</th>\n",
              "      <th>Effective Shooting percent</th>\n",
              "      <th>True shooting percent</th>\n",
              "      <th>PPG</th>\n",
              "      <th>RPG</th>\n",
              "      <th>TRB</th>\n",
              "      <th>APG</th>\n",
              "      <th>ASTpercent</th>\n",
              "      <th>SPG</th>\n",
              "      <th>BPG</th>\n",
              "      <th>TOPGTurnovers</th>\n",
              "      <th>Versatility Index</th>\n",
              "      <th>ORTG</th>\n",
              "      <th>DRTG</th>\n",
              "      <th>Rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Aaron Brooks</td>\n",
              "      <td>Min</td>\n",
              "      <td>PG</td>\n",
              "      <td>33.0</td>\n",
              "      <td>32</td>\n",
              "      <td>5.9</td>\n",
              "      <td>12.3</td>\n",
              "      <td>19.8</td>\n",
              "      <td>0.130</td>\n",
              "      <td>11</td>\n",
              "      <td>0.727</td>\n",
              "      <td>38</td>\n",
              "      <td>0.447</td>\n",
              "      <td>31</td>\n",
              "      <td>0.355</td>\n",
              "      <td>0.486</td>\n",
              "      <td>0.508</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0.5</td>\n",
              "      <td>5.2</td>\n",
              "      <td>0.6</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.34</td>\n",
              "      <td>6.0</td>\n",
              "      <td>103.1</td>\n",
              "      <td>100.4</td>\n",
              "      <td>73.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Aaron Gordon</td>\n",
              "      <td>Orl</td>\n",
              "      <td>SF</td>\n",
              "      <td>22.0</td>\n",
              "      <td>58</td>\n",
              "      <td>32.9</td>\n",
              "      <td>68.6</td>\n",
              "      <td>24.7</td>\n",
              "      <td>0.100</td>\n",
              "      <td>225</td>\n",
              "      <td>0.698</td>\n",
              "      <td>523</td>\n",
              "      <td>0.497</td>\n",
              "      <td>342</td>\n",
              "      <td>0.336</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.530</td>\n",
              "      <td>17.6</td>\n",
              "      <td>7.9</td>\n",
              "      <td>13.2</td>\n",
              "      <td>2.3</td>\n",
              "      <td>11.7</td>\n",
              "      <td>1.02</td>\n",
              "      <td>0.78</td>\n",
              "      <td>1.84</td>\n",
              "      <td>8.3</td>\n",
              "      <td>103.6</td>\n",
              "      <td>106.2</td>\n",
              "      <td>78.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Aaron Harrison</td>\n",
              "      <td>Dal</td>\n",
              "      <td>SG</td>\n",
              "      <td>23.0</td>\n",
              "      <td>9</td>\n",
              "      <td>25.9</td>\n",
              "      <td>54.0</td>\n",
              "      <td>15.5</td>\n",
              "      <td>0.038</td>\n",
              "      <td>17</td>\n",
              "      <td>0.765</td>\n",
              "      <td>26</td>\n",
              "      <td>0.385</td>\n",
              "      <td>43</td>\n",
              "      <td>0.209</td>\n",
              "      <td>0.341</td>\n",
              "      <td>0.392</td>\n",
              "      <td>6.7</td>\n",
              "      <td>2.7</td>\n",
              "      <td>5.7</td>\n",
              "      <td>1.2</td>\n",
              "      <td>6.7</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.33</td>\n",
              "      <td>4.3</td>\n",
              "      <td>87.9</td>\n",
              "      <td>100.8</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Aaron Jackson</td>\n",
              "      <td>Hou</td>\n",
              "      <td>F</td>\n",
              "      <td>31.0</td>\n",
              "      <td>1</td>\n",
              "      <td>34.5</td>\n",
              "      <td>71.9</td>\n",
              "      <td>13.9</td>\n",
              "      <td>0.092</td>\n",
              "      <td>2</td>\n",
              "      <td>0.500</td>\n",
              "      <td>5</td>\n",
              "      <td>0.400</td>\n",
              "      <td>4</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.389</td>\n",
              "      <td>0.405</td>\n",
              "      <td>8.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.9</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>3.3</td>\n",
              "      <td>91.3</td>\n",
              "      <td>101.6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Abdel Nader</td>\n",
              "      <td>Bos</td>\n",
              "      <td>SF</td>\n",
              "      <td>24.0</td>\n",
              "      <td>48</td>\n",
              "      <td>10.9</td>\n",
              "      <td>22.6</td>\n",
              "      <td>17.1</td>\n",
              "      <td>0.170</td>\n",
              "      <td>39</td>\n",
              "      <td>0.590</td>\n",
              "      <td>84</td>\n",
              "      <td>0.321</td>\n",
              "      <td>65</td>\n",
              "      <td>0.354</td>\n",
              "      <td>0.413</td>\n",
              "      <td>0.439</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>7.4</td>\n",
              "      <td>0.5</td>\n",
              "      <td>7.2</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.71</td>\n",
              "      <td>4.8</td>\n",
              "      <td>83.1</td>\n",
              "      <td>99.1</td>\n",
              "      <td>68.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Name Team Pos   Age  ...  Versatility Index   ORTG   DRTG  Rating\n",
              "0    Aaron Brooks  Min  PG  33.0  ...                6.0  103.1  100.4    73.0\n",
              "1    Aaron Gordon  Orl  SF  22.0  ...                8.3  103.6  106.2    78.0\n",
              "2  Aaron Harrison  Dal  SG  23.0  ...                4.3   87.9  100.8     NaN\n",
              "3   Aaron Jackson  Hou   F  31.0  ...                3.3   91.3  101.6     NaN\n",
              "4     Abdel Nader  Bos  SF  24.0  ...                4.8   83.1   99.1    68.0\n",
              "\n",
              "[5 rows x 29 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpE0NEl1PO2u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "713640c2-4d72-439d-a5ed-71d2baddfcc8"
      },
      "source": [
        "df2.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Team</th>\n",
              "      <th>Pos</th>\n",
              "      <th>Age</th>\n",
              "      <th>GP</th>\n",
              "      <th>MPG</th>\n",
              "      <th>Minpercent</th>\n",
              "      <th>USG</th>\n",
              "      <th>TOR</th>\n",
              "      <th>FTA</th>\n",
              "      <th>FTpercent</th>\n",
              "      <th>2PA</th>\n",
              "      <th>2Ppercent</th>\n",
              "      <th>3PA</th>\n",
              "      <th>3Ppercent</th>\n",
              "      <th>Effective Shooting percent</th>\n",
              "      <th>True shooting percent</th>\n",
              "      <th>PPG</th>\n",
              "      <th>RPG</th>\n",
              "      <th>TRB</th>\n",
              "      <th>APG</th>\n",
              "      <th>ASTpercent</th>\n",
              "      <th>SPG</th>\n",
              "      <th>BPG</th>\n",
              "      <th>TOPGTurnovers</th>\n",
              "      <th>Versatility Index</th>\n",
              "      <th>ORTG</th>\n",
              "      <th>DRTG</th>\n",
              "      <th>Rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Alex Abrines</td>\n",
              "      <td>Okc</td>\n",
              "      <td>G</td>\n",
              "      <td>25.69</td>\n",
              "      <td>31</td>\n",
              "      <td>19.0</td>\n",
              "      <td>39.5</td>\n",
              "      <td>12.2</td>\n",
              "      <td>7.9</td>\n",
              "      <td>13</td>\n",
              "      <td>0.923</td>\n",
              "      <td>30</td>\n",
              "      <td>0.500</td>\n",
              "      <td>127</td>\n",
              "      <td>0.323</td>\n",
              "      <td>0.487</td>\n",
              "      <td>0.507</td>\n",
              "      <td>5.3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>4.2</td>\n",
              "      <td>0.6</td>\n",
              "      <td>4.3</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.45</td>\n",
              "      <td>3.5</td>\n",
              "      <td>103.1</td>\n",
              "      <td>103.9</td>\n",
              "      <td>72.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Quincy Acy</td>\n",
              "      <td>Pho</td>\n",
              "      <td>F</td>\n",
              "      <td>28.51</td>\n",
              "      <td>10</td>\n",
              "      <td>12.3</td>\n",
              "      <td>25.6</td>\n",
              "      <td>9.2</td>\n",
              "      <td>15.2</td>\n",
              "      <td>10</td>\n",
              "      <td>0.700</td>\n",
              "      <td>3</td>\n",
              "      <td>0.667</td>\n",
              "      <td>15</td>\n",
              "      <td>0.133</td>\n",
              "      <td>0.278</td>\n",
              "      <td>0.379</td>\n",
              "      <td>1.7</td>\n",
              "      <td>2.5</td>\n",
              "      <td>11.3</td>\n",
              "      <td>0.8</td>\n",
              "      <td>8.2</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.40</td>\n",
              "      <td>4.9</td>\n",
              "      <td>87.1</td>\n",
              "      <td>98.5</td>\n",
              "      <td>71.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Jaylen Adams</td>\n",
              "      <td>Atl</td>\n",
              "      <td>G</td>\n",
              "      <td>22.93</td>\n",
              "      <td>34</td>\n",
              "      <td>12.6</td>\n",
              "      <td>26.2</td>\n",
              "      <td>13.5</td>\n",
              "      <td>19.7</td>\n",
              "      <td>9</td>\n",
              "      <td>0.778</td>\n",
              "      <td>36</td>\n",
              "      <td>0.361</td>\n",
              "      <td>74</td>\n",
              "      <td>0.338</td>\n",
              "      <td>0.459</td>\n",
              "      <td>0.474</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.8</td>\n",
              "      <td>7.5</td>\n",
              "      <td>1.9</td>\n",
              "      <td>20.2</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.82</td>\n",
              "      <td>7.0</td>\n",
              "      <td>99.5</td>\n",
              "      <td>108.1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Steven Adams</td>\n",
              "      <td>Okc</td>\n",
              "      <td>C</td>\n",
              "      <td>25.73</td>\n",
              "      <td>80</td>\n",
              "      <td>33.4</td>\n",
              "      <td>69.5</td>\n",
              "      <td>16.4</td>\n",
              "      <td>12.8</td>\n",
              "      <td>292</td>\n",
              "      <td>0.500</td>\n",
              "      <td>807</td>\n",
              "      <td>0.596</td>\n",
              "      <td>2</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.595</td>\n",
              "      <td>0.591</td>\n",
              "      <td>13.9</td>\n",
              "      <td>9.5</td>\n",
              "      <td>14.7</td>\n",
              "      <td>1.6</td>\n",
              "      <td>6.6</td>\n",
              "      <td>1.49</td>\n",
              "      <td>0.96</td>\n",
              "      <td>1.73</td>\n",
              "      <td>7.1</td>\n",
              "      <td>119.9</td>\n",
              "      <td>102.7</td>\n",
              "      <td>84.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Bam Adebayo</td>\n",
              "      <td>Mia</td>\n",
              "      <td>C-F</td>\n",
              "      <td>21.73</td>\n",
              "      <td>82</td>\n",
              "      <td>23.3</td>\n",
              "      <td>48.6</td>\n",
              "      <td>15.8</td>\n",
              "      <td>17.1</td>\n",
              "      <td>226</td>\n",
              "      <td>0.735</td>\n",
              "      <td>471</td>\n",
              "      <td>0.588</td>\n",
              "      <td>15</td>\n",
              "      <td>0.200</td>\n",
              "      <td>0.579</td>\n",
              "      <td>0.623</td>\n",
              "      <td>8.9</td>\n",
              "      <td>7.3</td>\n",
              "      <td>16.6</td>\n",
              "      <td>2.2</td>\n",
              "      <td>14.2</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.79</td>\n",
              "      <td>1.48</td>\n",
              "      <td>9.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>97.2</td>\n",
              "      <td>78.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Name Team  Pos    Age  ...  Versatility Index   ORTG   DRTG  Rating\n",
              "0  Alex Abrines  Okc    G  25.69  ...                3.5  103.1  103.9    72.0\n",
              "1    Quincy Acy  Pho    F  28.51  ...                4.9   87.1   98.5    71.0\n",
              "2  Jaylen Adams  Atl    G  22.93  ...                7.0   99.5  108.1     NaN\n",
              "3  Steven Adams  Okc    C  25.73  ...                7.1  119.9  102.7    84.0\n",
              "4   Bam Adebayo  Mia  C-F  21.73  ...                9.0  120.0   97.2    78.0\n",
              "\n",
              "[5 rows x 29 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crLcQoSxPR51",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1 = df1[df1['Rating'].notna()]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e3FXNNUPgTX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1 = df1[df1['ORTG'].notna()]\n",
        "df1=df1[df1['DRTG'].notna()]\n",
        "df1=df1[df1['Effective Shooting percent'].notna()]\n",
        "df1=df1[df1['True shooting percent'].notna()]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAF-9o3MP84-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df2 = df2[df2['Rating'].notna()]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "835bF3sFQL2-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df2 = df2[df2['ORTG'].notna()]\n",
        "df2=df2[df2['DRTG'].notna()]\n",
        "df2=df2[df2['Effective Shooting percent'].notna()]\n",
        "df2=df2[df2['True shooting percent'].notna()]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-M5pk2nQkQ-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "c9982f84-53c5-4da3-b82f-33c3b99b8d27"
      },
      "source": [
        "df1.isnull().sum()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Name                          0\n",
              "Team                          0\n",
              "Pos                           0\n",
              "Age                           1\n",
              "GP                            0\n",
              "MPG                           0\n",
              "Minpercent                    0\n",
              "USG                           0\n",
              "TOR                           0\n",
              "FTA                           0\n",
              "FTpercent                     0\n",
              "2PA                           0\n",
              "2Ppercent                     0\n",
              "3PA                           0\n",
              "3Ppercent                     0\n",
              "Effective Shooting percent    0\n",
              "True shooting percent         0\n",
              "PPG                           0\n",
              "RPG                           0\n",
              "TRB                           0\n",
              "APG                           0\n",
              "ASTpercent                    0\n",
              "SPG                           0\n",
              "BPG                           0\n",
              "TOPGTurnovers                 0\n",
              "Versatility Index             0\n",
              "ORTG                          0\n",
              "DRTG                          0\n",
              "Rating                        0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3h3Y_ahQcPE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "365d4e28-64ff-4be5-ef42-a3458cc2e58b"
      },
      "source": [
        "df2.isnull().sum()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Name                          0\n",
              "Team                          0\n",
              "Pos                           0\n",
              "Age                           0\n",
              "GP                            0\n",
              "MPG                           0\n",
              "Minpercent                    0\n",
              "USG                           0\n",
              "TOR                           0\n",
              "FTA                           0\n",
              "FTpercent                     0\n",
              "2PA                           0\n",
              "2Ppercent                     0\n",
              "3PA                           0\n",
              "3Ppercent                     0\n",
              "Effective Shooting percent    0\n",
              "True shooting percent         0\n",
              "PPG                           0\n",
              "RPG                           0\n",
              "TRB                           0\n",
              "APG                           0\n",
              "ASTpercent                    0\n",
              "SPG                           0\n",
              "BPG                           0\n",
              "TOPGTurnovers                 0\n",
              "Versatility Index             0\n",
              "ORTG                          0\n",
              "DRTG                          0\n",
              "Rating                        0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2fzHcBgQn-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train=df1.append(df2, ignore_index=True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LePqGTNQWmsl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "5efc4fe7-6da5-4b6a-c161-6aa26de262aa"
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Team</th>\n",
              "      <th>Pos</th>\n",
              "      <th>Age</th>\n",
              "      <th>GP</th>\n",
              "      <th>MPG</th>\n",
              "      <th>Minpercent</th>\n",
              "      <th>USG</th>\n",
              "      <th>TOR</th>\n",
              "      <th>FTA</th>\n",
              "      <th>FTpercent</th>\n",
              "      <th>2PA</th>\n",
              "      <th>2Ppercent</th>\n",
              "      <th>3PA</th>\n",
              "      <th>3Ppercent</th>\n",
              "      <th>Effective Shooting percent</th>\n",
              "      <th>True shooting percent</th>\n",
              "      <th>PPG</th>\n",
              "      <th>RPG</th>\n",
              "      <th>TRB</th>\n",
              "      <th>APG</th>\n",
              "      <th>ASTpercent</th>\n",
              "      <th>SPG</th>\n",
              "      <th>BPG</th>\n",
              "      <th>TOPGTurnovers</th>\n",
              "      <th>Versatility Index</th>\n",
              "      <th>ORTG</th>\n",
              "      <th>DRTG</th>\n",
              "      <th>Rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Aaron Brooks</td>\n",
              "      <td>Min</td>\n",
              "      <td>PG</td>\n",
              "      <td>33.0</td>\n",
              "      <td>32</td>\n",
              "      <td>5.9</td>\n",
              "      <td>12.3</td>\n",
              "      <td>19.8</td>\n",
              "      <td>0.130</td>\n",
              "      <td>11</td>\n",
              "      <td>0.727</td>\n",
              "      <td>38</td>\n",
              "      <td>0.447</td>\n",
              "      <td>31</td>\n",
              "      <td>0.355</td>\n",
              "      <td>0.486</td>\n",
              "      <td>0.508</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0.5</td>\n",
              "      <td>5.2</td>\n",
              "      <td>0.6</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.34</td>\n",
              "      <td>6.0</td>\n",
              "      <td>103.1</td>\n",
              "      <td>100.4</td>\n",
              "      <td>73.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Aaron Gordon</td>\n",
              "      <td>Orl</td>\n",
              "      <td>SF</td>\n",
              "      <td>22.0</td>\n",
              "      <td>58</td>\n",
              "      <td>32.9</td>\n",
              "      <td>68.6</td>\n",
              "      <td>24.7</td>\n",
              "      <td>0.100</td>\n",
              "      <td>225</td>\n",
              "      <td>0.698</td>\n",
              "      <td>523</td>\n",
              "      <td>0.497</td>\n",
              "      <td>342</td>\n",
              "      <td>0.336</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.530</td>\n",
              "      <td>17.6</td>\n",
              "      <td>7.9</td>\n",
              "      <td>13.2</td>\n",
              "      <td>2.3</td>\n",
              "      <td>11.7</td>\n",
              "      <td>1.02</td>\n",
              "      <td>0.78</td>\n",
              "      <td>1.84</td>\n",
              "      <td>8.3</td>\n",
              "      <td>103.6</td>\n",
              "      <td>106.2</td>\n",
              "      <td>78.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Abdel Nader</td>\n",
              "      <td>Bos</td>\n",
              "      <td>SF</td>\n",
              "      <td>24.0</td>\n",
              "      <td>48</td>\n",
              "      <td>10.9</td>\n",
              "      <td>22.6</td>\n",
              "      <td>17.1</td>\n",
              "      <td>0.170</td>\n",
              "      <td>39</td>\n",
              "      <td>0.590</td>\n",
              "      <td>84</td>\n",
              "      <td>0.321</td>\n",
              "      <td>65</td>\n",
              "      <td>0.354</td>\n",
              "      <td>0.413</td>\n",
              "      <td>0.439</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>7.4</td>\n",
              "      <td>0.5</td>\n",
              "      <td>7.2</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.71</td>\n",
              "      <td>4.8</td>\n",
              "      <td>83.1</td>\n",
              "      <td>99.1</td>\n",
              "      <td>68.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Adreian Payne</td>\n",
              "      <td>Orl</td>\n",
              "      <td>PF</td>\n",
              "      <td>26.0</td>\n",
              "      <td>5</td>\n",
              "      <td>8.5</td>\n",
              "      <td>17.8</td>\n",
              "      <td>15.1</td>\n",
              "      <td>0.137</td>\n",
              "      <td>6</td>\n",
              "      <td>0.833</td>\n",
              "      <td>7</td>\n",
              "      <td>0.714</td>\n",
              "      <td>3</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.800</td>\n",
              "      <td>0.831</td>\n",
              "      <td>4.2</td>\n",
              "      <td>1.8</td>\n",
              "      <td>11.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.0</td>\n",
              "      <td>140.6</td>\n",
              "      <td>97.1</td>\n",
              "      <td>71.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Al Horford</td>\n",
              "      <td>Bos</td>\n",
              "      <td>C</td>\n",
              "      <td>31.0</td>\n",
              "      <td>72</td>\n",
              "      <td>31.6</td>\n",
              "      <td>65.9</td>\n",
              "      <td>18.4</td>\n",
              "      <td>0.141</td>\n",
              "      <td>120</td>\n",
              "      <td>0.783</td>\n",
              "      <td>527</td>\n",
              "      <td>0.514</td>\n",
              "      <td>226</td>\n",
              "      <td>0.429</td>\n",
              "      <td>0.553</td>\n",
              "      <td>0.575</td>\n",
              "      <td>12.9</td>\n",
              "      <td>7.4</td>\n",
              "      <td>12.7</td>\n",
              "      <td>4.7</td>\n",
              "      <td>23.6</td>\n",
              "      <td>0.60</td>\n",
              "      <td>1.08</td>\n",
              "      <td>1.83</td>\n",
              "      <td>9.7</td>\n",
              "      <td>115.4</td>\n",
              "      <td>100.4</td>\n",
              "      <td>83.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Name Team Pos   Age  ...  Versatility Index   ORTG   DRTG  Rating\n",
              "0   Aaron Brooks  Min  PG  33.0  ...                6.0  103.1  100.4    73.0\n",
              "1   Aaron Gordon  Orl  SF  22.0  ...                8.3  103.6  106.2    78.0\n",
              "2    Abdel Nader  Bos  SF  24.0  ...                4.8   83.1   99.1    68.0\n",
              "3  Adreian Payne  Orl  PF  26.0  ...                0.0  140.6   97.1    71.0\n",
              "4     Al Horford  Bos   C  31.0  ...                9.7  115.4  100.4    83.0\n",
              "\n",
              "[5 rows x 29 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fybNSmxkWs63",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "c352e9f1-2ee6-40a3-9663-09da88c6c509"
      },
      "source": [
        "df_train.tail()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Team</th>\n",
              "      <th>Pos</th>\n",
              "      <th>Age</th>\n",
              "      <th>GP</th>\n",
              "      <th>MPG</th>\n",
              "      <th>Minpercent</th>\n",
              "      <th>USG</th>\n",
              "      <th>TOR</th>\n",
              "      <th>FTA</th>\n",
              "      <th>FTpercent</th>\n",
              "      <th>2PA</th>\n",
              "      <th>2Ppercent</th>\n",
              "      <th>3PA</th>\n",
              "      <th>3Ppercent</th>\n",
              "      <th>Effective Shooting percent</th>\n",
              "      <th>True shooting percent</th>\n",
              "      <th>PPG</th>\n",
              "      <th>RPG</th>\n",
              "      <th>TRB</th>\n",
              "      <th>APG</th>\n",
              "      <th>ASTpercent</th>\n",
              "      <th>SPG</th>\n",
              "      <th>BPG</th>\n",
              "      <th>TOPGTurnovers</th>\n",
              "      <th>Versatility Index</th>\n",
              "      <th>ORTG</th>\n",
              "      <th>DRTG</th>\n",
              "      <th>Rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>819</th>\n",
              "      <td>Trae Young</td>\n",
              "      <td>Atl</td>\n",
              "      <td>G</td>\n",
              "      <td>20.56</td>\n",
              "      <td>81</td>\n",
              "      <td>30.900000</td>\n",
              "      <td>64.4</td>\n",
              "      <td>28.4</td>\n",
              "      <td>17.6</td>\n",
              "      <td>414</td>\n",
              "      <td>0.829</td>\n",
              "      <td>775</td>\n",
              "      <td>0.476</td>\n",
              "      <td>482</td>\n",
              "      <td>0.324</td>\n",
              "      <td>0.480000</td>\n",
              "      <td>0.538000</td>\n",
              "      <td>19.100000</td>\n",
              "      <td>3.700000</td>\n",
              "      <td>6.3</td>\n",
              "      <td>8.100000</td>\n",
              "      <td>40.5</td>\n",
              "      <td>0.860000</td>\n",
              "      <td>0.190000</td>\n",
              "      <td>3.80</td>\n",
              "      <td>10.7</td>\n",
              "      <td>107.5</td>\n",
              "      <td>114.2</td>\n",
              "      <td>77.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>820</th>\n",
              "      <td>Cody Zeller</td>\n",
              "      <td>Cha</td>\n",
              "      <td>F-C</td>\n",
              "      <td>26.51</td>\n",
              "      <td>49</td>\n",
              "      <td>25.400000</td>\n",
              "      <td>52.8</td>\n",
              "      <td>16.2</td>\n",
              "      <td>13.0</td>\n",
              "      <td>141</td>\n",
              "      <td>0.787</td>\n",
              "      <td>323</td>\n",
              "      <td>0.570</td>\n",
              "      <td>22</td>\n",
              "      <td>0.273</td>\n",
              "      <td>0.559000</td>\n",
              "      <td>0.611000</td>\n",
              "      <td>10.100000</td>\n",
              "      <td>6.800000</td>\n",
              "      <td>14.4</td>\n",
              "      <td>2.100000</td>\n",
              "      <td>12.2</td>\n",
              "      <td>0.780000</td>\n",
              "      <td>0.860000</td>\n",
              "      <td>1.24</td>\n",
              "      <td>8.3</td>\n",
              "      <td>123.7</td>\n",
              "      <td>100.3</td>\n",
              "      <td>77.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>821</th>\n",
              "      <td>Tyler Zeller</td>\n",
              "      <td>Atl</td>\n",
              "      <td>C</td>\n",
              "      <td>29.23</td>\n",
              "      <td>6</td>\n",
              "      <td>15.533333</td>\n",
              "      <td>11.1</td>\n",
              "      <td>7.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.558851</td>\n",
              "      <td>0.613333</td>\n",
              "      <td>7.666667</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>29.5</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>10.9</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>82.4</td>\n",
              "      <td>74.1</td>\n",
              "      <td>76.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>822</th>\n",
              "      <td>Ante Zizic</td>\n",
              "      <td>Cle</td>\n",
              "      <td>C</td>\n",
              "      <td>22.27</td>\n",
              "      <td>59</td>\n",
              "      <td>18.300000</td>\n",
              "      <td>38.2</td>\n",
              "      <td>18.2</td>\n",
              "      <td>13.8</td>\n",
              "      <td>132</td>\n",
              "      <td>0.705</td>\n",
              "      <td>330</td>\n",
              "      <td>0.555</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.555000</td>\n",
              "      <td>0.591000</td>\n",
              "      <td>7.800000</td>\n",
              "      <td>5.400000</td>\n",
              "      <td>16.4</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>7.7</td>\n",
              "      <td>0.220000</td>\n",
              "      <td>0.370000</td>\n",
              "      <td>1.05</td>\n",
              "      <td>7.3</td>\n",
              "      <td>116.2</td>\n",
              "      <td>108.1</td>\n",
              "      <td>73.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>823</th>\n",
              "      <td>Ivica Zubac</td>\n",
              "      <td>Lal</td>\n",
              "      <td>C</td>\n",
              "      <td>22.07</td>\n",
              "      <td>59</td>\n",
              "      <td>17.583051</td>\n",
              "      <td>32.6</td>\n",
              "      <td>20.5</td>\n",
              "      <td>12.9</td>\n",
              "      <td>66</td>\n",
              "      <td>0.864</td>\n",
              "      <td>193</td>\n",
              "      <td>0.580</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.559541</td>\n",
              "      <td>0.604259</td>\n",
              "      <td>8.896610</td>\n",
              "      <td>6.133898</td>\n",
              "      <td>16.3</td>\n",
              "      <td>1.108475</td>\n",
              "      <td>7.3</td>\n",
              "      <td>0.251356</td>\n",
              "      <td>0.864068</td>\n",
              "      <td>1.00</td>\n",
              "      <td>8.3</td>\n",
              "      <td>120.7</td>\n",
              "      <td>97.9</td>\n",
              "      <td>73.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Name Team  Pos    Age  ...  Versatility Index   ORTG   DRTG  Rating\n",
              "819    Trae Young  Atl    G  20.56  ...               10.7  107.5  114.2    77.0\n",
              "820   Cody Zeller  Cha  F-C  26.51  ...                8.3  123.7  100.3    77.0\n",
              "821  Tyler Zeller  Atl    C  29.23  ...                0.0   82.4   74.1    76.0\n",
              "822    Ante Zizic  Cle    C  22.27  ...                7.3  116.2  108.1    73.0\n",
              "823   Ivica Zubac  Lal    C  22.07  ...                8.3  120.7   97.9    73.0\n",
              "\n",
              "[5 rows x 29 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2gUzQ0rW6z8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train=df_train[df_train.GP>10]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrz86coKXJpn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train=df_train[df_train.MPG>5]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1Awbx3Y1BU4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y=df_train['Rating']"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrPjN3ZEntGh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6eb7e56c-401d-4e8e-ba67-8bc170a3cb13"
      },
      "source": [
        "l=range(3,29)\n",
        "print(l)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "range(3, 29)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9vKQgbvnvTD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Xdf=df_train.iloc[:,l]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ng6cgm9Ynyar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "correlation_matrix=Xdf.astype(float).corr()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ek4r2BjYn37n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 960
        },
        "outputId": "fc2ad1a5-83a7-46a5-d743-d8446736125a"
      },
      "source": [
        "correlation_matrix"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age</th>\n",
              "      <th>GP</th>\n",
              "      <th>MPG</th>\n",
              "      <th>Minpercent</th>\n",
              "      <th>USG</th>\n",
              "      <th>TOR</th>\n",
              "      <th>FTA</th>\n",
              "      <th>FTpercent</th>\n",
              "      <th>2PA</th>\n",
              "      <th>2Ppercent</th>\n",
              "      <th>3PA</th>\n",
              "      <th>3Ppercent</th>\n",
              "      <th>Effective Shooting percent</th>\n",
              "      <th>True shooting percent</th>\n",
              "      <th>PPG</th>\n",
              "      <th>RPG</th>\n",
              "      <th>TRB</th>\n",
              "      <th>APG</th>\n",
              "      <th>ASTpercent</th>\n",
              "      <th>SPG</th>\n",
              "      <th>BPG</th>\n",
              "      <th>TOPGTurnovers</th>\n",
              "      <th>Versatility Index</th>\n",
              "      <th>ORTG</th>\n",
              "      <th>DRTG</th>\n",
              "      <th>Rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Age</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.008239</td>\n",
              "      <td>0.024064</td>\n",
              "      <td>0.028886</td>\n",
              "      <td>-0.060251</td>\n",
              "      <td>0.100957</td>\n",
              "      <td>-0.048869</td>\n",
              "      <td>0.153754</td>\n",
              "      <td>-0.082679</td>\n",
              "      <td>0.015383</td>\n",
              "      <td>0.058879</td>\n",
              "      <td>0.053441</td>\n",
              "      <td>0.094074</td>\n",
              "      <td>0.109160</td>\n",
              "      <td>-0.001553</td>\n",
              "      <td>-0.015819</td>\n",
              "      <td>-0.021012</td>\n",
              "      <td>0.070956</td>\n",
              "      <td>0.085225</td>\n",
              "      <td>-0.002478</td>\n",
              "      <td>-0.046340</td>\n",
              "      <td>-0.015457</td>\n",
              "      <td>0.042361</td>\n",
              "      <td>0.128710</td>\n",
              "      <td>0.056493</td>\n",
              "      <td>0.191559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GP</th>\n",
              "      <td>0.008239</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.554816</td>\n",
              "      <td>0.553973</td>\n",
              "      <td>0.124929</td>\n",
              "      <td>-0.061192</td>\n",
              "      <td>0.501268</td>\n",
              "      <td>0.204742</td>\n",
              "      <td>0.588465</td>\n",
              "      <td>0.214247</td>\n",
              "      <td>0.509540</td>\n",
              "      <td>0.189666</td>\n",
              "      <td>0.338347</td>\n",
              "      <td>0.366066</td>\n",
              "      <td>0.432534</td>\n",
              "      <td>0.363632</td>\n",
              "      <td>0.024689</td>\n",
              "      <td>0.243824</td>\n",
              "      <td>0.087793</td>\n",
              "      <td>0.347364</td>\n",
              "      <td>0.230379</td>\n",
              "      <td>0.305037</td>\n",
              "      <td>0.203155</td>\n",
              "      <td>0.362878</td>\n",
              "      <td>0.125819</td>\n",
              "      <td>0.295306</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MPG</th>\n",
              "      <td>0.024064</td>\n",
              "      <td>0.554816</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.988246</td>\n",
              "      <td>0.467487</td>\n",
              "      <td>-0.034072</td>\n",
              "      <td>0.697886</td>\n",
              "      <td>0.239481</td>\n",
              "      <td>0.749629</td>\n",
              "      <td>0.043666</td>\n",
              "      <td>0.646986</td>\n",
              "      <td>0.305377</td>\n",
              "      <td>0.169207</td>\n",
              "      <td>0.239766</td>\n",
              "      <td>0.864957</td>\n",
              "      <td>0.574011</td>\n",
              "      <td>-0.061121</td>\n",
              "      <td>0.651385</td>\n",
              "      <td>0.382605</td>\n",
              "      <td>0.735672</td>\n",
              "      <td>0.321465</td>\n",
              "      <td>0.750676</td>\n",
              "      <td>0.422977</td>\n",
              "      <td>0.215054</td>\n",
              "      <td>0.330420</td>\n",
              "      <td>0.700848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Minpercent</th>\n",
              "      <td>0.028886</td>\n",
              "      <td>0.553973</td>\n",
              "      <td>0.988246</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.458707</td>\n",
              "      <td>-0.031655</td>\n",
              "      <td>0.694761</td>\n",
              "      <td>0.253600</td>\n",
              "      <td>0.748010</td>\n",
              "      <td>0.056714</td>\n",
              "      <td>0.646286</td>\n",
              "      <td>0.317442</td>\n",
              "      <td>0.169840</td>\n",
              "      <td>0.236845</td>\n",
              "      <td>0.851345</td>\n",
              "      <td>0.568137</td>\n",
              "      <td>-0.061986</td>\n",
              "      <td>0.638772</td>\n",
              "      <td>0.383059</td>\n",
              "      <td>0.726865</td>\n",
              "      <td>0.319956</td>\n",
              "      <td>0.752725</td>\n",
              "      <td>0.431630</td>\n",
              "      <td>0.244416</td>\n",
              "      <td>0.335428</td>\n",
              "      <td>0.694044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>USG</th>\n",
              "      <td>-0.060251</td>\n",
              "      <td>0.124929</td>\n",
              "      <td>0.467487</td>\n",
              "      <td>0.458707</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.016562</td>\n",
              "      <td>0.647839</td>\n",
              "      <td>0.231105</td>\n",
              "      <td>0.634755</td>\n",
              "      <td>-0.031518</td>\n",
              "      <td>0.413936</td>\n",
              "      <td>0.158574</td>\n",
              "      <td>-0.053940</td>\n",
              "      <td>0.066651</td>\n",
              "      <td>0.780809</td>\n",
              "      <td>0.335596</td>\n",
              "      <td>0.042481</td>\n",
              "      <td>0.521959</td>\n",
              "      <td>0.509777</td>\n",
              "      <td>0.363409</td>\n",
              "      <td>0.154494</td>\n",
              "      <td>0.720266</td>\n",
              "      <td>0.679291</td>\n",
              "      <td>-0.013017</td>\n",
              "      <td>0.090840</td>\n",
              "      <td>0.635584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TOR</th>\n",
              "      <td>0.100957</td>\n",
              "      <td>-0.061192</td>\n",
              "      <td>-0.034072</td>\n",
              "      <td>-0.031655</td>\n",
              "      <td>-0.016562</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.003606</td>\n",
              "      <td>-0.035587</td>\n",
              "      <td>-0.040272</td>\n",
              "      <td>0.037541</td>\n",
              "      <td>-0.041063</td>\n",
              "      <td>-0.039023</td>\n",
              "      <td>0.000407</td>\n",
              "      <td>0.003017</td>\n",
              "      <td>-0.016368</td>\n",
              "      <td>0.034493</td>\n",
              "      <td>0.035233</td>\n",
              "      <td>0.115143</td>\n",
              "      <td>0.147937</td>\n",
              "      <td>0.001083</td>\n",
              "      <td>-0.003420</td>\n",
              "      <td>0.076274</td>\n",
              "      <td>0.175858</td>\n",
              "      <td>0.027986</td>\n",
              "      <td>0.046262</td>\n",
              "      <td>0.050791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>FTA</th>\n",
              "      <td>-0.048869</td>\n",
              "      <td>0.501268</td>\n",
              "      <td>0.697886</td>\n",
              "      <td>0.694761</td>\n",
              "      <td>0.647839</td>\n",
              "      <td>0.003606</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.148428</td>\n",
              "      <td>0.881564</td>\n",
              "      <td>0.184241</td>\n",
              "      <td>0.493571</td>\n",
              "      <td>0.071932</td>\n",
              "      <td>0.189553</td>\n",
              "      <td>0.318057</td>\n",
              "      <td>0.852871</td>\n",
              "      <td>0.632411</td>\n",
              "      <td>0.202447</td>\n",
              "      <td>0.560561</td>\n",
              "      <td>0.406771</td>\n",
              "      <td>0.524444</td>\n",
              "      <td>0.432624</td>\n",
              "      <td>0.757213</td>\n",
              "      <td>0.639532</td>\n",
              "      <td>0.297503</td>\n",
              "      <td>0.049289</td>\n",
              "      <td>0.719526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>FTpercent</th>\n",
              "      <td>0.153754</td>\n",
              "      <td>0.204742</td>\n",
              "      <td>0.239481</td>\n",
              "      <td>0.253600</td>\n",
              "      <td>0.231105</td>\n",
              "      <td>-0.035587</td>\n",
              "      <td>0.148428</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.135521</td>\n",
              "      <td>-0.109377</td>\n",
              "      <td>0.384469</td>\n",
              "      <td>0.377354</td>\n",
              "      <td>0.013442</td>\n",
              "      <td>0.146920</td>\n",
              "      <td>0.282759</td>\n",
              "      <td>-0.050701</td>\n",
              "      <td>-0.238944</td>\n",
              "      <td>0.163421</td>\n",
              "      <td>0.167604</td>\n",
              "      <td>0.116648</td>\n",
              "      <td>-0.125544</td>\n",
              "      <td>0.151612</td>\n",
              "      <td>0.128195</td>\n",
              "      <td>0.230158</td>\n",
              "      <td>0.213775</td>\n",
              "      <td>0.158993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2PA</th>\n",
              "      <td>-0.082679</td>\n",
              "      <td>0.588465</td>\n",
              "      <td>0.749629</td>\n",
              "      <td>0.748010</td>\n",
              "      <td>0.634755</td>\n",
              "      <td>-0.040272</td>\n",
              "      <td>0.881564</td>\n",
              "      <td>0.135521</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.182515</td>\n",
              "      <td>0.448705</td>\n",
              "      <td>0.081202</td>\n",
              "      <td>0.183999</td>\n",
              "      <td>0.260652</td>\n",
              "      <td>0.841283</td>\n",
              "      <td>0.641522</td>\n",
              "      <td>0.188340</td>\n",
              "      <td>0.547845</td>\n",
              "      <td>0.387767</td>\n",
              "      <td>0.533292</td>\n",
              "      <td>0.429132</td>\n",
              "      <td>0.724872</td>\n",
              "      <td>0.611533</td>\n",
              "      <td>0.259003</td>\n",
              "      <td>0.104356</td>\n",
              "      <td>0.677429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2Ppercent</th>\n",
              "      <td>0.015383</td>\n",
              "      <td>0.214247</td>\n",
              "      <td>0.043666</td>\n",
              "      <td>0.056714</td>\n",
              "      <td>-0.031518</td>\n",
              "      <td>0.037541</td>\n",
              "      <td>0.184241</td>\n",
              "      <td>-0.109377</td>\n",
              "      <td>0.182515</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.098491</td>\n",
              "      <td>-0.222981</td>\n",
              "      <td>0.719143</td>\n",
              "      <td>0.691941</td>\n",
              "      <td>0.094908</td>\n",
              "      <td>0.345577</td>\n",
              "      <td>0.419818</td>\n",
              "      <td>-0.078669</td>\n",
              "      <td>-0.118023</td>\n",
              "      <td>-0.037532</td>\n",
              "      <td>0.384246</td>\n",
              "      <td>0.029259</td>\n",
              "      <td>0.221464</td>\n",
              "      <td>0.686784</td>\n",
              "      <td>-0.313134</td>\n",
              "      <td>0.127067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3PA</th>\n",
              "      <td>0.058879</td>\n",
              "      <td>0.509540</td>\n",
              "      <td>0.646986</td>\n",
              "      <td>0.646286</td>\n",
              "      <td>0.413936</td>\n",
              "      <td>-0.041063</td>\n",
              "      <td>0.493571</td>\n",
              "      <td>0.384469</td>\n",
              "      <td>0.448705</td>\n",
              "      <td>-0.098491</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.483224</td>\n",
              "      <td>0.069952</td>\n",
              "      <td>0.125462</td>\n",
              "      <td>0.652179</td>\n",
              "      <td>0.090596</td>\n",
              "      <td>-0.351448</td>\n",
              "      <td>0.454504</td>\n",
              "      <td>0.288400</td>\n",
              "      <td>0.469579</td>\n",
              "      <td>-0.058257</td>\n",
              "      <td>0.467335</td>\n",
              "      <td>0.194996</td>\n",
              "      <td>0.105277</td>\n",
              "      <td>0.344430</td>\n",
              "      <td>0.443916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3Ppercent</th>\n",
              "      <td>0.053441</td>\n",
              "      <td>0.189666</td>\n",
              "      <td>0.305377</td>\n",
              "      <td>0.317442</td>\n",
              "      <td>0.158574</td>\n",
              "      <td>-0.039023</td>\n",
              "      <td>0.071932</td>\n",
              "      <td>0.377354</td>\n",
              "      <td>0.081202</td>\n",
              "      <td>-0.222981</td>\n",
              "      <td>0.483224</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.005262</td>\n",
              "      <td>0.028087</td>\n",
              "      <td>0.264722</td>\n",
              "      <td>-0.128861</td>\n",
              "      <td>-0.440436</td>\n",
              "      <td>0.200579</td>\n",
              "      <td>0.151524</td>\n",
              "      <td>0.214192</td>\n",
              "      <td>-0.195270</td>\n",
              "      <td>0.144822</td>\n",
              "      <td>-0.003829</td>\n",
              "      <td>0.054601</td>\n",
              "      <td>0.358368</td>\n",
              "      <td>0.102731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Effective Shooting percent</th>\n",
              "      <td>0.094074</td>\n",
              "      <td>0.338347</td>\n",
              "      <td>0.169207</td>\n",
              "      <td>0.169840</td>\n",
              "      <td>-0.053940</td>\n",
              "      <td>0.000407</td>\n",
              "      <td>0.189553</td>\n",
              "      <td>0.013442</td>\n",
              "      <td>0.183999</td>\n",
              "      <td>0.719143</td>\n",
              "      <td>0.069952</td>\n",
              "      <td>0.005262</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.956555</td>\n",
              "      <td>0.190764</td>\n",
              "      <td>0.350211</td>\n",
              "      <td>0.365361</td>\n",
              "      <td>-0.061205</td>\n",
              "      <td>-0.153143</td>\n",
              "      <td>0.018825</td>\n",
              "      <td>0.337724</td>\n",
              "      <td>0.039424</td>\n",
              "      <td>0.183793</td>\n",
              "      <td>0.742513</td>\n",
              "      <td>-0.169796</td>\n",
              "      <td>0.151453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>True shooting percent</th>\n",
              "      <td>0.109160</td>\n",
              "      <td>0.366066</td>\n",
              "      <td>0.239766</td>\n",
              "      <td>0.236845</td>\n",
              "      <td>0.066651</td>\n",
              "      <td>0.003017</td>\n",
              "      <td>0.318057</td>\n",
              "      <td>0.146920</td>\n",
              "      <td>0.260652</td>\n",
              "      <td>0.691941</td>\n",
              "      <td>0.125462</td>\n",
              "      <td>0.028087</td>\n",
              "      <td>0.956555</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.305727</td>\n",
              "      <td>0.394118</td>\n",
              "      <td>0.364722</td>\n",
              "      <td>0.023688</td>\n",
              "      <td>-0.077054</td>\n",
              "      <td>0.077394</td>\n",
              "      <td>0.354060</td>\n",
              "      <td>0.137441</td>\n",
              "      <td>0.276052</td>\n",
              "      <td>0.774199</td>\n",
              "      <td>-0.155034</td>\n",
              "      <td>0.248449</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PPG</th>\n",
              "      <td>-0.001553</td>\n",
              "      <td>0.432534</td>\n",
              "      <td>0.864957</td>\n",
              "      <td>0.851345</td>\n",
              "      <td>0.780809</td>\n",
              "      <td>-0.016368</td>\n",
              "      <td>0.852871</td>\n",
              "      <td>0.282759</td>\n",
              "      <td>0.841283</td>\n",
              "      <td>0.094908</td>\n",
              "      <td>0.652179</td>\n",
              "      <td>0.264722</td>\n",
              "      <td>0.190764</td>\n",
              "      <td>0.305727</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.576048</td>\n",
              "      <td>0.035055</td>\n",
              "      <td>0.655371</td>\n",
              "      <td>0.459652</td>\n",
              "      <td>0.631433</td>\n",
              "      <td>0.333960</td>\n",
              "      <td>0.827280</td>\n",
              "      <td>0.626009</td>\n",
              "      <td>0.237893</td>\n",
              "      <td>0.238570</td>\n",
              "      <td>0.815068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RPG</th>\n",
              "      <td>-0.015819</td>\n",
              "      <td>0.363632</td>\n",
              "      <td>0.574011</td>\n",
              "      <td>0.568137</td>\n",
              "      <td>0.335596</td>\n",
              "      <td>0.034493</td>\n",
              "      <td>0.632411</td>\n",
              "      <td>-0.050701</td>\n",
              "      <td>0.641522</td>\n",
              "      <td>0.345577</td>\n",
              "      <td>0.090596</td>\n",
              "      <td>-0.128861</td>\n",
              "      <td>0.350211</td>\n",
              "      <td>0.394118</td>\n",
              "      <td>0.576048</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.721402</td>\n",
              "      <td>0.254604</td>\n",
              "      <td>0.074783</td>\n",
              "      <td>0.360394</td>\n",
              "      <td>0.687022</td>\n",
              "      <td>0.530920</td>\n",
              "      <td>0.601485</td>\n",
              "      <td>0.362782</td>\n",
              "      <td>-0.228796</td>\n",
              "      <td>0.594867</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TRB</th>\n",
              "      <td>-0.021012</td>\n",
              "      <td>0.024689</td>\n",
              "      <td>-0.061121</td>\n",
              "      <td>-0.061986</td>\n",
              "      <td>0.042481</td>\n",
              "      <td>0.035233</td>\n",
              "      <td>0.202447</td>\n",
              "      <td>-0.238944</td>\n",
              "      <td>0.188340</td>\n",
              "      <td>0.419818</td>\n",
              "      <td>-0.351448</td>\n",
              "      <td>-0.440436</td>\n",
              "      <td>0.365361</td>\n",
              "      <td>0.364722</td>\n",
              "      <td>0.035055</td>\n",
              "      <td>0.721402</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.185232</td>\n",
              "      <td>-0.216946</td>\n",
              "      <td>-0.123133</td>\n",
              "      <td>0.561677</td>\n",
              "      <td>0.057930</td>\n",
              "      <td>0.418163</td>\n",
              "      <td>0.320647</td>\n",
              "      <td>-0.595353</td>\n",
              "      <td>0.171194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>APG</th>\n",
              "      <td>0.070956</td>\n",
              "      <td>0.243824</td>\n",
              "      <td>0.651385</td>\n",
              "      <td>0.638772</td>\n",
              "      <td>0.521959</td>\n",
              "      <td>0.115143</td>\n",
              "      <td>0.560561</td>\n",
              "      <td>0.163421</td>\n",
              "      <td>0.547845</td>\n",
              "      <td>-0.078669</td>\n",
              "      <td>0.454504</td>\n",
              "      <td>0.200579</td>\n",
              "      <td>-0.061205</td>\n",
              "      <td>0.023688</td>\n",
              "      <td>0.655371</td>\n",
              "      <td>0.254604</td>\n",
              "      <td>-0.185232</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.907138</td>\n",
              "      <td>0.670698</td>\n",
              "      <td>0.045115</td>\n",
              "      <td>0.824122</td>\n",
              "      <td>0.672624</td>\n",
              "      <td>0.068748</td>\n",
              "      <td>0.249429</td>\n",
              "      <td>0.612017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ASTpercent</th>\n",
              "      <td>0.085225</td>\n",
              "      <td>0.087793</td>\n",
              "      <td>0.382605</td>\n",
              "      <td>0.383059</td>\n",
              "      <td>0.509777</td>\n",
              "      <td>0.147937</td>\n",
              "      <td>0.406771</td>\n",
              "      <td>0.167604</td>\n",
              "      <td>0.387767</td>\n",
              "      <td>-0.118023</td>\n",
              "      <td>0.288400</td>\n",
              "      <td>0.151524</td>\n",
              "      <td>-0.153143</td>\n",
              "      <td>-0.077054</td>\n",
              "      <td>0.459652</td>\n",
              "      <td>0.074783</td>\n",
              "      <td>-0.216946</td>\n",
              "      <td>0.907138</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.482634</td>\n",
              "      <td>-0.083014</td>\n",
              "      <td>0.694673</td>\n",
              "      <td>0.702775</td>\n",
              "      <td>0.027942</td>\n",
              "      <td>0.187475</td>\n",
              "      <td>0.442344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SPG</th>\n",
              "      <td>-0.002478</td>\n",
              "      <td>0.347364</td>\n",
              "      <td>0.735672</td>\n",
              "      <td>0.726865</td>\n",
              "      <td>0.363409</td>\n",
              "      <td>0.001083</td>\n",
              "      <td>0.524444</td>\n",
              "      <td>0.116648</td>\n",
              "      <td>0.533292</td>\n",
              "      <td>-0.037532</td>\n",
              "      <td>0.469579</td>\n",
              "      <td>0.214192</td>\n",
              "      <td>0.018825</td>\n",
              "      <td>0.077394</td>\n",
              "      <td>0.631433</td>\n",
              "      <td>0.360394</td>\n",
              "      <td>-0.123133</td>\n",
              "      <td>0.670698</td>\n",
              "      <td>0.482634</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.210913</td>\n",
              "      <td>0.656771</td>\n",
              "      <td>0.402608</td>\n",
              "      <td>0.083560</td>\n",
              "      <td>0.148376</td>\n",
              "      <td>0.563638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BPG</th>\n",
              "      <td>-0.046340</td>\n",
              "      <td>0.230379</td>\n",
              "      <td>0.321465</td>\n",
              "      <td>0.319956</td>\n",
              "      <td>0.154494</td>\n",
              "      <td>-0.003420</td>\n",
              "      <td>0.432624</td>\n",
              "      <td>-0.125544</td>\n",
              "      <td>0.429132</td>\n",
              "      <td>0.384246</td>\n",
              "      <td>-0.058257</td>\n",
              "      <td>-0.195270</td>\n",
              "      <td>0.337724</td>\n",
              "      <td>0.354060</td>\n",
              "      <td>0.333960</td>\n",
              "      <td>0.687022</td>\n",
              "      <td>0.561677</td>\n",
              "      <td>0.045115</td>\n",
              "      <td>-0.083014</td>\n",
              "      <td>0.210913</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.284406</td>\n",
              "      <td>0.330867</td>\n",
              "      <td>0.320508</td>\n",
              "      <td>-0.312841</td>\n",
              "      <td>0.399407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TOPGTurnovers</th>\n",
              "      <td>-0.015457</td>\n",
              "      <td>0.305037</td>\n",
              "      <td>0.750676</td>\n",
              "      <td>0.752725</td>\n",
              "      <td>0.720266</td>\n",
              "      <td>0.076274</td>\n",
              "      <td>0.757213</td>\n",
              "      <td>0.151612</td>\n",
              "      <td>0.724872</td>\n",
              "      <td>0.029259</td>\n",
              "      <td>0.467335</td>\n",
              "      <td>0.144822</td>\n",
              "      <td>0.039424</td>\n",
              "      <td>0.137441</td>\n",
              "      <td>0.827280</td>\n",
              "      <td>0.530920</td>\n",
              "      <td>0.057930</td>\n",
              "      <td>0.824122</td>\n",
              "      <td>0.694673</td>\n",
              "      <td>0.656771</td>\n",
              "      <td>0.284406</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.738656</td>\n",
              "      <td>0.050300</td>\n",
              "      <td>0.119872</td>\n",
              "      <td>0.744478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Versatility Index</th>\n",
              "      <td>0.042361</td>\n",
              "      <td>0.203155</td>\n",
              "      <td>0.422977</td>\n",
              "      <td>0.431630</td>\n",
              "      <td>0.679291</td>\n",
              "      <td>0.175858</td>\n",
              "      <td>0.639532</td>\n",
              "      <td>0.128195</td>\n",
              "      <td>0.611533</td>\n",
              "      <td>0.221464</td>\n",
              "      <td>0.194996</td>\n",
              "      <td>-0.003829</td>\n",
              "      <td>0.183793</td>\n",
              "      <td>0.276052</td>\n",
              "      <td>0.626009</td>\n",
              "      <td>0.601485</td>\n",
              "      <td>0.418163</td>\n",
              "      <td>0.672624</td>\n",
              "      <td>0.702775</td>\n",
              "      <td>0.402608</td>\n",
              "      <td>0.330867</td>\n",
              "      <td>0.738656</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.324397</td>\n",
              "      <td>-0.150039</td>\n",
              "      <td>0.628115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ORTG</th>\n",
              "      <td>0.128710</td>\n",
              "      <td>0.362878</td>\n",
              "      <td>0.215054</td>\n",
              "      <td>0.244416</td>\n",
              "      <td>-0.013017</td>\n",
              "      <td>0.027986</td>\n",
              "      <td>0.297503</td>\n",
              "      <td>0.230158</td>\n",
              "      <td>0.259003</td>\n",
              "      <td>0.686784</td>\n",
              "      <td>0.105277</td>\n",
              "      <td>0.054601</td>\n",
              "      <td>0.742513</td>\n",
              "      <td>0.774199</td>\n",
              "      <td>0.237893</td>\n",
              "      <td>0.362782</td>\n",
              "      <td>0.320647</td>\n",
              "      <td>0.068748</td>\n",
              "      <td>0.027942</td>\n",
              "      <td>0.083560</td>\n",
              "      <td>0.320508</td>\n",
              "      <td>0.050300</td>\n",
              "      <td>0.324397</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.109250</td>\n",
              "      <td>0.217665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DRTG</th>\n",
              "      <td>0.056493</td>\n",
              "      <td>0.125819</td>\n",
              "      <td>0.330420</td>\n",
              "      <td>0.335428</td>\n",
              "      <td>0.090840</td>\n",
              "      <td>0.046262</td>\n",
              "      <td>0.049289</td>\n",
              "      <td>0.213775</td>\n",
              "      <td>0.104356</td>\n",
              "      <td>-0.313134</td>\n",
              "      <td>0.344430</td>\n",
              "      <td>0.358368</td>\n",
              "      <td>-0.169796</td>\n",
              "      <td>-0.155034</td>\n",
              "      <td>0.238570</td>\n",
              "      <td>-0.228796</td>\n",
              "      <td>-0.595353</td>\n",
              "      <td>0.249429</td>\n",
              "      <td>0.187475</td>\n",
              "      <td>0.148376</td>\n",
              "      <td>-0.312841</td>\n",
              "      <td>0.119872</td>\n",
              "      <td>-0.150039</td>\n",
              "      <td>-0.109250</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.064855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Rating</th>\n",
              "      <td>0.191559</td>\n",
              "      <td>0.295306</td>\n",
              "      <td>0.700848</td>\n",
              "      <td>0.694044</td>\n",
              "      <td>0.635584</td>\n",
              "      <td>0.050791</td>\n",
              "      <td>0.719526</td>\n",
              "      <td>0.158993</td>\n",
              "      <td>0.677429</td>\n",
              "      <td>0.127067</td>\n",
              "      <td>0.443916</td>\n",
              "      <td>0.102731</td>\n",
              "      <td>0.151453</td>\n",
              "      <td>0.248449</td>\n",
              "      <td>0.815068</td>\n",
              "      <td>0.594867</td>\n",
              "      <td>0.171194</td>\n",
              "      <td>0.612017</td>\n",
              "      <td>0.442344</td>\n",
              "      <td>0.563638</td>\n",
              "      <td>0.399407</td>\n",
              "      <td>0.744478</td>\n",
              "      <td>0.628115</td>\n",
              "      <td>0.217665</td>\n",
              "      <td>0.064855</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 Age        GP  ...      DRTG    Rating\n",
              "Age                         1.000000  0.008239  ...  0.056493  0.191559\n",
              "GP                          0.008239  1.000000  ...  0.125819  0.295306\n",
              "MPG                         0.024064  0.554816  ...  0.330420  0.700848\n",
              "Minpercent                  0.028886  0.553973  ...  0.335428  0.694044\n",
              "USG                        -0.060251  0.124929  ...  0.090840  0.635584\n",
              "TOR                         0.100957 -0.061192  ...  0.046262  0.050791\n",
              "FTA                        -0.048869  0.501268  ...  0.049289  0.719526\n",
              "FTpercent                   0.153754  0.204742  ...  0.213775  0.158993\n",
              "2PA                        -0.082679  0.588465  ...  0.104356  0.677429\n",
              "2Ppercent                   0.015383  0.214247  ... -0.313134  0.127067\n",
              "3PA                         0.058879  0.509540  ...  0.344430  0.443916\n",
              "3Ppercent                   0.053441  0.189666  ...  0.358368  0.102731\n",
              "Effective Shooting percent  0.094074  0.338347  ... -0.169796  0.151453\n",
              "True shooting percent       0.109160  0.366066  ... -0.155034  0.248449\n",
              "PPG                        -0.001553  0.432534  ...  0.238570  0.815068\n",
              "RPG                        -0.015819  0.363632  ... -0.228796  0.594867\n",
              "TRB                        -0.021012  0.024689  ... -0.595353  0.171194\n",
              "APG                         0.070956  0.243824  ...  0.249429  0.612017\n",
              "ASTpercent                  0.085225  0.087793  ...  0.187475  0.442344\n",
              "SPG                        -0.002478  0.347364  ...  0.148376  0.563638\n",
              "BPG                        -0.046340  0.230379  ... -0.312841  0.399407\n",
              "TOPGTurnovers              -0.015457  0.305037  ...  0.119872  0.744478\n",
              "Versatility Index           0.042361  0.203155  ... -0.150039  0.628115\n",
              "ORTG                        0.128710  0.362878  ... -0.109250  0.217665\n",
              "DRTG                        0.056493  0.125819  ...  1.000000  0.064855\n",
              "Rating                      0.191559  0.295306  ...  0.064855  1.000000\n",
              "\n",
              "[26 rows x 26 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GLcQ2orn7HQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "correlated_features = [[] for _ in range(26)]\n",
        "for i in range(0,26):\n",
        "    correlated_features[i].append(i)\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(len(correlation_matrix.columns)):\n",
        "        if abs(correlation_matrix.iloc[i, j]) > 0.80:\n",
        "            colname1 = correlation_matrix.columns[i]\n",
        "            colname2=correlation_matrix.columns[j]\n",
        "            if(i!=j):  \n",
        "                correlated_features[i].append(colname2)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fz7tz_ZaoEcR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dic={}\n",
        "count=0\n",
        "for i in Xdf.columns:\n",
        "  dic[count]=i\n",
        "  count+=1"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryJjvB1ooFk7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "5397eba7-2432-4093-b815-3b942711c468"
      },
      "source": [
        "for i in range(0,len(correlated_features)):\n",
        "  correlated_features[i][0]=dic[i]\n",
        "for i in correlated_features:\n",
        "  print(i)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Age']\n",
            "['GP']\n",
            "['MPG', 'Minpercent', 'PPG']\n",
            "['Minpercent', 'MPG', 'PPG']\n",
            "['USG']\n",
            "['TOR']\n",
            "['FTA', '2PA', 'PPG']\n",
            "['FTpercent']\n",
            "['2PA', 'FTA', 'PPG']\n",
            "['2Ppercent']\n",
            "['3PA']\n",
            "['3Ppercent']\n",
            "['Effective Shooting percent', 'True shooting percent']\n",
            "['True shooting percent', 'Effective Shooting percent']\n",
            "['PPG', 'MPG', 'Minpercent', 'FTA', '2PA', 'TOPGTurnovers', 'Rating']\n",
            "['RPG']\n",
            "['TRB']\n",
            "['APG', 'ASTpercent', 'TOPGTurnovers']\n",
            "['ASTpercent', 'APG']\n",
            "['SPG']\n",
            "['BPG']\n",
            "['TOPGTurnovers', 'PPG', 'APG']\n",
            "['Versatility Index']\n",
            "['ORTG']\n",
            "['DRTG']\n",
            "['Rating', 'PPG']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhAlC9r_PnVu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "71a9b9c0-3536-487b-c4fe-1144cc3b5791"
      },
      "source": [
        "l1=[]\n",
        "for i in sorted(correlation_matrix.iloc[25,:]):\n",
        "  for j in range(0,26):\n",
        "    if correlation_matrix.iloc[25,j]==i:\n",
        "      l1.append(dic[j])\n",
        "for i in range(len(l1)-1,-1,-1):\n",
        "  print(l1[i])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rating\n",
            "PPG\n",
            "TOPGTurnovers\n",
            "FTA\n",
            "MPG\n",
            "Minpercent\n",
            "2PA\n",
            "USG\n",
            "Versatility Index\n",
            "APG\n",
            "RPG\n",
            "SPG\n",
            "3PA\n",
            "ASTpercent\n",
            "BPG\n",
            "GP\n",
            "True shooting percent\n",
            "ORTG\n",
            "Age\n",
            "TRB\n",
            "FTpercent\n",
            "Effective Shooting percent\n",
            "2Ppercent\n",
            "3Ppercent\n",
            "DRTG\n",
            "TOR\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty20dk71pwDV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import preprocessing\n",
        "mm_scaler = preprocessing.MinMaxScaler()\n",
        "df_mm = mm_scaler.fit_transform(Xdf.astype(float))\n",
        "\n",
        "df_mm = pd.DataFrame(df_mm, columns=Xdf.columns)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXn_Ve8Tpx4i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "89d4374f-439e-4063-b797-8cf7538994c6"
      },
      "source": [
        "df_mm.head()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age</th>\n",
              "      <th>GP</th>\n",
              "      <th>MPG</th>\n",
              "      <th>Minpercent</th>\n",
              "      <th>USG</th>\n",
              "      <th>TOR</th>\n",
              "      <th>FTA</th>\n",
              "      <th>FTpercent</th>\n",
              "      <th>2PA</th>\n",
              "      <th>2Ppercent</th>\n",
              "      <th>3PA</th>\n",
              "      <th>3Ppercent</th>\n",
              "      <th>Effective Shooting percent</th>\n",
              "      <th>True shooting percent</th>\n",
              "      <th>PPG</th>\n",
              "      <th>RPG</th>\n",
              "      <th>TRB</th>\n",
              "      <th>APG</th>\n",
              "      <th>ASTpercent</th>\n",
              "      <th>SPG</th>\n",
              "      <th>BPG</th>\n",
              "      <th>TOPGTurnovers</th>\n",
              "      <th>Versatility Index</th>\n",
              "      <th>ORTG</th>\n",
              "      <th>DRTG</th>\n",
              "      <th>Rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.603188</td>\n",
              "      <td>0.295775</td>\n",
              "      <td>0.025157</td>\n",
              "      <td>0.058309</td>\n",
              "      <td>0.355140</td>\n",
              "      <td>0.003757</td>\n",
              "      <td>0.012821</td>\n",
              "      <td>0.727</td>\n",
              "      <td>0.027206</td>\n",
              "      <td>0.447</td>\n",
              "      <td>0.030156</td>\n",
              "      <td>0.355</td>\n",
              "      <td>0.576857</td>\n",
              "      <td>0.529644</td>\n",
              "      <td>0.047887</td>\n",
              "      <td>0.011059</td>\n",
              "      <td>0.196226</td>\n",
              "      <td>0.047170</td>\n",
              "      <td>0.301205</td>\n",
              "      <td>0.080851</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.067460</td>\n",
              "      <td>0.387097</td>\n",
              "      <td>0.699932</td>\n",
              "      <td>0.518703</td>\n",
              "      <td>0.242424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.129255</td>\n",
              "      <td>0.661972</td>\n",
              "      <td>0.874214</td>\n",
              "      <td>0.879009</td>\n",
              "      <td>0.507788</td>\n",
              "      <td>0.002890</td>\n",
              "      <td>0.262238</td>\n",
              "      <td>0.698</td>\n",
              "      <td>0.383824</td>\n",
              "      <td>0.497</td>\n",
              "      <td>0.332685</td>\n",
              "      <td>0.336</td>\n",
              "      <td>0.601036</td>\n",
              "      <td>0.573123</td>\n",
              "      <td>0.478873</td>\n",
              "      <td>0.483199</td>\n",
              "      <td>0.498113</td>\n",
              "      <td>0.207547</td>\n",
              "      <td>0.234940</td>\n",
              "      <td>0.434043</td>\n",
              "      <td>0.289963</td>\n",
              "      <td>0.365079</td>\n",
              "      <td>0.535484</td>\n",
              "      <td>0.703327</td>\n",
              "      <td>0.663342</td>\n",
              "      <td>0.393939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.215424</td>\n",
              "      <td>0.521127</td>\n",
              "      <td>0.182390</td>\n",
              "      <td>0.208455</td>\n",
              "      <td>0.271028</td>\n",
              "      <td>0.004913</td>\n",
              "      <td>0.045455</td>\n",
              "      <td>0.590</td>\n",
              "      <td>0.061029</td>\n",
              "      <td>0.321</td>\n",
              "      <td>0.063230</td>\n",
              "      <td>0.354</td>\n",
              "      <td>0.450777</td>\n",
              "      <td>0.393281</td>\n",
              "      <td>0.067606</td>\n",
              "      <td>0.074862</td>\n",
              "      <td>0.279245</td>\n",
              "      <td>0.037736</td>\n",
              "      <td>0.144578</td>\n",
              "      <td>0.131915</td>\n",
              "      <td>0.078067</td>\n",
              "      <td>0.140873</td>\n",
              "      <td>0.309677</td>\n",
              "      <td>0.564155</td>\n",
              "      <td>0.486284</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.517019</td>\n",
              "      <td>0.859155</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.839650</td>\n",
              "      <td>0.311526</td>\n",
              "      <td>0.004075</td>\n",
              "      <td>0.139860</td>\n",
              "      <td>0.783</td>\n",
              "      <td>0.386765</td>\n",
              "      <td>0.514</td>\n",
              "      <td>0.219844</td>\n",
              "      <td>0.429</td>\n",
              "      <td>0.692573</td>\n",
              "      <td>0.662055</td>\n",
              "      <td>0.346479</td>\n",
              "      <td>0.451297</td>\n",
              "      <td>0.479245</td>\n",
              "      <td>0.433962</td>\n",
              "      <td>0.473896</td>\n",
              "      <td>0.255319</td>\n",
              "      <td>0.401487</td>\n",
              "      <td>0.363095</td>\n",
              "      <td>0.625806</td>\n",
              "      <td>0.783435</td>\n",
              "      <td>0.518703</td>\n",
              "      <td>0.545455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.603188</td>\n",
              "      <td>0.352113</td>\n",
              "      <td>0.261006</td>\n",
              "      <td>0.287172</td>\n",
              "      <td>0.442368</td>\n",
              "      <td>0.002486</td>\n",
              "      <td>0.041958</td>\n",
              "      <td>0.833</td>\n",
              "      <td>0.150000</td>\n",
              "      <td>0.541</td>\n",
              "      <td>0.002918</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.659758</td>\n",
              "      <td>0.638340</td>\n",
              "      <td>0.180282</td>\n",
              "      <td>0.234368</td>\n",
              "      <td>0.630189</td>\n",
              "      <td>0.066038</td>\n",
              "      <td>0.202811</td>\n",
              "      <td>0.187234</td>\n",
              "      <td>0.237918</td>\n",
              "      <td>0.115079</td>\n",
              "      <td>0.541935</td>\n",
              "      <td>0.766463</td>\n",
              "      <td>0.356608</td>\n",
              "      <td>0.303030</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Age        GP       MPG  ...      ORTG      DRTG    Rating\n",
              "0  0.603188  0.295775  0.025157  ...  0.699932  0.518703  0.242424\n",
              "1  0.129255  0.661972  0.874214  ...  0.703327  0.663342  0.393939\n",
              "2  0.215424  0.521127  0.182390  ...  0.564155  0.486284  0.090909\n",
              "3  0.517019  0.859155  0.833333  ...  0.783435  0.518703  0.545455\n",
              "4  0.603188  0.352113  0.261006  ...  0.766463  0.356608  0.303030\n",
              "\n",
              "[5 rows x 26 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23mf8YIyrKM4",
        "colab_type": "text"
      },
      "source": [
        "We use the features most correlated to the Rating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LH7P5ngUzVwG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cols=['PPG','TOPGTurnovers','FTA','MPG','2PA','USG','Versatility Index','APG','RPG','SPG','3PA','BPG','GP','True shooting percent']"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jiPXtEu0OFr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_mm=df_mm[cols]"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dn4K1Z40RNs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "74f10dfb-0fba-4d9c-88bf-352dee6f17bc"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PbtIJr60sI6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_trainForBestFeatures, X_testForBestFeatures, y_trainForBestFeatures, y_testForBestFeatures =\\\n",
        "    train_test_split(df_mm, y, \n",
        "                     test_size=0.2, \n",
        "                     random_state=0, \n",
        "                )"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3xmxtUV1Jsr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "94aca8b2-8236-4868-dcc2-49cadb4bfb9c"
      },
      "source": [
        "X_trainForBestFeatures.shape, X_testForBestFeatures.shape, y_trainForBestFeatures.shape, y_testForBestFeatures.shape"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((615, 14), (154, 14), (615,), (154,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwFXXpN8WN-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from tensorflow import keras"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVzUpARhNr69",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "737124da-121b-4c87-ca28-84f56c4828df"
      },
      "source": [
        "NN_model = Sequential()\n",
        "\n",
        "# The Input Layer :\n",
        "NN_model.add(Dense(128, kernel_initializer='normal',input_dim = X_trainForBestFeatures.shape[1], activation='relu'))\n",
        "\n",
        "# The Hidden Layers :\n",
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(128, kernel_initializer='normal',activation='relu'))\n",
        "# The Output Layer :\n",
        "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
        "\n",
        "# Compile the network :\n",
        "# opt = keras.optimizers.Adam(learning_rate=0.01)\n",
        "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
        "NN_model.summary()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 128)               1920      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 199,553\n",
            "Trainable params: 199,553\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8DVMJaBVpKZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
        "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEA6xMrxVvU7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7c664b23-9aeb-4620-9abd-ccb484976c2b"
      },
      "source": [
        "NN_model.fit(X_trainForBestFeatures, y_trainForBestFeatures, epochs=300, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 75.8671 - mean_absolute_error: 75.8671\n",
            "Epoch 00001: val_loss improved from inf to 69.32671, saving model to Weights-001--69.32671.hdf5\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 75.1565 - mean_absolute_error: 75.1565 - val_loss: 69.3267 - val_mean_absolute_error: 69.3267\n",
            "Epoch 2/300\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 43.7114 - mean_absolute_error: 43.7114\n",
            "Epoch 00002: val_loss improved from 69.32671 to 15.67557, saving model to Weights-002--15.67557.hdf5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 41.1220 - mean_absolute_error: 41.1220 - val_loss: 15.6756 - val_mean_absolute_error: 15.6756\n",
            "Epoch 3/300\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 14.4434 - mean_absolute_error: 14.4434\n",
            "Epoch 00003: val_loss improved from 15.67557 to 12.67177, saving model to Weights-003--12.67177.hdf5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 14.4032 - mean_absolute_error: 14.4032 - val_loss: 12.6718 - val_mean_absolute_error: 12.6718\n",
            "Epoch 4/300\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 11.6673 - mean_absolute_error: 11.6673\n",
            "Epoch 00004: val_loss improved from 12.67177 to 8.71189, saving model to Weights-004--8.71189.hdf5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 10.5194 - mean_absolute_error: 10.5194 - val_loss: 8.7119 - val_mean_absolute_error: 8.7119\n",
            "Epoch 5/300\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 6.2666 - mean_absolute_error: 6.2666\n",
            "Epoch 00005: val_loss improved from 8.71189 to 4.57913, saving model to Weights-005--4.57913.hdf5\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 6.0644 - mean_absolute_error: 6.0644 - val_loss: 4.5791 - val_mean_absolute_error: 4.5791\n",
            "Epoch 6/300\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 4.4016 - mean_absolute_error: 4.4016\n",
            "Epoch 00006: val_loss improved from 4.57913 to 4.06939, saving model to Weights-006--4.06939.hdf5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 4.3555 - mean_absolute_error: 4.3555 - val_loss: 4.0694 - val_mean_absolute_error: 4.0694\n",
            "Epoch 7/300\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 3.7997 - mean_absolute_error: 3.7997\n",
            "Epoch 00007: val_loss improved from 4.06939 to 3.83889, saving model to Weights-007--3.83889.hdf5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 3.7656 - mean_absolute_error: 3.7656 - val_loss: 3.8389 - val_mean_absolute_error: 3.8389\n",
            "Epoch 8/300\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 3.6200 - mean_absolute_error: 3.6200\n",
            "Epoch 00008: val_loss improved from 3.83889 to 3.54220, saving model to Weights-008--3.54220.hdf5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 3.6289 - mean_absolute_error: 3.6289 - val_loss: 3.5422 - val_mean_absolute_error: 3.5422\n",
            "Epoch 9/300\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 4.1336 - mean_absolute_error: 4.1336\n",
            "Epoch 00009: val_loss did not improve from 3.54220\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 4.1213 - mean_absolute_error: 4.1213 - val_loss: 5.4462 - val_mean_absolute_error: 5.4462\n",
            "Epoch 10/300\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 3.7519 - mean_absolute_error: 3.7519\n",
            "Epoch 00010: val_loss improved from 3.54220 to 3.42410, saving model to Weights-010--3.42410.hdf5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 3.7220 - mean_absolute_error: 3.7220 - val_loss: 3.4241 - val_mean_absolute_error: 3.4241\n",
            "Epoch 11/300\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 3.9156 - mean_absolute_error: 3.9156\n",
            "Epoch 00011: val_loss improved from 3.42410 to 3.40442, saving model to Weights-011--3.40442.hdf5\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 3.8171 - mean_absolute_error: 3.8171 - val_loss: 3.4044 - val_mean_absolute_error: 3.4044\n",
            "Epoch 12/300\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 3.3726 - mean_absolute_error: 3.3726\n",
            "Epoch 00012: val_loss did not improve from 3.40442\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 3.3477 - mean_absolute_error: 3.3477 - val_loss: 4.6258 - val_mean_absolute_error: 4.6258\n",
            "Epoch 13/300\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 3.2927 - mean_absolute_error: 3.2927\n",
            "Epoch 00013: val_loss improved from 3.40442 to 3.13251, saving model to Weights-013--3.13251.hdf5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 3.2718 - mean_absolute_error: 3.2718 - val_loss: 3.1325 - val_mean_absolute_error: 3.1325\n",
            "Epoch 14/300\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 2.9863 - mean_absolute_error: 2.9863\n",
            "Epoch 00014: val_loss did not improve from 3.13251\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.9902 - mean_absolute_error: 2.9902 - val_loss: 3.9273 - val_mean_absolute_error: 3.9273\n",
            "Epoch 15/300\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 3.3227 - mean_absolute_error: 3.3227\n",
            "Epoch 00015: val_loss improved from 3.13251 to 3.09882, saving model to Weights-015--3.09882.hdf5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 3.3156 - mean_absolute_error: 3.3156 - val_loss: 3.0988 - val_mean_absolute_error: 3.0988\n",
            "Epoch 16/300\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 3.5813 - mean_absolute_error: 3.5813\n",
            "Epoch 00016: val_loss did not improve from 3.09882\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 3.5277 - mean_absolute_error: 3.5277 - val_loss: 3.2621 - val_mean_absolute_error: 3.2621\n",
            "Epoch 17/300\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 3.1313 - mean_absolute_error: 3.1313\n",
            "Epoch 00017: val_loss improved from 3.09882 to 2.89604, saving model to Weights-017--2.89604.hdf5\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 3.1259 - mean_absolute_error: 3.1259 - val_loss: 2.8960 - val_mean_absolute_error: 2.8960\n",
            "Epoch 18/300\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 2.6233 - mean_absolute_error: 2.6233\n",
            "Epoch 00018: val_loss improved from 2.89604 to 2.80573, saving model to Weights-018--2.80573.hdf5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 2.6794 - mean_absolute_error: 2.6794 - val_loss: 2.8057 - val_mean_absolute_error: 2.8057\n",
            "Epoch 19/300\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.6189 - mean_absolute_error: 2.6189\n",
            "Epoch 00019: val_loss improved from 2.80573 to 2.71613, saving model to Weights-019--2.71613.hdf5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 2.6104 - mean_absolute_error: 2.6104 - val_loss: 2.7161 - val_mean_absolute_error: 2.7161\n",
            "Epoch 20/300\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 2.7540 - mean_absolute_error: 2.7540\n",
            "Epoch 00020: val_loss did not improve from 2.71613\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 2.6954 - mean_absolute_error: 2.6954 - val_loss: 2.7929 - val_mean_absolute_error: 2.7929\n",
            "Epoch 21/300\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 2.6490 - mean_absolute_error: 2.6490\n",
            "Epoch 00021: val_loss did not improve from 2.71613\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.7579 - mean_absolute_error: 2.7579 - val_loss: 3.6252 - val_mean_absolute_error: 3.6252\n",
            "Epoch 22/300\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 3.4521 - mean_absolute_error: 3.4521\n",
            "Epoch 00022: val_loss did not improve from 2.71613\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 3.4393 - mean_absolute_error: 3.4393 - val_loss: 3.3079 - val_mean_absolute_error: 3.3079\n",
            "Epoch 23/300\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 3.1298 - mean_absolute_error: 3.1298\n",
            "Epoch 00023: val_loss did not improve from 2.71613\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 3.1151 - mean_absolute_error: 3.1151 - val_loss: 3.0558 - val_mean_absolute_error: 3.0558\n",
            "Epoch 24/300\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 2.8702 - mean_absolute_error: 2.8702\n",
            "Epoch 00024: val_loss did not improve from 2.71613\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 2.9155 - mean_absolute_error: 2.9155 - val_loss: 3.1278 - val_mean_absolute_error: 3.1278\n",
            "Epoch 25/300\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 3.2944 - mean_absolute_error: 3.2944\n",
            "Epoch 00025: val_loss improved from 2.71613 to 2.69962, saving model to Weights-025--2.69962.hdf5\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 3.1976 - mean_absolute_error: 3.1976 - val_loss: 2.6996 - val_mean_absolute_error: 2.6996\n",
            "Epoch 26/300\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 2.6710 - mean_absolute_error: 2.6710\n",
            "Epoch 00026: val_loss improved from 2.69962 to 2.40846, saving model to Weights-026--2.40846.hdf5\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 2.6743 - mean_absolute_error: 2.6743 - val_loss: 2.4085 - val_mean_absolute_error: 2.4085\n",
            "Epoch 27/300\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 2.4659 - mean_absolute_error: 2.4659\n",
            "Epoch 00027: val_loss improved from 2.40846 to 2.40538, saving model to Weights-027--2.40538.hdf5\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 2.4897 - mean_absolute_error: 2.4897 - val_loss: 2.4054 - val_mean_absolute_error: 2.4054\n",
            "Epoch 28/300\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 2.5553 - mean_absolute_error: 2.5553\n",
            "Epoch 00028: val_loss improved from 2.40538 to 2.39686, saving model to Weights-028--2.39686.hdf5\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 2.4614 - mean_absolute_error: 2.4614 - val_loss: 2.3969 - val_mean_absolute_error: 2.3969\n",
            "Epoch 29/300\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 2.4708 - mean_absolute_error: 2.4708\n",
            "Epoch 00029: val_loss improved from 2.39686 to 2.25934, saving model to Weights-029--2.25934.hdf5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 2.4666 - mean_absolute_error: 2.4666 - val_loss: 2.2593 - val_mean_absolute_error: 2.2593\n",
            "Epoch 30/300\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 2.4113 - mean_absolute_error: 2.4113\n",
            "Epoch 00030: val_loss did not improve from 2.25934\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 2.3927 - mean_absolute_error: 2.3927 - val_loss: 2.2814 - val_mean_absolute_error: 2.2814\n",
            "Epoch 31/300\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 2.7984 - mean_absolute_error: 2.7984\n",
            "Epoch 00031: val_loss did not improve from 2.25934\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 2.7681 - mean_absolute_error: 2.7681 - val_loss: 2.9088 - val_mean_absolute_error: 2.9088\n",
            "Epoch 32/300\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 3.1770 - mean_absolute_error: 3.1770\n",
            "Epoch 00032: val_loss did not improve from 2.25934\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.9425 - mean_absolute_error: 2.9425 - val_loss: 2.2639 - val_mean_absolute_error: 2.2639\n",
            "Epoch 33/300\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 2.4838 - mean_absolute_error: 2.4838\n",
            "Epoch 00033: val_loss did not improve from 2.25934\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 2.4977 - mean_absolute_error: 2.4977 - val_loss: 2.3080 - val_mean_absolute_error: 2.3080\n",
            "Epoch 34/300\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 2.4009 - mean_absolute_error: 2.4009\n",
            "Epoch 00034: val_loss improved from 2.25934 to 2.16526, saving model to Weights-034--2.16526.hdf5\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 2.3767 - mean_absolute_error: 2.3767 - val_loss: 2.1653 - val_mean_absolute_error: 2.1653\n",
            "Epoch 35/300\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 2.3543 - mean_absolute_error: 2.3543\n",
            "Epoch 00035: val_loss did not improve from 2.16526\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.3865 - mean_absolute_error: 2.3865 - val_loss: 3.1486 - val_mean_absolute_error: 3.1486\n",
            "Epoch 36/300\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 2.8336 - mean_absolute_error: 2.8336\n",
            "Epoch 00036: val_loss improved from 2.16526 to 2.11507, saving model to Weights-036--2.11507.hdf5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 2.7467 - mean_absolute_error: 2.7467 - val_loss: 2.1151 - val_mean_absolute_error: 2.1151\n",
            "Epoch 37/300\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.3619 - mean_absolute_error: 2.3619\n",
            "Epoch 00037: val_loss did not improve from 2.11507\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.3591 - mean_absolute_error: 2.3591 - val_loss: 2.5725 - val_mean_absolute_error: 2.5725\n",
            "Epoch 38/300\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 2.6615 - mean_absolute_error: 2.6615\n",
            "Epoch 00038: val_loss did not improve from 2.11507\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.6752 - mean_absolute_error: 2.6752 - val_loss: 2.2651 - val_mean_absolute_error: 2.2651\n",
            "Epoch 39/300\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.5108 - mean_absolute_error: 2.5108\n",
            "Epoch 00039: val_loss improved from 2.11507 to 2.09146, saving model to Weights-039--2.09146.hdf5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 2.5304 - mean_absolute_error: 2.5304 - val_loss: 2.0915 - val_mean_absolute_error: 2.0915\n",
            "Epoch 40/300\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 2.4517 - mean_absolute_error: 2.4517\n",
            "Epoch 00040: val_loss did not improve from 2.09146\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 2.3775 - mean_absolute_error: 2.3775 - val_loss: 2.0993 - val_mean_absolute_error: 2.0993\n",
            "Epoch 41/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.3679 - mean_absolute_error: 2.3679\n",
            "Epoch 00041: val_loss did not improve from 2.09146\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.3679 - mean_absolute_error: 2.3679 - val_loss: 3.0012 - val_mean_absolute_error: 3.0012\n",
            "Epoch 42/300\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 2.7919 - mean_absolute_error: 2.7919\n",
            "Epoch 00042: val_loss did not improve from 2.09146\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 2.6899 - mean_absolute_error: 2.6899 - val_loss: 2.2643 - val_mean_absolute_error: 2.2643\n",
            "Epoch 43/300\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.3902 - mean_absolute_error: 2.3902\n",
            "Epoch 00043: val_loss did not improve from 2.09146\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.3907 - mean_absolute_error: 2.3907 - val_loss: 2.5408 - val_mean_absolute_error: 2.5408\n",
            "Epoch 44/300\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 2.4461 - mean_absolute_error: 2.4461\n",
            "Epoch 00044: val_loss did not improve from 2.09146\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 2.4964 - mean_absolute_error: 2.4964 - val_loss: 3.7116 - val_mean_absolute_error: 3.7116\n",
            "Epoch 45/300\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.7772 - mean_absolute_error: 2.7772\n",
            "Epoch 00045: val_loss did not improve from 2.09146\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.7660 - mean_absolute_error: 2.7660 - val_loss: 2.1027 - val_mean_absolute_error: 2.1027\n",
            "Epoch 46/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.3464 - mean_absolute_error: 2.3464\n",
            "Epoch 00046: val_loss did not improve from 2.09146\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.3464 - mean_absolute_error: 2.3464 - val_loss: 2.9574 - val_mean_absolute_error: 2.9574\n",
            "Epoch 47/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.4917 - mean_absolute_error: 2.4917\n",
            "Epoch 00047: val_loss did not improve from 2.09146\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.4917 - mean_absolute_error: 2.4917 - val_loss: 2.1549 - val_mean_absolute_error: 2.1549\n",
            "Epoch 48/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.4761 - mean_absolute_error: 2.4761\n",
            "Epoch 00048: val_loss improved from 2.09146 to 2.05923, saving model to Weights-048--2.05923.hdf5\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 2.4253 - mean_absolute_error: 2.4253 - val_loss: 2.0592 - val_mean_absolute_error: 2.0592\n",
            "Epoch 49/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.3796 - mean_absolute_error: 2.3796\n",
            "Epoch 00049: val_loss did not improve from 2.05923\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.4057 - mean_absolute_error: 2.4057 - val_loss: 2.2082 - val_mean_absolute_error: 2.2082\n",
            "Epoch 50/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.9700 - mean_absolute_error: 2.9700\n",
            "Epoch 00050: val_loss did not improve from 2.05923\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 3.3252 - mean_absolute_error: 3.3252 - val_loss: 2.0750 - val_mean_absolute_error: 2.0750\n",
            "Epoch 51/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.2354 - mean_absolute_error: 2.2354\n",
            "Epoch 00051: val_loss did not improve from 2.05923\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 3.0726 - mean_absolute_error: 3.0726 - val_loss: 2.1915 - val_mean_absolute_error: 2.1915\n",
            "Epoch 52/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.2909 - mean_absolute_error: 2.2909\n",
            "Epoch 00052: val_loss did not improve from 2.05923\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.8544 - mean_absolute_error: 2.8544 - val_loss: 2.8599 - val_mean_absolute_error: 2.8599\n",
            "Epoch 53/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 3.3787 - mean_absolute_error: 3.3787\n",
            "Epoch 00053: val_loss did not improve from 2.05923\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.5192 - mean_absolute_error: 2.5192 - val_loss: 2.4978 - val_mean_absolute_error: 2.4978\n",
            "Epoch 54/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.3522 - mean_absolute_error: 2.3522\n",
            "Epoch 00054: val_loss did not improve from 2.05923\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.5523 - mean_absolute_error: 2.5523 - val_loss: 2.1170 - val_mean_absolute_error: 2.1170\n",
            "Epoch 55/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.0404 - mean_absolute_error: 2.0404\n",
            "Epoch 00055: val_loss did not improve from 2.05923\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.3283 - mean_absolute_error: 2.3283 - val_loss: 2.6715 - val_mean_absolute_error: 2.6715\n",
            "Epoch 56/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.4327 - mean_absolute_error: 2.4327\n",
            "Epoch 00056: val_loss did not improve from 2.05923\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 3.1624 - mean_absolute_error: 3.1624 - val_loss: 2.5465 - val_mean_absolute_error: 2.5465\n",
            "Epoch 57/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 3.0708 - mean_absolute_error: 3.0708\n",
            "Epoch 00057: val_loss did not improve from 2.05923\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 3.1218 - mean_absolute_error: 3.1218 - val_loss: 2.2475 - val_mean_absolute_error: 2.2475\n",
            "Epoch 58/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.9898 - mean_absolute_error: 2.9898\n",
            "Epoch 00058: val_loss did not improve from 2.05923\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.5233 - mean_absolute_error: 2.5233 - val_loss: 2.2270 - val_mean_absolute_error: 2.2270\n",
            "Epoch 59/300\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.6702 - mean_absolute_error: 2.6702\n",
            "Epoch 00059: val_loss did not improve from 2.05923\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.6831 - mean_absolute_error: 2.6831 - val_loss: 2.1308 - val_mean_absolute_error: 2.1308\n",
            "Epoch 60/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.4666 - mean_absolute_error: 2.4666\n",
            "Epoch 00060: val_loss did not improve from 2.05923\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.6330 - mean_absolute_error: 2.6330 - val_loss: 3.1137 - val_mean_absolute_error: 3.1137\n",
            "Epoch 61/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 3.8253 - mean_absolute_error: 3.8253\n",
            "Epoch 00061: val_loss did not improve from 2.05923\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.6378 - mean_absolute_error: 2.6378 - val_loss: 2.8256 - val_mean_absolute_error: 2.8256\n",
            "Epoch 62/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 3.1433 - mean_absolute_error: 3.1433\n",
            "Epoch 00062: val_loss did not improve from 2.05923\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.6347 - mean_absolute_error: 2.6347 - val_loss: 2.4632 - val_mean_absolute_error: 2.4632\n",
            "Epoch 63/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.8338 - mean_absolute_error: 2.8338\n",
            "Epoch 00063: val_loss did not improve from 2.05923\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.4348 - mean_absolute_error: 2.4348 - val_loss: 2.1289 - val_mean_absolute_error: 2.1289\n",
            "Epoch 64/300\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 2.2928 - mean_absolute_error: 2.2928\n",
            "Epoch 00064: val_loss did not improve from 2.05923\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 2.3141 - mean_absolute_error: 2.3141 - val_loss: 2.3522 - val_mean_absolute_error: 2.3522\n",
            "Epoch 65/300\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.5180 - mean_absolute_error: 2.5180\n",
            "Epoch 00065: val_loss did not improve from 2.05923\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.5154 - mean_absolute_error: 2.5154 - val_loss: 3.5830 - val_mean_absolute_error: 3.5830\n",
            "Epoch 66/300\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 2.9053 - mean_absolute_error: 2.9053\n",
            "Epoch 00066: val_loss did not improve from 2.05923\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 2.8595 - mean_absolute_error: 2.8595 - val_loss: 2.4783 - val_mean_absolute_error: 2.4783\n",
            "Epoch 67/300\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 2.2991 - mean_absolute_error: 2.2991\n",
            "Epoch 00067: val_loss did not improve from 2.05923\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 2.2342 - mean_absolute_error: 2.2342 - val_loss: 2.1664 - val_mean_absolute_error: 2.1664\n",
            "Epoch 68/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.0419 - mean_absolute_error: 2.0419\n",
            "Epoch 00068: val_loss did not improve from 2.05923\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.5290 - mean_absolute_error: 2.5290 - val_loss: 5.0420 - val_mean_absolute_error: 5.0420\n",
            "Epoch 69/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 4.9355 - mean_absolute_error: 4.9355\n",
            "Epoch 00069: val_loss did not improve from 2.05923\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 3.2291 - mean_absolute_error: 3.2291 - val_loss: 2.0728 - val_mean_absolute_error: 2.0728\n",
            "Epoch 70/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 1.4679 - mean_absolute_error: 1.4679\n",
            "Epoch 00070: val_loss did not improve from 2.05923\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.3759 - mean_absolute_error: 2.3759 - val_loss: 2.0627 - val_mean_absolute_error: 2.0627\n",
            "Epoch 71/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.2388 - mean_absolute_error: 2.2388\n",
            "Epoch 00071: val_loss did not improve from 2.05923\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.2315 - mean_absolute_error: 2.2315 - val_loss: 2.0689 - val_mean_absolute_error: 2.0689\n",
            "Epoch 72/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 1.8956 - mean_absolute_error: 1.8956\n",
            "Epoch 00072: val_loss did not improve from 2.05923\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.5124 - mean_absolute_error: 2.5124 - val_loss: 2.2868 - val_mean_absolute_error: 2.2868\n",
            "Epoch 73/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.2915 - mean_absolute_error: 2.2915\n",
            "Epoch 00073: val_loss did not improve from 2.05923\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.6386 - mean_absolute_error: 2.6386 - val_loss: 2.2286 - val_mean_absolute_error: 2.2286\n",
            "Epoch 74/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.8622 - mean_absolute_error: 2.8622\n",
            "Epoch 00074: val_loss did not improve from 2.05923\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.4585 - mean_absolute_error: 2.4585 - val_loss: 2.1324 - val_mean_absolute_error: 2.1324\n",
            "Epoch 75/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.4310 - mean_absolute_error: 2.4310\n",
            "Epoch 00075: val_loss did not improve from 2.05923\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.4798 - mean_absolute_error: 2.4798 - val_loss: 2.2872 - val_mean_absolute_error: 2.2872\n",
            "Epoch 76/300\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 2.3882 - mean_absolute_error: 2.3882\n",
            "Epoch 00076: val_loss did not improve from 2.05923\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.3419 - mean_absolute_error: 2.3419 - val_loss: 4.3393 - val_mean_absolute_error: 4.3393\n",
            "Epoch 77/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 5.0446 - mean_absolute_error: 5.0446\n",
            "Epoch 00077: val_loss did not improve from 2.05923\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.9647 - mean_absolute_error: 2.9647 - val_loss: 3.8372 - val_mean_absolute_error: 3.8372\n",
            "Epoch 78/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 4.5631 - mean_absolute_error: 4.5631\n",
            "Epoch 00078: val_loss did not improve from 2.05923\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 3.2812 - mean_absolute_error: 3.2812 - val_loss: 2.4053 - val_mean_absolute_error: 2.4053\n",
            "Epoch 79/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.6269 - mean_absolute_error: 2.6269\n",
            "Epoch 00079: val_loss did not improve from 2.05923\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.2915 - mean_absolute_error: 2.2915 - val_loss: 2.2638 - val_mean_absolute_error: 2.2638\n",
            "Epoch 80/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 3.0516 - mean_absolute_error: 3.0516\n",
            "Epoch 00080: val_loss improved from 2.05923 to 2.03060, saving model to Weights-080--2.03060.hdf5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 2.3850 - mean_absolute_error: 2.3850 - val_loss: 2.0306 - val_mean_absolute_error: 2.0306\n",
            "Epoch 81/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.2242 - mean_absolute_error: 2.2242\n",
            "Epoch 00081: val_loss did not improve from 2.03060\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.4964 - mean_absolute_error: 2.4964 - val_loss: 2.1703 - val_mean_absolute_error: 2.1703\n",
            "Epoch 82/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.3812 - mean_absolute_error: 2.3812\n",
            "Epoch 00082: val_loss did not improve from 2.03060\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.2754 - mean_absolute_error: 2.2754 - val_loss: 2.2929 - val_mean_absolute_error: 2.2929\n",
            "Epoch 83/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.1677 - mean_absolute_error: 2.1677\n",
            "Epoch 00083: val_loss did not improve from 2.03060\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.3051 - mean_absolute_error: 2.3051 - val_loss: 2.2084 - val_mean_absolute_error: 2.2084\n",
            "Epoch 84/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.4026 - mean_absolute_error: 2.4026\n",
            "Epoch 00084: val_loss did not improve from 2.03060\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.2425 - mean_absolute_error: 2.2425 - val_loss: 2.6645 - val_mean_absolute_error: 2.6645\n",
            "Epoch 85/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.3038 - mean_absolute_error: 2.3038\n",
            "Epoch 00085: val_loss did not improve from 2.03060\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.3038 - mean_absolute_error: 2.3038 - val_loss: 2.2497 - val_mean_absolute_error: 2.2497\n",
            "Epoch 86/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.4556 - mean_absolute_error: 2.4556\n",
            "Epoch 00086: val_loss did not improve from 2.03060\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.1832 - mean_absolute_error: 2.1832 - val_loss: 2.2528 - val_mean_absolute_error: 2.2528\n",
            "Epoch 87/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.3788 - mean_absolute_error: 2.3788\n",
            "Epoch 00087: val_loss did not improve from 2.03060\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.5060 - mean_absolute_error: 2.5060 - val_loss: 2.0410 - val_mean_absolute_error: 2.0410\n",
            "Epoch 88/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 1.9024 - mean_absolute_error: 1.9024\n",
            "Epoch 00088: val_loss did not improve from 2.03060\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.4374 - mean_absolute_error: 2.4374 - val_loss: 2.0994 - val_mean_absolute_error: 2.0994\n",
            "Epoch 89/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 1.8300 - mean_absolute_error: 1.8300\n",
            "Epoch 00089: val_loss did not improve from 2.03060\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.2210 - mean_absolute_error: 2.2210 - val_loss: 2.1218 - val_mean_absolute_error: 2.1218\n",
            "Epoch 90/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 1.8293 - mean_absolute_error: 1.8293\n",
            "Epoch 00090: val_loss did not improve from 2.03060\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.4742 - mean_absolute_error: 2.4742 - val_loss: 2.1372 - val_mean_absolute_error: 2.1372\n",
            "Epoch 91/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.4375 - mean_absolute_error: 2.4375\n",
            "Epoch 00091: val_loss did not improve from 2.03060\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.4122 - mean_absolute_error: 2.4122 - val_loss: 2.0562 - val_mean_absolute_error: 2.0562\n",
            "Epoch 92/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.0288 - mean_absolute_error: 2.0288\n",
            "Epoch 00092: val_loss did not improve from 2.03060\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.4865 - mean_absolute_error: 2.4865 - val_loss: 2.0376 - val_mean_absolute_error: 2.0376\n",
            "Epoch 93/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.5637 - mean_absolute_error: 2.5637\n",
            "Epoch 00093: val_loss did not improve from 2.03060\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.5597 - mean_absolute_error: 2.5597 - val_loss: 2.1773 - val_mean_absolute_error: 2.1773\n",
            "Epoch 94/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.5649 - mean_absolute_error: 2.5649\n",
            "Epoch 00094: val_loss did not improve from 2.03060\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.9783 - mean_absolute_error: 2.9783 - val_loss: 2.8892 - val_mean_absolute_error: 2.8892\n",
            "Epoch 95/300\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 2.5151 - mean_absolute_error: 2.5151\n",
            "Epoch 00095: val_loss did not improve from 2.03060\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.5254 - mean_absolute_error: 2.5254 - val_loss: 2.1943 - val_mean_absolute_error: 2.1943\n",
            "Epoch 96/300\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.3494 - mean_absolute_error: 2.3494\n",
            "Epoch 00096: val_loss improved from 2.03060 to 2.02100, saving model to Weights-096--2.02100.hdf5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 2.3381 - mean_absolute_error: 2.3381 - val_loss: 2.0210 - val_mean_absolute_error: 2.0210\n",
            "Epoch 97/300\n",
            " 9/16 [===============>..............] - ETA: 0s - loss: 2.1583 - mean_absolute_error: 2.1583\n",
            "Epoch 00097: val_loss did not improve from 2.02100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 2.2804 - mean_absolute_error: 2.2804 - val_loss: 2.1621 - val_mean_absolute_error: 2.1621\n",
            "Epoch 98/300\n",
            "10/16 [=================>............] - ETA: 0s - loss: 2.4540 - mean_absolute_error: 2.4540\n",
            "Epoch 00098: val_loss improved from 2.02100 to 2.01167, saving model to Weights-098--2.01167.hdf5\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 2.4112 - mean_absolute_error: 2.4112 - val_loss: 2.0117 - val_mean_absolute_error: 2.0117\n",
            "Epoch 99/300\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 2.2088 - mean_absolute_error: 2.2088\n",
            "Epoch 00099: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 2.2800 - mean_absolute_error: 2.2800 - val_loss: 2.3997 - val_mean_absolute_error: 2.3997\n",
            "Epoch 100/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 1.9249 - mean_absolute_error: 1.9249\n",
            "Epoch 00100: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.6123 - mean_absolute_error: 2.6123 - val_loss: 2.8000 - val_mean_absolute_error: 2.8000\n",
            "Epoch 101/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.8653 - mean_absolute_error: 2.8653\n",
            "Epoch 00101: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.3963 - mean_absolute_error: 2.3963 - val_loss: 2.0454 - val_mean_absolute_error: 2.0454\n",
            "Epoch 102/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.1047 - mean_absolute_error: 2.1047\n",
            "Epoch 00102: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.3019 - mean_absolute_error: 2.3019 - val_loss: 3.2476 - val_mean_absolute_error: 3.2476\n",
            "Epoch 103/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.7918 - mean_absolute_error: 2.7918\n",
            "Epoch 00103: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.4366 - mean_absolute_error: 2.4366 - val_loss: 2.0875 - val_mean_absolute_error: 2.0875\n",
            "Epoch 104/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.2803 - mean_absolute_error: 2.2803\n",
            "Epoch 00104: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.3586 - mean_absolute_error: 2.3586 - val_loss: 2.0255 - val_mean_absolute_error: 2.0255\n",
            "Epoch 105/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 1.9528 - mean_absolute_error: 1.9528\n",
            "Epoch 00105: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.2941 - mean_absolute_error: 2.2941 - val_loss: 2.0541 - val_mean_absolute_error: 2.0541\n",
            "Epoch 106/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.3560 - mean_absolute_error: 2.3560\n",
            "Epoch 00106: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.2797 - mean_absolute_error: 2.2797 - val_loss: 2.9148 - val_mean_absolute_error: 2.9148\n",
            "Epoch 107/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 3.2845 - mean_absolute_error: 3.2845\n",
            "Epoch 00107: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.6801 - mean_absolute_error: 2.6801 - val_loss: 5.5618 - val_mean_absolute_error: 5.5618\n",
            "Epoch 108/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 3.5988 - mean_absolute_error: 3.5988\n",
            "Epoch 00108: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 3.5988 - mean_absolute_error: 3.5988 - val_loss: 2.5476 - val_mean_absolute_error: 2.5476\n",
            "Epoch 109/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.3751 - mean_absolute_error: 2.3751\n",
            "Epoch 00109: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.8974 - mean_absolute_error: 2.8974 - val_loss: 4.5820 - val_mean_absolute_error: 4.5820\n",
            "Epoch 110/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 3.4430 - mean_absolute_error: 3.4430\n",
            "Epoch 00110: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 3.4430 - mean_absolute_error: 3.4430 - val_loss: 2.4646 - val_mean_absolute_error: 2.4646\n",
            "Epoch 111/300\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 3.7058 - mean_absolute_error: 3.7058\n",
            "Epoch 00111: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 3.7487 - mean_absolute_error: 3.7487 - val_loss: 4.8663 - val_mean_absolute_error: 4.8663\n",
            "Epoch 112/300\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 3.7898 - mean_absolute_error: 3.7898\n",
            "Epoch 00112: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 3.7828 - mean_absolute_error: 3.7828 - val_loss: 2.9445 - val_mean_absolute_error: 2.9445\n",
            "Epoch 113/300\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.6089 - mean_absolute_error: 2.6089\n",
            "Epoch 00113: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.6221 - mean_absolute_error: 2.6221 - val_loss: 2.3115 - val_mean_absolute_error: 2.3115\n",
            "Epoch 114/300\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.3383 - mean_absolute_error: 2.3383\n",
            "Epoch 00114: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.3331 - mean_absolute_error: 2.3331 - val_loss: 2.5800 - val_mean_absolute_error: 2.5800\n",
            "Epoch 115/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.2897 - mean_absolute_error: 2.2897\n",
            "Epoch 00115: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.2897 - mean_absolute_error: 2.2897 - val_loss: 2.7799 - val_mean_absolute_error: 2.7799\n",
            "Epoch 116/300\n",
            " 8/16 [==============>...............] - ETA: 0s - loss: 2.5850 - mean_absolute_error: 2.5850\n",
            "Epoch 00116: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.4710 - mean_absolute_error: 2.4710 - val_loss: 2.0355 - val_mean_absolute_error: 2.0355\n",
            "Epoch 117/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.2165 - mean_absolute_error: 2.2165\n",
            "Epoch 00117: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.5182 - mean_absolute_error: 2.5182 - val_loss: 3.2584 - val_mean_absolute_error: 3.2584\n",
            "Epoch 118/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.6839 - mean_absolute_error: 2.6839\n",
            "Epoch 00118: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.6839 - mean_absolute_error: 2.6839 - val_loss: 2.9050 - val_mean_absolute_error: 2.9050\n",
            "Epoch 119/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.5047 - mean_absolute_error: 2.5047\n",
            "Epoch 00119: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.5047 - mean_absolute_error: 2.5047 - val_loss: 2.2507 - val_mean_absolute_error: 2.2507\n",
            "Epoch 120/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.8349 - mean_absolute_error: 2.8349\n",
            "Epoch 00120: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.3444 - mean_absolute_error: 2.3444 - val_loss: 2.0919 - val_mean_absolute_error: 2.0919\n",
            "Epoch 121/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.2754 - mean_absolute_error: 2.2754\n",
            "Epoch 00121: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.4300 - mean_absolute_error: 2.4300 - val_loss: 2.9567 - val_mean_absolute_error: 2.9567\n",
            "Epoch 122/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.6318 - mean_absolute_error: 2.6318\n",
            "Epoch 00122: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.9128 - mean_absolute_error: 2.9128 - val_loss: 2.0691 - val_mean_absolute_error: 2.0691\n",
            "Epoch 123/300\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 2.4033 - mean_absolute_error: 2.4033\n",
            "Epoch 00123: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.3732 - mean_absolute_error: 2.3732 - val_loss: 2.3908 - val_mean_absolute_error: 2.3908\n",
            "Epoch 124/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.4306 - mean_absolute_error: 2.4306\n",
            "Epoch 00124: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.1874 - mean_absolute_error: 2.1874 - val_loss: 2.0407 - val_mean_absolute_error: 2.0407\n",
            "Epoch 125/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.1791 - mean_absolute_error: 2.1791\n",
            "Epoch 00125: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.2781 - mean_absolute_error: 2.2781 - val_loss: 2.2370 - val_mean_absolute_error: 2.2370\n",
            "Epoch 126/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 1.9441 - mean_absolute_error: 1.9441\n",
            "Epoch 00126: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.3992 - mean_absolute_error: 2.3992 - val_loss: 2.1063 - val_mean_absolute_error: 2.1063\n",
            "Epoch 127/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.3867 - mean_absolute_error: 2.3867\n",
            "Epoch 00127: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.1674 - mean_absolute_error: 2.1674 - val_loss: 2.4057 - val_mean_absolute_error: 2.4057\n",
            "Epoch 128/300\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.3849 - mean_absolute_error: 2.3849\n",
            "Epoch 00128: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.3766 - mean_absolute_error: 2.3766 - val_loss: 2.6303 - val_mean_absolute_error: 2.6303\n",
            "Epoch 129/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.6748 - mean_absolute_error: 2.6748\n",
            "Epoch 00129: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.7358 - mean_absolute_error: 2.7358 - val_loss: 2.8014 - val_mean_absolute_error: 2.8014\n",
            "Epoch 130/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.9783 - mean_absolute_error: 2.9783\n",
            "Epoch 00130: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.2331 - mean_absolute_error: 2.2331 - val_loss: 2.0974 - val_mean_absolute_error: 2.0974\n",
            "Epoch 131/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 1.7133 - mean_absolute_error: 1.7133\n",
            "Epoch 00131: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.2965 - mean_absolute_error: 2.2965 - val_loss: 2.1457 - val_mean_absolute_error: 2.1457\n",
            "Epoch 132/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.5076 - mean_absolute_error: 2.5076\n",
            "Epoch 00132: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.4547 - mean_absolute_error: 2.4547 - val_loss: 2.2940 - val_mean_absolute_error: 2.2940\n",
            "Epoch 133/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.3856 - mean_absolute_error: 2.3856\n",
            "Epoch 00133: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.2576 - mean_absolute_error: 2.2576 - val_loss: 3.0831 - val_mean_absolute_error: 3.0831\n",
            "Epoch 134/300\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 3.2366 - mean_absolute_error: 3.2366\n",
            "Epoch 00134: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 3.2584 - mean_absolute_error: 3.2584 - val_loss: 2.0643 - val_mean_absolute_error: 2.0643\n",
            "Epoch 135/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.1827 - mean_absolute_error: 2.1827\n",
            "Epoch 00135: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.3392 - mean_absolute_error: 2.3392 - val_loss: 2.5484 - val_mean_absolute_error: 2.5484\n",
            "Epoch 136/300\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.1516 - mean_absolute_error: 2.1516\n",
            "Epoch 00136: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.1832 - mean_absolute_error: 2.1832 - val_loss: 2.0463 - val_mean_absolute_error: 2.0463\n",
            "Epoch 137/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.2176 - mean_absolute_error: 2.2176\n",
            "Epoch 00137: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.2176 - mean_absolute_error: 2.2176 - val_loss: 2.0442 - val_mean_absolute_error: 2.0442\n",
            "Epoch 138/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.1559 - mean_absolute_error: 2.1559\n",
            "Epoch 00138: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.2691 - mean_absolute_error: 2.2691 - val_loss: 2.1072 - val_mean_absolute_error: 2.1072\n",
            "Epoch 139/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 1.8743 - mean_absolute_error: 1.8743\n",
            "Epoch 00139: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.2339 - mean_absolute_error: 2.2339 - val_loss: 2.0671 - val_mean_absolute_error: 2.0671\n",
            "Epoch 140/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.3153 - mean_absolute_error: 2.3153\n",
            "Epoch 00140: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.7323 - mean_absolute_error: 2.7323 - val_loss: 3.0344 - val_mean_absolute_error: 3.0344\n",
            "Epoch 141/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 3.4680 - mean_absolute_error: 3.4680\n",
            "Epoch 00141: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.4678 - mean_absolute_error: 2.4678 - val_loss: 2.1085 - val_mean_absolute_error: 2.1085\n",
            "Epoch 142/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.2113 - mean_absolute_error: 2.2113\n",
            "Epoch 00142: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.1654 - mean_absolute_error: 2.1654 - val_loss: 2.2063 - val_mean_absolute_error: 2.2063\n",
            "Epoch 143/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.5596 - mean_absolute_error: 2.5596\n",
            "Epoch 00143: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.2420 - mean_absolute_error: 2.2420 - val_loss: 2.1896 - val_mean_absolute_error: 2.1896\n",
            "Epoch 144/300\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 2.2362 - mean_absolute_error: 2.2362\n",
            "Epoch 00144: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 2.1327 - mean_absolute_error: 2.1327 - val_loss: 2.2880 - val_mean_absolute_error: 2.2880\n",
            "Epoch 145/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.8021 - mean_absolute_error: 2.8021\n",
            "Epoch 00145: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.3378 - mean_absolute_error: 2.3378 - val_loss: 2.5397 - val_mean_absolute_error: 2.5397\n",
            "Epoch 146/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.5863 - mean_absolute_error: 2.5863\n",
            "Epoch 00146: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.7532 - mean_absolute_error: 2.7532 - val_loss: 2.2672 - val_mean_absolute_error: 2.2672\n",
            "Epoch 147/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 1.8541 - mean_absolute_error: 1.8541\n",
            "Epoch 00147: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.4592 - mean_absolute_error: 2.4592 - val_loss: 2.0619 - val_mean_absolute_error: 2.0619\n",
            "Epoch 148/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.1848 - mean_absolute_error: 2.1848\n",
            "Epoch 00148: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.1848 - mean_absolute_error: 2.1848 - val_loss: 2.2235 - val_mean_absolute_error: 2.2235\n",
            "Epoch 149/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.1520 - mean_absolute_error: 2.1520\n",
            "Epoch 00149: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.1520 - mean_absolute_error: 2.1520 - val_loss: 2.1277 - val_mean_absolute_error: 2.1277\n",
            "Epoch 150/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.2148 - mean_absolute_error: 2.2148\n",
            "Epoch 00150: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.3272 - mean_absolute_error: 2.3272 - val_loss: 2.0866 - val_mean_absolute_error: 2.0866\n",
            "Epoch 151/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.1315 - mean_absolute_error: 2.1315\n",
            "Epoch 00151: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.2194 - mean_absolute_error: 2.2194 - val_loss: 2.2444 - val_mean_absolute_error: 2.2444\n",
            "Epoch 152/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.4654 - mean_absolute_error: 2.4654\n",
            "Epoch 00152: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.2741 - mean_absolute_error: 2.2741 - val_loss: 2.0771 - val_mean_absolute_error: 2.0771\n",
            "Epoch 153/300\n",
            "10/16 [=================>............] - ETA: 0s - loss: 2.1937 - mean_absolute_error: 2.1937\n",
            "Epoch 00153: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 2.2332 - mean_absolute_error: 2.2332 - val_loss: 2.0852 - val_mean_absolute_error: 2.0852\n",
            "Epoch 154/300\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 2.5415 - mean_absolute_error: 2.5415\n",
            "Epoch 00154: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.6202 - mean_absolute_error: 2.6202 - val_loss: 2.3466 - val_mean_absolute_error: 2.3466\n",
            "Epoch 155/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.0635 - mean_absolute_error: 2.0635\n",
            "Epoch 00155: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.1967 - mean_absolute_error: 2.1967 - val_loss: 2.4425 - val_mean_absolute_error: 2.4425\n",
            "Epoch 156/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 1.9540 - mean_absolute_error: 1.9540\n",
            "Epoch 00156: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.1547 - mean_absolute_error: 2.1547 - val_loss: 2.6481 - val_mean_absolute_error: 2.6481\n",
            "Epoch 157/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.9191 - mean_absolute_error: 2.9191\n",
            "Epoch 00157: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.4831 - mean_absolute_error: 2.4831 - val_loss: 2.0919 - val_mean_absolute_error: 2.0919\n",
            "Epoch 158/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.3573 - mean_absolute_error: 2.3573\n",
            "Epoch 00158: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.3573 - mean_absolute_error: 2.3573 - val_loss: 2.6838 - val_mean_absolute_error: 2.6838\n",
            "Epoch 159/300\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.4181 - mean_absolute_error: 2.4181\n",
            "Epoch 00159: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.4232 - mean_absolute_error: 2.4232 - val_loss: 2.1886 - val_mean_absolute_error: 2.1886\n",
            "Epoch 160/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.1489 - mean_absolute_error: 2.1489\n",
            "Epoch 00160: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.1489 - mean_absolute_error: 2.1489 - val_loss: 2.1692 - val_mean_absolute_error: 2.1692\n",
            "Epoch 161/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.3621 - mean_absolute_error: 2.3621\n",
            "Epoch 00161: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.3621 - mean_absolute_error: 2.3621 - val_loss: 2.5698 - val_mean_absolute_error: 2.5698\n",
            "Epoch 162/300\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.5064 - mean_absolute_error: 2.5064\n",
            "Epoch 00162: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.5141 - mean_absolute_error: 2.5141 - val_loss: 2.1477 - val_mean_absolute_error: 2.1477\n",
            "Epoch 163/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.1326 - mean_absolute_error: 2.1326\n",
            "Epoch 00163: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.3539 - mean_absolute_error: 2.3539 - val_loss: 2.2362 - val_mean_absolute_error: 2.2362\n",
            "Epoch 164/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 1.9492 - mean_absolute_error: 1.9492\n",
            "Epoch 00164: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.0780 - mean_absolute_error: 2.0780 - val_loss: 2.3180 - val_mean_absolute_error: 2.3180\n",
            "Epoch 165/300\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.0831 - mean_absolute_error: 2.0831\n",
            "Epoch 00165: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.1174 - mean_absolute_error: 2.1174 - val_loss: 2.0614 - val_mean_absolute_error: 2.0614\n",
            "Epoch 166/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 1.5776 - mean_absolute_error: 1.5776\n",
            "Epoch 00166: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.1114 - mean_absolute_error: 2.1114 - val_loss: 2.1165 - val_mean_absolute_error: 2.1165\n",
            "Epoch 167/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 1.9117 - mean_absolute_error: 1.9117\n",
            "Epoch 00167: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.3034 - mean_absolute_error: 2.3034 - val_loss: 2.1138 - val_mean_absolute_error: 2.1138\n",
            "Epoch 168/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 1.8540 - mean_absolute_error: 1.8540\n",
            "Epoch 00168: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.3652 - mean_absolute_error: 2.3652 - val_loss: 2.3000 - val_mean_absolute_error: 2.3000\n",
            "Epoch 169/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.5344 - mean_absolute_error: 2.5344\n",
            "Epoch 00169: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.2336 - mean_absolute_error: 2.2336 - val_loss: 2.1918 - val_mean_absolute_error: 2.1918\n",
            "Epoch 170/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.3604 - mean_absolute_error: 2.3604\n",
            "Epoch 00170: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.4574 - mean_absolute_error: 2.4574 - val_loss: 2.7213 - val_mean_absolute_error: 2.7213\n",
            "Epoch 171/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.4233 - mean_absolute_error: 2.4233\n",
            "Epoch 00171: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.4900 - mean_absolute_error: 2.4900 - val_loss: 2.1401 - val_mean_absolute_error: 2.1401\n",
            "Epoch 172/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.4270 - mean_absolute_error: 2.4270\n",
            "Epoch 00172: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.6532 - mean_absolute_error: 2.6532 - val_loss: 2.1986 - val_mean_absolute_error: 2.1986\n",
            "Epoch 173/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.3840 - mean_absolute_error: 2.3840\n",
            "Epoch 00173: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.3840 - mean_absolute_error: 2.3840 - val_loss: 2.2415 - val_mean_absolute_error: 2.2415\n",
            "Epoch 174/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.5325 - mean_absolute_error: 2.5325\n",
            "Epoch 00174: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.2004 - mean_absolute_error: 2.2004 - val_loss: 2.1114 - val_mean_absolute_error: 2.1114\n",
            "Epoch 175/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 1.9053 - mean_absolute_error: 1.9053\n",
            "Epoch 00175: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.4577 - mean_absolute_error: 2.4577 - val_loss: 3.0327 - val_mean_absolute_error: 3.0327\n",
            "Epoch 176/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.3515 - mean_absolute_error: 2.3515\n",
            "Epoch 00176: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.3515 - mean_absolute_error: 2.3515 - val_loss: 2.4327 - val_mean_absolute_error: 2.4327\n",
            "Epoch 177/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.7573 - mean_absolute_error: 2.7573\n",
            "Epoch 00177: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.2962 - mean_absolute_error: 2.2962 - val_loss: 2.1172 - val_mean_absolute_error: 2.1172\n",
            "Epoch 178/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.0693 - mean_absolute_error: 2.0693\n",
            "Epoch 00178: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.4286 - mean_absolute_error: 2.4286 - val_loss: 2.1574 - val_mean_absolute_error: 2.1574\n",
            "Epoch 179/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.4032 - mean_absolute_error: 2.4032\n",
            "Epoch 00179: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.1487 - mean_absolute_error: 2.1487 - val_loss: 2.0831 - val_mean_absolute_error: 2.0831\n",
            "Epoch 180/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.3713 - mean_absolute_error: 2.3713\n",
            "Epoch 00180: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.3713 - mean_absolute_error: 2.3713 - val_loss: 2.0780 - val_mean_absolute_error: 2.0780\n",
            "Epoch 181/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.3343 - mean_absolute_error: 2.3343\n",
            "Epoch 00181: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.3343 - mean_absolute_error: 2.3343 - val_loss: 2.0579 - val_mean_absolute_error: 2.0579\n",
            "Epoch 182/300\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.3146 - mean_absolute_error: 2.3146\n",
            "Epoch 00182: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.2916 - mean_absolute_error: 2.2916 - val_loss: 2.9171 - val_mean_absolute_error: 2.9171\n",
            "Epoch 183/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.6777 - mean_absolute_error: 2.6777\n",
            "Epoch 00183: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.6777 - mean_absolute_error: 2.6777 - val_loss: 2.6398 - val_mean_absolute_error: 2.6398\n",
            "Epoch 184/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.2455 - mean_absolute_error: 2.2455\n",
            "Epoch 00184: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.2455 - mean_absolute_error: 2.2455 - val_loss: 2.0884 - val_mean_absolute_error: 2.0884\n",
            "Epoch 185/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.1610 - mean_absolute_error: 2.1610\n",
            "Epoch 00185: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.1610 - mean_absolute_error: 2.1610 - val_loss: 2.0454 - val_mean_absolute_error: 2.0454\n",
            "Epoch 186/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.1664 - mean_absolute_error: 2.1664\n",
            "Epoch 00186: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.1664 - mean_absolute_error: 2.1664 - val_loss: 2.0835 - val_mean_absolute_error: 2.0835\n",
            "Epoch 187/300\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.1848 - mean_absolute_error: 2.1848\n",
            "Epoch 00187: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.1841 - mean_absolute_error: 2.1841 - val_loss: 2.0591 - val_mean_absolute_error: 2.0591\n",
            "Epoch 188/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.6692 - mean_absolute_error: 2.6692\n",
            "Epoch 00188: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.0909 - mean_absolute_error: 2.0909 - val_loss: 2.0760 - val_mean_absolute_error: 2.0760\n",
            "Epoch 189/300\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 2.0566 - mean_absolute_error: 2.0566\n",
            "Epoch 00189: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 2.1325 - mean_absolute_error: 2.1325 - val_loss: 2.1300 - val_mean_absolute_error: 2.1300\n",
            "Epoch 190/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.4394 - mean_absolute_error: 2.4394\n",
            "Epoch 00190: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.4394 - mean_absolute_error: 2.4394 - val_loss: 2.2965 - val_mean_absolute_error: 2.2965\n",
            "Epoch 191/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.5223 - mean_absolute_error: 2.5223\n",
            "Epoch 00191: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.2252 - mean_absolute_error: 2.2252 - val_loss: 2.1951 - val_mean_absolute_error: 2.1951\n",
            "Epoch 192/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.0662 - mean_absolute_error: 2.0662\n",
            "Epoch 00192: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.0662 - mean_absolute_error: 2.0662 - val_loss: 2.1058 - val_mean_absolute_error: 2.1058\n",
            "Epoch 193/300\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 2.2034 - mean_absolute_error: 2.2034\n",
            "Epoch 00193: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 2.2425 - mean_absolute_error: 2.2425 - val_loss: 2.0768 - val_mean_absolute_error: 2.0768\n",
            "Epoch 194/300\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 2.4257 - mean_absolute_error: 2.4257\n",
            "Epoch 00194: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.4196 - mean_absolute_error: 2.4196 - val_loss: 2.6808 - val_mean_absolute_error: 2.6808\n",
            "Epoch 195/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.1631 - mean_absolute_error: 2.1631\n",
            "Epoch 00195: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.1631 - mean_absolute_error: 2.1631 - val_loss: 2.0728 - val_mean_absolute_error: 2.0728\n",
            "Epoch 196/300\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 2.2541 - mean_absolute_error: 2.2541\n",
            "Epoch 00196: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.2230 - mean_absolute_error: 2.2230 - val_loss: 2.1137 - val_mean_absolute_error: 2.1137\n",
            "Epoch 197/300\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.5087 - mean_absolute_error: 2.5087\n",
            "Epoch 00197: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.4920 - mean_absolute_error: 2.4920 - val_loss: 2.0967 - val_mean_absolute_error: 2.0967\n",
            "Epoch 198/300\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 2.1672 - mean_absolute_error: 2.1672\n",
            "Epoch 00198: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 2.2197 - mean_absolute_error: 2.2197 - val_loss: 2.1878 - val_mean_absolute_error: 2.1878\n",
            "Epoch 199/300\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 2.1299 - mean_absolute_error: 2.1299\n",
            "Epoch 00199: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.0875 - mean_absolute_error: 2.0875 - val_loss: 2.4758 - val_mean_absolute_error: 2.4758\n",
            "Epoch 200/300\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.3680 - mean_absolute_error: 2.3680\n",
            "Epoch 00200: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.4103 - mean_absolute_error: 2.4103 - val_loss: 2.7714 - val_mean_absolute_error: 2.7714\n",
            "Epoch 201/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 3.0107 - mean_absolute_error: 3.0107\n",
            "Epoch 00201: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.1195 - mean_absolute_error: 2.1195 - val_loss: 2.0958 - val_mean_absolute_error: 2.0958\n",
            "Epoch 202/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 1.6786 - mean_absolute_error: 1.6786\n",
            "Epoch 00202: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.7154 - mean_absolute_error: 2.7154 - val_loss: 2.0729 - val_mean_absolute_error: 2.0729\n",
            "Epoch 203/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.6807 - mean_absolute_error: 2.6807\n",
            "Epoch 00203: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.4110 - mean_absolute_error: 2.4110 - val_loss: 2.6233 - val_mean_absolute_error: 2.6233\n",
            "Epoch 204/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.6747 - mean_absolute_error: 2.6747\n",
            "Epoch 00204: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.3920 - mean_absolute_error: 2.3920 - val_loss: 2.1272 - val_mean_absolute_error: 2.1272\n",
            "Epoch 205/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.1764 - mean_absolute_error: 2.1764\n",
            "Epoch 00205: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.3073 - mean_absolute_error: 2.3073 - val_loss: 2.0942 - val_mean_absolute_error: 2.0942\n",
            "Epoch 206/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.1835 - mean_absolute_error: 2.1835\n",
            "Epoch 00206: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.1361 - mean_absolute_error: 2.1361 - val_loss: 2.1220 - val_mean_absolute_error: 2.1220\n",
            "Epoch 207/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.0303 - mean_absolute_error: 2.0303\n",
            "Epoch 00207: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.3918 - mean_absolute_error: 2.3918 - val_loss: 2.9069 - val_mean_absolute_error: 2.9069\n",
            "Epoch 208/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.9483 - mean_absolute_error: 2.9483\n",
            "Epoch 00208: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.5051 - mean_absolute_error: 2.5051 - val_loss: 2.3639 - val_mean_absolute_error: 2.3639\n",
            "Epoch 209/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.6427 - mean_absolute_error: 2.6427\n",
            "Epoch 00209: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.3318 - mean_absolute_error: 2.3318 - val_loss: 2.1777 - val_mean_absolute_error: 2.1777\n",
            "Epoch 210/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.0693 - mean_absolute_error: 2.0693\n",
            "Epoch 00210: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.1732 - mean_absolute_error: 2.1732 - val_loss: 2.0869 - val_mean_absolute_error: 2.0869\n",
            "Epoch 211/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 1.5709 - mean_absolute_error: 1.5709\n",
            "Epoch 00211: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.2725 - mean_absolute_error: 2.2725 - val_loss: 2.3337 - val_mean_absolute_error: 2.3337\n",
            "Epoch 212/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.3605 - mean_absolute_error: 2.3605\n",
            "Epoch 00212: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.1100 - mean_absolute_error: 2.1100 - val_loss: 2.0941 - val_mean_absolute_error: 2.0941\n",
            "Epoch 213/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 1.9872 - mean_absolute_error: 1.9872\n",
            "Epoch 00213: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.0343 - mean_absolute_error: 2.0343 - val_loss: 2.1387 - val_mean_absolute_error: 2.1387\n",
            "Epoch 214/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.3660 - mean_absolute_error: 2.3660\n",
            "Epoch 00214: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.4293 - mean_absolute_error: 2.4293 - val_loss: 2.1377 - val_mean_absolute_error: 2.1377\n",
            "Epoch 215/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.2818 - mean_absolute_error: 2.2818\n",
            "Epoch 00215: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 2.2818 - mean_absolute_error: 2.2818 - val_loss: 2.9760 - val_mean_absolute_error: 2.9760\n",
            "Epoch 216/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.4590 - mean_absolute_error: 2.4590\n",
            "Epoch 00216: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.4590 - mean_absolute_error: 2.4590 - val_loss: 2.1932 - val_mean_absolute_error: 2.1932\n",
            "Epoch 217/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.4520 - mean_absolute_error: 2.4520\n",
            "Epoch 00217: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.4520 - mean_absolute_error: 2.4520 - val_loss: 2.4293 - val_mean_absolute_error: 2.4293\n",
            "Epoch 218/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.1216 - mean_absolute_error: 2.1216\n",
            "Epoch 00218: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.3438 - mean_absolute_error: 2.3438 - val_loss: 2.0938 - val_mean_absolute_error: 2.0938\n",
            "Epoch 219/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 1.6421 - mean_absolute_error: 1.6421\n",
            "Epoch 00219: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.1800 - mean_absolute_error: 2.1800 - val_loss: 2.8310 - val_mean_absolute_error: 2.8310\n",
            "Epoch 220/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.4895 - mean_absolute_error: 2.4895\n",
            "Epoch 00220: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.5962 - mean_absolute_error: 2.5962 - val_loss: 2.3497 - val_mean_absolute_error: 2.3497\n",
            "Epoch 221/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.8344 - mean_absolute_error: 2.8344\n",
            "Epoch 00221: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.0358 - mean_absolute_error: 2.0358 - val_loss: 2.2735 - val_mean_absolute_error: 2.2735\n",
            "Epoch 222/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.1363 - mean_absolute_error: 2.1363\n",
            "Epoch 00222: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.1363 - mean_absolute_error: 2.1363 - val_loss: 2.5301 - val_mean_absolute_error: 2.5301\n",
            "Epoch 223/300\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 2.6885 - mean_absolute_error: 2.6885\n",
            "Epoch 00223: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.7157 - mean_absolute_error: 2.7157 - val_loss: 2.9447 - val_mean_absolute_error: 2.9447\n",
            "Epoch 224/300\n",
            "12/16 [=====================>........] - ETA: 0s - loss: 2.6420 - mean_absolute_error: 2.6420\n",
            "Epoch 00224: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 2.6418 - mean_absolute_error: 2.6418 - val_loss: 2.6409 - val_mean_absolute_error: 2.6409\n",
            "Epoch 225/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.7233 - mean_absolute_error: 2.7233\n",
            "Epoch 00225: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.3448 - mean_absolute_error: 2.3448 - val_loss: 2.3471 - val_mean_absolute_error: 2.3471\n",
            "Epoch 226/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.2146 - mean_absolute_error: 2.2146\n",
            "Epoch 00226: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.2146 - mean_absolute_error: 2.2146 - val_loss: 2.0372 - val_mean_absolute_error: 2.0372\n",
            "Epoch 227/300\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.2044 - mean_absolute_error: 2.2044\n",
            "Epoch 00227: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.2136 - mean_absolute_error: 2.2136 - val_loss: 2.2103 - val_mean_absolute_error: 2.2103\n",
            "Epoch 228/300\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 2.0406 - mean_absolute_error: 2.0406\n",
            "Epoch 00228: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.0739 - mean_absolute_error: 2.0739 - val_loss: 2.2369 - val_mean_absolute_error: 2.2369\n",
            "Epoch 229/300\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.1520 - mean_absolute_error: 2.1520\n",
            "Epoch 00229: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.1497 - mean_absolute_error: 2.1497 - val_loss: 2.5054 - val_mean_absolute_error: 2.5054\n",
            "Epoch 230/300\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.3242 - mean_absolute_error: 2.3242\n",
            "Epoch 00230: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.3228 - mean_absolute_error: 2.3228 - val_loss: 2.2971 - val_mean_absolute_error: 2.2971\n",
            "Epoch 231/300\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 2.1182 - mean_absolute_error: 2.1182\n",
            "Epoch 00231: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.1144 - mean_absolute_error: 2.1144 - val_loss: 2.1080 - val_mean_absolute_error: 2.1080\n",
            "Epoch 232/300\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 2.5317 - mean_absolute_error: 2.5317\n",
            "Epoch 00232: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.5350 - mean_absolute_error: 2.5350 - val_loss: 2.6272 - val_mean_absolute_error: 2.6272\n",
            "Epoch 233/300\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.2609 - mean_absolute_error: 2.2609\n",
            "Epoch 00233: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.2685 - mean_absolute_error: 2.2685 - val_loss: 2.1965 - val_mean_absolute_error: 2.1965\n",
            "Epoch 234/300\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 2.2609 - mean_absolute_error: 2.2609\n",
            "Epoch 00234: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 2.1878 - mean_absolute_error: 2.1878 - val_loss: 2.0796 - val_mean_absolute_error: 2.0796\n",
            "Epoch 235/300\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 2.0445 - mean_absolute_error: 2.0445\n",
            "Epoch 00235: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.0905 - mean_absolute_error: 2.0905 - val_loss: 2.0959 - val_mean_absolute_error: 2.0959\n",
            "Epoch 236/300\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.0411 - mean_absolute_error: 2.0411\n",
            "Epoch 00236: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.0449 - mean_absolute_error: 2.0449 - val_loss: 2.0840 - val_mean_absolute_error: 2.0840\n",
            "Epoch 237/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.3608 - mean_absolute_error: 2.3608\n",
            "Epoch 00237: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.3608 - mean_absolute_error: 2.3608 - val_loss: 2.4342 - val_mean_absolute_error: 2.4342\n",
            "Epoch 238/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.2668 - mean_absolute_error: 2.2668\n",
            "Epoch 00238: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.5340 - mean_absolute_error: 2.5340 - val_loss: 2.3785 - val_mean_absolute_error: 2.3785\n",
            "Epoch 239/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.7453 - mean_absolute_error: 2.7453\n",
            "Epoch 00239: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.8539 - mean_absolute_error: 2.8539 - val_loss: 2.3120 - val_mean_absolute_error: 2.3120\n",
            "Epoch 240/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.3735 - mean_absolute_error: 2.3735\n",
            "Epoch 00240: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.4209 - mean_absolute_error: 2.4209 - val_loss: 2.0392 - val_mean_absolute_error: 2.0392\n",
            "Epoch 241/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 1.9106 - mean_absolute_error: 1.9106\n",
            "Epoch 00241: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.1471 - mean_absolute_error: 2.1471 - val_loss: 2.0500 - val_mean_absolute_error: 2.0500\n",
            "Epoch 242/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.2341 - mean_absolute_error: 2.2341\n",
            "Epoch 00242: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.2341 - mean_absolute_error: 2.2341 - val_loss: 2.0204 - val_mean_absolute_error: 2.0204\n",
            "Epoch 243/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.1732 - mean_absolute_error: 2.1732\n",
            "Epoch 00243: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.0307 - mean_absolute_error: 2.0307 - val_loss: 2.0499 - val_mean_absolute_error: 2.0499\n",
            "Epoch 244/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.0035 - mean_absolute_error: 2.0035\n",
            "Epoch 00244: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.0035 - mean_absolute_error: 2.0035 - val_loss: 2.1077 - val_mean_absolute_error: 2.1077\n",
            "Epoch 245/300\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.0383 - mean_absolute_error: 2.0383\n",
            "Epoch 00245: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.0199 - mean_absolute_error: 2.0199 - val_loss: 2.1270 - val_mean_absolute_error: 2.1270\n",
            "Epoch 246/300\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.2536 - mean_absolute_error: 2.2536\n",
            "Epoch 00246: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 2.2519 - mean_absolute_error: 2.2519 - val_loss: 2.1011 - val_mean_absolute_error: 2.1011\n",
            "Epoch 247/300\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 2.0877 - mean_absolute_error: 2.0877\n",
            "Epoch 00247: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.0900 - mean_absolute_error: 2.0900 - val_loss: 2.0965 - val_mean_absolute_error: 2.0965\n",
            "Epoch 248/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.0856 - mean_absolute_error: 2.0856\n",
            "Epoch 00248: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.0856 - mean_absolute_error: 2.0856 - val_loss: 2.0452 - val_mean_absolute_error: 2.0452\n",
            "Epoch 249/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.1138 - mean_absolute_error: 2.1138\n",
            "Epoch 00249: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.1138 - mean_absolute_error: 2.1138 - val_loss: 2.1191 - val_mean_absolute_error: 2.1191\n",
            "Epoch 250/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.1268 - mean_absolute_error: 2.1268\n",
            "Epoch 00250: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.1268 - mean_absolute_error: 2.1268 - val_loss: 2.4241 - val_mean_absolute_error: 2.4241\n",
            "Epoch 251/300\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.2922 - mean_absolute_error: 2.2922\n",
            "Epoch 00251: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.3159 - mean_absolute_error: 2.3159 - val_loss: 2.5370 - val_mean_absolute_error: 2.5370\n",
            "Epoch 252/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.6921 - mean_absolute_error: 2.6921\n",
            "Epoch 00252: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.1006 - mean_absolute_error: 2.1006 - val_loss: 2.0439 - val_mean_absolute_error: 2.0439\n",
            "Epoch 253/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 1.7236 - mean_absolute_error: 1.7236\n",
            "Epoch 00253: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.1047 - mean_absolute_error: 2.1047 - val_loss: 2.0926 - val_mean_absolute_error: 2.0926\n",
            "Epoch 254/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 1.9230 - mean_absolute_error: 1.9230\n",
            "Epoch 00254: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.1050 - mean_absolute_error: 2.1050 - val_loss: 2.2357 - val_mean_absolute_error: 2.2357\n",
            "Epoch 255/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.0528 - mean_absolute_error: 2.0528\n",
            "Epoch 00255: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.0528 - mean_absolute_error: 2.0528 - val_loss: 2.0544 - val_mean_absolute_error: 2.0544\n",
            "Epoch 256/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.1896 - mean_absolute_error: 2.1896\n",
            "Epoch 00256: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.1896 - mean_absolute_error: 2.1896 - val_loss: 2.7912 - val_mean_absolute_error: 2.7912\n",
            "Epoch 257/300\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 2.5280 - mean_absolute_error: 2.5280\n",
            "Epoch 00257: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 2.5306 - mean_absolute_error: 2.5306 - val_loss: 2.4299 - val_mean_absolute_error: 2.4299\n",
            "Epoch 258/300\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 2.3914 - mean_absolute_error: 2.3914\n",
            "Epoch 00258: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 2.4224 - mean_absolute_error: 2.4224 - val_loss: 2.1870 - val_mean_absolute_error: 2.1870\n",
            "Epoch 259/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.6433 - mean_absolute_error: 2.6433\n",
            "Epoch 00259: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.6433 - mean_absolute_error: 2.6433 - val_loss: 2.4704 - val_mean_absolute_error: 2.4704\n",
            "Epoch 260/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.3676 - mean_absolute_error: 2.3676\n",
            "Epoch 00260: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.3676 - mean_absolute_error: 2.3676 - val_loss: 2.5320 - val_mean_absolute_error: 2.5320\n",
            "Epoch 261/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.3054 - mean_absolute_error: 2.3054\n",
            "Epoch 00261: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.5673 - mean_absolute_error: 2.5673 - val_loss: 2.1208 - val_mean_absolute_error: 2.1208\n",
            "Epoch 262/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 1.9317 - mean_absolute_error: 1.9317\n",
            "Epoch 00262: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.1482 - mean_absolute_error: 2.1482 - val_loss: 2.0184 - val_mean_absolute_error: 2.0184\n",
            "Epoch 263/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.9550 - mean_absolute_error: 1.9550\n",
            "Epoch 00263: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1.9550 - mean_absolute_error: 1.9550 - val_loss: 2.1349 - val_mean_absolute_error: 2.1349\n",
            "Epoch 264/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.4606 - mean_absolute_error: 2.4606\n",
            "Epoch 00264: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.9839 - mean_absolute_error: 1.9839 - val_loss: 2.1129 - val_mean_absolute_error: 2.1129\n",
            "Epoch 265/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 1.9419 - mean_absolute_error: 1.9419\n",
            "Epoch 00265: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.2909 - mean_absolute_error: 2.2909 - val_loss: 2.1900 - val_mean_absolute_error: 2.1900\n",
            "Epoch 266/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 1.5115 - mean_absolute_error: 1.5115\n",
            "Epoch 00266: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.3015 - mean_absolute_error: 2.3015 - val_loss: 2.0772 - val_mean_absolute_error: 2.0772\n",
            "Epoch 267/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.2248 - mean_absolute_error: 2.2248\n",
            "Epoch 00267: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.0896 - mean_absolute_error: 2.0896 - val_loss: 2.0418 - val_mean_absolute_error: 2.0418\n",
            "Epoch 268/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 1.9537 - mean_absolute_error: 1.9537\n",
            "Epoch 00268: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.2558 - mean_absolute_error: 2.2558 - val_loss: 2.0495 - val_mean_absolute_error: 2.0495\n",
            "Epoch 269/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.1224 - mean_absolute_error: 2.1224\n",
            "Epoch 00269: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.1224 - mean_absolute_error: 2.1224 - val_loss: 2.0836 - val_mean_absolute_error: 2.0836\n",
            "Epoch 270/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.1306 - mean_absolute_error: 2.1306\n",
            "Epoch 00270: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.1726 - mean_absolute_error: 2.1726 - val_loss: 2.0495 - val_mean_absolute_error: 2.0495\n",
            "Epoch 271/300\n",
            "13/16 [=======================>......] - ETA: 0s - loss: 2.1008 - mean_absolute_error: 2.1008\n",
            "Epoch 00271: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.1278 - mean_absolute_error: 2.1278 - val_loss: 2.4496 - val_mean_absolute_error: 2.4496\n",
            "Epoch 272/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 1.9532 - mean_absolute_error: 1.9532\n",
            "Epoch 00272: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.2010 - mean_absolute_error: 2.2010 - val_loss: 3.1826 - val_mean_absolute_error: 3.1826\n",
            "Epoch 273/300\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.5852 - mean_absolute_error: 2.5852\n",
            "Epoch 00273: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.5709 - mean_absolute_error: 2.5709 - val_loss: 2.8535 - val_mean_absolute_error: 2.8535\n",
            "Epoch 274/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.7375 - mean_absolute_error: 2.7375\n",
            "Epoch 00274: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.2068 - mean_absolute_error: 2.2068 - val_loss: 2.0900 - val_mean_absolute_error: 2.0900\n",
            "Epoch 275/300\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.1849 - mean_absolute_error: 2.1849\n",
            "Epoch 00275: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.1887 - mean_absolute_error: 2.1887 - val_loss: 2.7890 - val_mean_absolute_error: 2.7890\n",
            "Epoch 276/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.3338 - mean_absolute_error: 2.3338\n",
            "Epoch 00276: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.3338 - mean_absolute_error: 2.3338 - val_loss: 2.0280 - val_mean_absolute_error: 2.0280\n",
            "Epoch 277/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.4607 - mean_absolute_error: 2.4607\n",
            "Epoch 00277: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.4607 - mean_absolute_error: 2.4607 - val_loss: 2.2262 - val_mean_absolute_error: 2.2262\n",
            "Epoch 278/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.4974 - mean_absolute_error: 2.4974\n",
            "Epoch 00278: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.1935 - mean_absolute_error: 2.1935 - val_loss: 2.1230 - val_mean_absolute_error: 2.1230\n",
            "Epoch 279/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 1.6185 - mean_absolute_error: 1.6185\n",
            "Epoch 00279: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.0856 - mean_absolute_error: 2.0856 - val_loss: 2.2529 - val_mean_absolute_error: 2.2529\n",
            "Epoch 280/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 1.8717 - mean_absolute_error: 1.8717\n",
            "Epoch 00280: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.0235 - mean_absolute_error: 2.0235 - val_loss: 2.8019 - val_mean_absolute_error: 2.8019\n",
            "Epoch 281/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 2.7450 - mean_absolute_error: 2.7450\n",
            "Epoch 00281: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.3370 - mean_absolute_error: 2.3370 - val_loss: 2.0245 - val_mean_absolute_error: 2.0245\n",
            "Epoch 282/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.9825 - mean_absolute_error: 1.9825\n",
            "Epoch 00282: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1.9825 - mean_absolute_error: 1.9825 - val_loss: 2.2149 - val_mean_absolute_error: 2.2149\n",
            "Epoch 283/300\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 2.0759 - mean_absolute_error: 2.0759\n",
            "Epoch 00283: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.0925 - mean_absolute_error: 2.0925 - val_loss: 2.3500 - val_mean_absolute_error: 2.3500\n",
            "Epoch 284/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.9828 - mean_absolute_error: 1.9828\n",
            "Epoch 00284: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1.9828 - mean_absolute_error: 1.9828 - val_loss: 2.5109 - val_mean_absolute_error: 2.5109\n",
            "Epoch 285/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.4371 - mean_absolute_error: 2.4371\n",
            "Epoch 00285: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.4371 - mean_absolute_error: 2.4371 - val_loss: 3.6099 - val_mean_absolute_error: 3.6099\n",
            "Epoch 286/300\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.4774 - mean_absolute_error: 2.4774\n",
            "Epoch 00286: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.4785 - mean_absolute_error: 2.4785 - val_loss: 2.7832 - val_mean_absolute_error: 2.7832\n",
            "Epoch 287/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.1668 - mean_absolute_error: 2.1668\n",
            "Epoch 00287: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.1668 - mean_absolute_error: 2.1668 - val_loss: 2.1126 - val_mean_absolute_error: 2.1126\n",
            "Epoch 288/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 1.8917 - mean_absolute_error: 1.8917\n",
            "Epoch 00288: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 2.0782 - mean_absolute_error: 2.0782 - val_loss: 2.2144 - val_mean_absolute_error: 2.2144\n",
            "Epoch 289/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.1063 - mean_absolute_error: 2.1063\n",
            "Epoch 00289: val_loss did not improve from 2.01167\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.1063 - mean_absolute_error: 2.1063 - val_loss: 2.0434 - val_mean_absolute_error: 2.0434\n",
            "Epoch 290/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.0207 - mean_absolute_error: 2.0207\n",
            "Epoch 00290: val_loss improved from 2.01167 to 2.00994, saving model to Weights-290--2.00994.hdf5\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 2.0207 - mean_absolute_error: 2.0207 - val_loss: 2.0099 - val_mean_absolute_error: 2.0099\n",
            "Epoch 291/300\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 2.4063 - mean_absolute_error: 2.4063\n",
            "Epoch 00291: val_loss did not improve from 2.00994\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.3801 - mean_absolute_error: 2.3801 - val_loss: 3.8723 - val_mean_absolute_error: 3.8723\n",
            "Epoch 292/300\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 3.1836 - mean_absolute_error: 3.1836\n",
            "Epoch 00292: val_loss did not improve from 2.00994\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 3.0968 - mean_absolute_error: 3.0968 - val_loss: 2.2619 - val_mean_absolute_error: 2.2619\n",
            "Epoch 293/300\n",
            "14/16 [=========================>....] - ETA: 0s - loss: 2.1493 - mean_absolute_error: 2.1493\n",
            "Epoch 00293: val_loss did not improve from 2.00994\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.1533 - mean_absolute_error: 2.1533 - val_loss: 2.0465 - val_mean_absolute_error: 2.0465\n",
            "Epoch 294/300\n",
            "11/16 [===================>..........] - ETA: 0s - loss: 2.0675 - mean_absolute_error: 2.0675\n",
            "Epoch 00294: val_loss did not improve from 2.00994\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 1.9732 - mean_absolute_error: 1.9732 - val_loss: 3.0593 - val_mean_absolute_error: 3.0593\n",
            "Epoch 295/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.5085 - mean_absolute_error: 2.5085\n",
            "Epoch 00295: val_loss improved from 2.00994 to 2.00421, saving model to Weights-295--2.00421.hdf5\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 2.5085 - mean_absolute_error: 2.5085 - val_loss: 2.0042 - val_mean_absolute_error: 2.0042\n",
            "Epoch 296/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 1.8598 - mean_absolute_error: 1.8598\n",
            "Epoch 00296: val_loss did not improve from 2.00421\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.9944 - mean_absolute_error: 1.9944 - val_loss: 2.0211 - val_mean_absolute_error: 2.0211\n",
            "Epoch 297/300\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 1.4322 - mean_absolute_error: 1.4322\n",
            "Epoch 00297: val_loss did not improve from 2.00421\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.8914 - mean_absolute_error: 1.8914 - val_loss: 2.1061 - val_mean_absolute_error: 2.1061\n",
            "Epoch 298/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.2844 - mean_absolute_error: 2.2844\n",
            "Epoch 00298: val_loss did not improve from 2.00421\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.2844 - mean_absolute_error: 2.2844 - val_loss: 2.1424 - val_mean_absolute_error: 2.1424\n",
            "Epoch 299/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.1070 - mean_absolute_error: 2.1070\n",
            "Epoch 00299: val_loss did not improve from 2.00421\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 2.1070 - mean_absolute_error: 2.1070 - val_loss: 2.0942 - val_mean_absolute_error: 2.0942\n",
            "Epoch 300/300\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.9609 - mean_absolute_error: 1.9609\n",
            "Epoch 00300: val_loss did not improve from 2.00421\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 1.9609 - mean_absolute_error: 1.9609 - val_loss: 2.1710 - val_mean_absolute_error: 2.1710\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdc5fab6b70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_O3qpBOWA_G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wights_file =  'Weights-098--2.01167.hdf5' # choose the best checkpoint \n",
        "NN_model.load_weights(wights_file) # load it\n",
        "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHMgOp_cX77s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "dc89a162-3298-4e75-b809-847d3c8a5f31"
      },
      "source": [
        "NN_model.evaluate(X_testForBestFeatures,y_testForBestFeatures)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5/5 [==============================] - 0s 2ms/step - loss: 2.6317 - mean_absolute_error: 2.6317\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.631742477416992, 2.631742477416992]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qr4UNyDDA_i6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df3=pd.read_csv('nba20.csv',index_col=0)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8KKfpkfBFIP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "0b91da2d-9cb6-41d7-fad7-40cd7ee26610"
      },
      "source": [
        "df3.head()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Team</th>\n",
              "      <th>Pos</th>\n",
              "      <th>Age</th>\n",
              "      <th>GP</th>\n",
              "      <th>MPG</th>\n",
              "      <th>Minpercent</th>\n",
              "      <th>USG</th>\n",
              "      <th>TOR</th>\n",
              "      <th>FTA</th>\n",
              "      <th>FTpercent</th>\n",
              "      <th>2PA</th>\n",
              "      <th>2Ppercent</th>\n",
              "      <th>3PA</th>\n",
              "      <th>3Ppercent</th>\n",
              "      <th>Effective Shooting percent</th>\n",
              "      <th>True shooting percent</th>\n",
              "      <th>PPG</th>\n",
              "      <th>RPG</th>\n",
              "      <th>TRB</th>\n",
              "      <th>APG</th>\n",
              "      <th>ASTpercent</th>\n",
              "      <th>SPG</th>\n",
              "      <th>BPG</th>\n",
              "      <th>TOPGTurnovers</th>\n",
              "      <th>Versatility Index</th>\n",
              "      <th>ORTG</th>\n",
              "      <th>DRTG</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>FULL NAME</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Steven Adams</th>\n",
              "      <td>Steven Adams</td>\n",
              "      <td>Okc</td>\n",
              "      <td>C</td>\n",
              "      <td>26.64</td>\n",
              "      <td>58</td>\n",
              "      <td>27.0</td>\n",
              "      <td>56.2</td>\n",
              "      <td>17.2</td>\n",
              "      <td>14.3</td>\n",
              "      <td>183</td>\n",
              "      <td>0.590</td>\n",
              "      <td>440</td>\n",
              "      <td>0.593</td>\n",
              "      <td>3</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.593</td>\n",
              "      <td>0.605</td>\n",
              "      <td>10.9</td>\n",
              "      <td>9.4</td>\n",
              "      <td>19.3</td>\n",
              "      <td>2.4</td>\n",
              "      <td>13.5</td>\n",
              "      <td>0.86</td>\n",
              "      <td>1.12</td>\n",
              "      <td>1.50</td>\n",
              "      <td>9.3</td>\n",
              "      <td>123.0</td>\n",
              "      <td>102.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Bam Adebayo</th>\n",
              "      <td>Bam Adebayo</td>\n",
              "      <td>Mia</td>\n",
              "      <td>C-F</td>\n",
              "      <td>22.65</td>\n",
              "      <td>65</td>\n",
              "      <td>34.4</td>\n",
              "      <td>71.6</td>\n",
              "      <td>20.8</td>\n",
              "      <td>17.4</td>\n",
              "      <td>342</td>\n",
              "      <td>0.690</td>\n",
              "      <td>707</td>\n",
              "      <td>0.576</td>\n",
              "      <td>13</td>\n",
              "      <td>0.077</td>\n",
              "      <td>0.567</td>\n",
              "      <td>0.605</td>\n",
              "      <td>16.2</td>\n",
              "      <td>10.5</td>\n",
              "      <td>17.2</td>\n",
              "      <td>5.1</td>\n",
              "      <td>23.6</td>\n",
              "      <td>1.20</td>\n",
              "      <td>1.31</td>\n",
              "      <td>2.83</td>\n",
              "      <td>11.1</td>\n",
              "      <td>117.3</td>\n",
              "      <td>102.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LaMarcus Aldridge</th>\n",
              "      <td>LaMarcus Aldridge</td>\n",
              "      <td>San</td>\n",
              "      <td>F-C</td>\n",
              "      <td>34.65</td>\n",
              "      <td>53</td>\n",
              "      <td>33.1</td>\n",
              "      <td>68.9</td>\n",
              "      <td>23.6</td>\n",
              "      <td>7.8</td>\n",
              "      <td>191</td>\n",
              "      <td>0.827</td>\n",
              "      <td>637</td>\n",
              "      <td>0.518</td>\n",
              "      <td>157</td>\n",
              "      <td>0.389</td>\n",
              "      <td>0.531</td>\n",
              "      <td>0.570</td>\n",
              "      <td>18.9</td>\n",
              "      <td>7.4</td>\n",
              "      <td>12.0</td>\n",
              "      <td>2.4</td>\n",
              "      <td>11.4</td>\n",
              "      <td>0.68</td>\n",
              "      <td>1.64</td>\n",
              "      <td>1.40</td>\n",
              "      <td>8.4</td>\n",
              "      <td>114.5</td>\n",
              "      <td>109.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Nickeil Alexander-Walker</th>\n",
              "      <td>Nickeil Alexander-Walker</td>\n",
              "      <td>Nor</td>\n",
              "      <td>G</td>\n",
              "      <td>21.52</td>\n",
              "      <td>41</td>\n",
              "      <td>12.2</td>\n",
              "      <td>25.4</td>\n",
              "      <td>22.8</td>\n",
              "      <td>14.3</td>\n",
              "      <td>28</td>\n",
              "      <td>0.607</td>\n",
              "      <td>110</td>\n",
              "      <td>0.336</td>\n",
              "      <td>117</td>\n",
              "      <td>0.342</td>\n",
              "      <td>0.427</td>\n",
              "      <td>0.441</td>\n",
              "      <td>5.1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8.4</td>\n",
              "      <td>1.8</td>\n",
              "      <td>20.4</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.98</td>\n",
              "      <td>8.6</td>\n",
              "      <td>90.7</td>\n",
              "      <td>106.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Grayson Allen</th>\n",
              "      <td>Grayson Allen</td>\n",
              "      <td>Mem</td>\n",
              "      <td>G</td>\n",
              "      <td>24.43</td>\n",
              "      <td>30</td>\n",
              "      <td>16.6</td>\n",
              "      <td>34.6</td>\n",
              "      <td>17.9</td>\n",
              "      <td>10.7</td>\n",
              "      <td>35</td>\n",
              "      <td>0.857</td>\n",
              "      <td>85</td>\n",
              "      <td>0.541</td>\n",
              "      <td>91</td>\n",
              "      <td>0.363</td>\n",
              "      <td>0.543</td>\n",
              "      <td>0.577</td>\n",
              "      <td>7.4</td>\n",
              "      <td>2.2</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>11.8</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.77</td>\n",
              "      <td>6.8</td>\n",
              "      <td>110.9</td>\n",
              "      <td>109.1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Name Team  ...   ORTG   DRTG\n",
              "FULL NAME                                                ...              \n",
              "Steven Adams                          Steven Adams  Okc  ...  123.0  102.2\n",
              "Bam Adebayo                            Bam Adebayo  Mia  ...  117.3  102.2\n",
              "LaMarcus Aldridge                LaMarcus Aldridge  San  ...  114.5  109.3\n",
              "Nickeil Alexander-Walker  Nickeil Alexander-Walker  Nor  ...   90.7  106.5\n",
              "Grayson Allen                        Grayson Allen  Mem  ...  110.9  109.1\n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVgLiMe5BRA8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "04c53e61-f0e0-4cfc-eb29-e8d6d1d3c636"
      },
      "source": [
        "df3.shape"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(514, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XOoSv4uCjSA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l=range(0,514)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzgiwV60DLRM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df3.index=l"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k07ggbLnDNOS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "592770cb-70b0-4a2a-8826-3257222751c2"
      },
      "source": [
        "df3.head()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Team</th>\n",
              "      <th>Pos</th>\n",
              "      <th>Age</th>\n",
              "      <th>GP</th>\n",
              "      <th>MPG</th>\n",
              "      <th>Minpercent</th>\n",
              "      <th>USG</th>\n",
              "      <th>TOR</th>\n",
              "      <th>FTA</th>\n",
              "      <th>FTpercent</th>\n",
              "      <th>2PA</th>\n",
              "      <th>2Ppercent</th>\n",
              "      <th>3PA</th>\n",
              "      <th>3Ppercent</th>\n",
              "      <th>Effective Shooting percent</th>\n",
              "      <th>True shooting percent</th>\n",
              "      <th>PPG</th>\n",
              "      <th>RPG</th>\n",
              "      <th>TRB</th>\n",
              "      <th>APG</th>\n",
              "      <th>ASTpercent</th>\n",
              "      <th>SPG</th>\n",
              "      <th>BPG</th>\n",
              "      <th>TOPGTurnovers</th>\n",
              "      <th>Versatility Index</th>\n",
              "      <th>ORTG</th>\n",
              "      <th>DRTG</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Steven Adams</td>\n",
              "      <td>Okc</td>\n",
              "      <td>C</td>\n",
              "      <td>26.64</td>\n",
              "      <td>58</td>\n",
              "      <td>27.0</td>\n",
              "      <td>56.2</td>\n",
              "      <td>17.2</td>\n",
              "      <td>14.3</td>\n",
              "      <td>183</td>\n",
              "      <td>0.590</td>\n",
              "      <td>440</td>\n",
              "      <td>0.593</td>\n",
              "      <td>3</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.593</td>\n",
              "      <td>0.605</td>\n",
              "      <td>10.9</td>\n",
              "      <td>9.4</td>\n",
              "      <td>19.3</td>\n",
              "      <td>2.4</td>\n",
              "      <td>13.5</td>\n",
              "      <td>0.86</td>\n",
              "      <td>1.12</td>\n",
              "      <td>1.50</td>\n",
              "      <td>9.3</td>\n",
              "      <td>123.0</td>\n",
              "      <td>102.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Bam Adebayo</td>\n",
              "      <td>Mia</td>\n",
              "      <td>C-F</td>\n",
              "      <td>22.65</td>\n",
              "      <td>65</td>\n",
              "      <td>34.4</td>\n",
              "      <td>71.6</td>\n",
              "      <td>20.8</td>\n",
              "      <td>17.4</td>\n",
              "      <td>342</td>\n",
              "      <td>0.690</td>\n",
              "      <td>707</td>\n",
              "      <td>0.576</td>\n",
              "      <td>13</td>\n",
              "      <td>0.077</td>\n",
              "      <td>0.567</td>\n",
              "      <td>0.605</td>\n",
              "      <td>16.2</td>\n",
              "      <td>10.5</td>\n",
              "      <td>17.2</td>\n",
              "      <td>5.1</td>\n",
              "      <td>23.6</td>\n",
              "      <td>1.20</td>\n",
              "      <td>1.31</td>\n",
              "      <td>2.83</td>\n",
              "      <td>11.1</td>\n",
              "      <td>117.3</td>\n",
              "      <td>102.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>LaMarcus Aldridge</td>\n",
              "      <td>San</td>\n",
              "      <td>F-C</td>\n",
              "      <td>34.65</td>\n",
              "      <td>53</td>\n",
              "      <td>33.1</td>\n",
              "      <td>68.9</td>\n",
              "      <td>23.6</td>\n",
              "      <td>7.8</td>\n",
              "      <td>191</td>\n",
              "      <td>0.827</td>\n",
              "      <td>637</td>\n",
              "      <td>0.518</td>\n",
              "      <td>157</td>\n",
              "      <td>0.389</td>\n",
              "      <td>0.531</td>\n",
              "      <td>0.570</td>\n",
              "      <td>18.9</td>\n",
              "      <td>7.4</td>\n",
              "      <td>12.0</td>\n",
              "      <td>2.4</td>\n",
              "      <td>11.4</td>\n",
              "      <td>0.68</td>\n",
              "      <td>1.64</td>\n",
              "      <td>1.40</td>\n",
              "      <td>8.4</td>\n",
              "      <td>114.5</td>\n",
              "      <td>109.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Nickeil Alexander-Walker</td>\n",
              "      <td>Nor</td>\n",
              "      <td>G</td>\n",
              "      <td>21.52</td>\n",
              "      <td>41</td>\n",
              "      <td>12.2</td>\n",
              "      <td>25.4</td>\n",
              "      <td>22.8</td>\n",
              "      <td>14.3</td>\n",
              "      <td>28</td>\n",
              "      <td>0.607</td>\n",
              "      <td>110</td>\n",
              "      <td>0.336</td>\n",
              "      <td>117</td>\n",
              "      <td>0.342</td>\n",
              "      <td>0.427</td>\n",
              "      <td>0.441</td>\n",
              "      <td>5.1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8.4</td>\n",
              "      <td>1.8</td>\n",
              "      <td>20.4</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.98</td>\n",
              "      <td>8.6</td>\n",
              "      <td>90.7</td>\n",
              "      <td>106.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Grayson Allen</td>\n",
              "      <td>Mem</td>\n",
              "      <td>G</td>\n",
              "      <td>24.43</td>\n",
              "      <td>30</td>\n",
              "      <td>16.6</td>\n",
              "      <td>34.6</td>\n",
              "      <td>17.9</td>\n",
              "      <td>10.7</td>\n",
              "      <td>35</td>\n",
              "      <td>0.857</td>\n",
              "      <td>85</td>\n",
              "      <td>0.541</td>\n",
              "      <td>91</td>\n",
              "      <td>0.363</td>\n",
              "      <td>0.543</td>\n",
              "      <td>0.577</td>\n",
              "      <td>7.4</td>\n",
              "      <td>2.2</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>11.8</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.77</td>\n",
              "      <td>6.8</td>\n",
              "      <td>110.9</td>\n",
              "      <td>109.1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       Name Team  Pos  ...  Versatility Index   ORTG   DRTG\n",
              "0              Steven Adams  Okc    C  ...                9.3  123.0  102.2\n",
              "1               Bam Adebayo  Mia  C-F  ...               11.1  117.3  102.2\n",
              "2         LaMarcus Aldridge  San  F-C  ...                8.4  114.5  109.3\n",
              "3  Nickeil Alexander-Walker  Nor    G  ...                8.6   90.7  106.5\n",
              "4             Grayson Allen  Mem    G  ...                6.8  110.9  109.1\n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ks1IO7MWEbH3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "f75391d0-f68b-43d3-c78e-5a0d181a25aa"
      },
      "source": [
        "df3.isnull().sum()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Name                           0\n",
              "Team                           0\n",
              "Pos                            0\n",
              "Age                            0\n",
              "GP                             0\n",
              "MPG                            0\n",
              "Minpercent                     0\n",
              "USG                            0\n",
              "TOR                            4\n",
              "FTA                            0\n",
              "FTpercent                      0\n",
              "2PA                            0\n",
              "2Ppercent                      0\n",
              "3PA                            0\n",
              "3Ppercent                      0\n",
              "Effective Shooting percent     7\n",
              "True shooting percent          5\n",
              "PPG                            0\n",
              "RPG                            0\n",
              "TRB                            0\n",
              "APG                            0\n",
              "ASTpercent                     0\n",
              "SPG                            0\n",
              "BPG                            0\n",
              "TOPGTurnovers                  0\n",
              "Versatility Index              0\n",
              "ORTG                          33\n",
              "DRTG                          33\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eImxJDbOEsLI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df3 = df3[df3['ORTG'].notna()]\n",
        "df3=df3[df3['DRTG'].notna()]\n",
        "df3=df3[df3['Effective Shooting percent'].notna()]\n",
        "df3=df3[df3['True shooting percent'].notna()]"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ow3YqaTDPzt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dftemp=df3"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xptq8UW6c8vz",
        "colab_type": "text"
      },
      "source": [
        "Statistics on Age distribution of players in the NBA. A peak is seen at around 24 years of age.\n",
        "Average age=26.91 years\n",
        "Vince Carter is the oldest player at 43.11 years.\n",
        "Sekou Doumbouya is the youngest player at only 19.22 years."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quNrcdvvc4DM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "outputId": "180d39f8-94a4-42d0-eedc-a599738106bc"
      },
      "source": [
        "#Age statistics\n",
        "print(\"Max Age:\")\n",
        "print(dftemp.Age.max())\n",
        "print(dftemp[dftemp.Age>=dftemp.Age.max()].iloc[0,[0,1]])\n",
        "print(\"Min Age:\")\n",
        "print(dftemp.Age.min())\n",
        "print(dftemp[dftemp.Age<=dftemp.Age.min()].iloc[0,[0,1]])\n",
        "print(\"Avg Age:\")\n",
        "print(dftemp.Age.mean())\n",
        "sns.kdeplot(data=dftemp.Age, shade=True)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max Age:\n",
            "43.12\n",
            "Name    Vince Carter\n",
            "Team             Atl\n",
            "Name: 86, dtype: object\n",
            "Min Age:\n",
            "19.22\n",
            "Name    Sekou Doumbouya\n",
            "Team                Det\n",
            "Name: 131, dtype: object\n",
            "Avg Age:\n",
            "26.294416666666677\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fdc5c2f3080>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nu8mT3cqeZth",
        "colab_type": "text"
      },
      "source": [
        "Ten youngest players in the NBA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqHRkmfVeVLc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "outputId": "e158e476-354d-4c82-b6a1-f4d3a507d2c5"
      },
      "source": [
        "dftemp.sort_values(by=['Age']).head(10)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Team</th>\n",
              "      <th>Pos</th>\n",
              "      <th>Age</th>\n",
              "      <th>GP</th>\n",
              "      <th>MPG</th>\n",
              "      <th>Minpercent</th>\n",
              "      <th>USG</th>\n",
              "      <th>TOR</th>\n",
              "      <th>FTA</th>\n",
              "      <th>FTpercent</th>\n",
              "      <th>2PA</th>\n",
              "      <th>2Ppercent</th>\n",
              "      <th>3PA</th>\n",
              "      <th>3Ppercent</th>\n",
              "      <th>Effective Shooting percent</th>\n",
              "      <th>True shooting percent</th>\n",
              "      <th>PPG</th>\n",
              "      <th>RPG</th>\n",
              "      <th>TRB</th>\n",
              "      <th>APG</th>\n",
              "      <th>ASTpercent</th>\n",
              "      <th>SPG</th>\n",
              "      <th>BPG</th>\n",
              "      <th>TOPGTurnovers</th>\n",
              "      <th>Versatility Index</th>\n",
              "      <th>ORTG</th>\n",
              "      <th>DRTG</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>131</th>\n",
              "      <td>Sekou Doumbouya</td>\n",
              "      <td>Det</td>\n",
              "      <td>F</td>\n",
              "      <td>19.22</td>\n",
              "      <td>38</td>\n",
              "      <td>19.8</td>\n",
              "      <td>41.3</td>\n",
              "      <td>16.9</td>\n",
              "      <td>12.0</td>\n",
              "      <td>46</td>\n",
              "      <td>0.674</td>\n",
              "      <td>138</td>\n",
              "      <td>0.464</td>\n",
              "      <td>98</td>\n",
              "      <td>0.286</td>\n",
              "      <td>0.449</td>\n",
              "      <td>0.474</td>\n",
              "      <td>6.4</td>\n",
              "      <td>3.1</td>\n",
              "      <td>8.9</td>\n",
              "      <td>0.5</td>\n",
              "      <td>3.7</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.92</td>\n",
              "      <td>4.3</td>\n",
              "      <td>90.3</td>\n",
              "      <td>105.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>441</th>\n",
              "      <td>Alen Smailagic</td>\n",
              "      <td>Gol</td>\n",
              "      <td>F</td>\n",
              "      <td>19.56</td>\n",
              "      <td>14</td>\n",
              "      <td>9.9</td>\n",
              "      <td>20.7</td>\n",
              "      <td>18.3</td>\n",
              "      <td>18.5</td>\n",
              "      <td>19</td>\n",
              "      <td>0.842</td>\n",
              "      <td>27</td>\n",
              "      <td>0.630</td>\n",
              "      <td>13</td>\n",
              "      <td>0.231</td>\n",
              "      <td>0.538</td>\n",
              "      <td>0.610</td>\n",
              "      <td>4.2</td>\n",
              "      <td>1.9</td>\n",
              "      <td>10.5</td>\n",
              "      <td>0.9</td>\n",
              "      <td>14.3</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.79</td>\n",
              "      <td>7.8</td>\n",
              "      <td>111.9</td>\n",
              "      <td>107.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>503</th>\n",
              "      <td>Zion Williamson</td>\n",
              "      <td>Nor</td>\n",
              "      <td>F</td>\n",
              "      <td>19.68</td>\n",
              "      <td>19</td>\n",
              "      <td>29.7</td>\n",
              "      <td>61.9</td>\n",
              "      <td>29.7</td>\n",
              "      <td>12.4</td>\n",
              "      <td>152</td>\n",
              "      <td>0.645</td>\n",
              "      <td>279</td>\n",
              "      <td>0.595</td>\n",
              "      <td>13</td>\n",
              "      <td>0.462</td>\n",
              "      <td>0.599</td>\n",
              "      <td>0.624</td>\n",
              "      <td>23.6</td>\n",
              "      <td>6.8</td>\n",
              "      <td>12.0</td>\n",
              "      <td>2.2</td>\n",
              "      <td>12.6</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0.47</td>\n",
              "      <td>2.68</td>\n",
              "      <td>9.5</td>\n",
              "      <td>115.2</td>\n",
              "      <td>110.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>RJ Barrett</td>\n",
              "      <td>Nyk</td>\n",
              "      <td>F-G</td>\n",
              "      <td>19.74</td>\n",
              "      <td>56</td>\n",
              "      <td>30.4</td>\n",
              "      <td>63.4</td>\n",
              "      <td>24.0</td>\n",
              "      <td>12.9</td>\n",
              "      <td>254</td>\n",
              "      <td>0.614</td>\n",
              "      <td>530</td>\n",
              "      <td>0.432</td>\n",
              "      <td>197</td>\n",
              "      <td>0.320</td>\n",
              "      <td>0.445</td>\n",
              "      <td>0.479</td>\n",
              "      <td>14.3</td>\n",
              "      <td>5.0</td>\n",
              "      <td>8.8</td>\n",
              "      <td>2.6</td>\n",
              "      <td>12.8</td>\n",
              "      <td>0.95</td>\n",
              "      <td>0.30</td>\n",
              "      <td>2.21</td>\n",
              "      <td>7.5</td>\n",
              "      <td>94.5</td>\n",
              "      <td>109.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>283</th>\n",
              "      <td>Jalen Lecque</td>\n",
              "      <td>Pho</td>\n",
              "      <td>G</td>\n",
              "      <td>19.74</td>\n",
              "      <td>4</td>\n",
              "      <td>6.6</td>\n",
              "      <td>13.8</td>\n",
              "      <td>19.1</td>\n",
              "      <td>8.4</td>\n",
              "      <td>2</td>\n",
              "      <td>1.000</td>\n",
              "      <td>6</td>\n",
              "      <td>0.667</td>\n",
              "      <td>4</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.400</td>\n",
              "      <td>0.460</td>\n",
              "      <td>2.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>10.9</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>93.9</td>\n",
              "      <td>112.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>Darius Bazley</td>\n",
              "      <td>Okc</td>\n",
              "      <td>F-G</td>\n",
              "      <td>19.75</td>\n",
              "      <td>53</td>\n",
              "      <td>17.2</td>\n",
              "      <td>35.7</td>\n",
              "      <td>14.1</td>\n",
              "      <td>13.8</td>\n",
              "      <td>47</td>\n",
              "      <td>0.681</td>\n",
              "      <td>130</td>\n",
              "      <td>0.446</td>\n",
              "      <td>100</td>\n",
              "      <td>0.300</td>\n",
              "      <td>0.448</td>\n",
              "      <td>0.475</td>\n",
              "      <td>4.5</td>\n",
              "      <td>3.7</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>4.3</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.75</td>\n",
              "      <td>4.7</td>\n",
              "      <td>90.2</td>\n",
              "      <td>105.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202</th>\n",
              "      <td>Jaxson Hayes</td>\n",
              "      <td>Nor</td>\n",
              "      <td>C-F</td>\n",
              "      <td>19.80</td>\n",
              "      <td>56</td>\n",
              "      <td>17.0</td>\n",
              "      <td>35.4</td>\n",
              "      <td>15.2</td>\n",
              "      <td>11.6</td>\n",
              "      <td>173</td>\n",
              "      <td>0.630</td>\n",
              "      <td>232</td>\n",
              "      <td>0.664</td>\n",
              "      <td>4</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.659</td>\n",
              "      <td>0.673</td>\n",
              "      <td>7.5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>12.4</td>\n",
              "      <td>0.9</td>\n",
              "      <td>7.5</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0.73</td>\n",
              "      <td>7.1</td>\n",
              "      <td>129.6</td>\n",
              "      <td>100.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>396</th>\n",
              "      <td>Kevin Porter Jr.</td>\n",
              "      <td>Cle</td>\n",
              "      <td>G-F</td>\n",
              "      <td>19.85</td>\n",
              "      <td>50</td>\n",
              "      <td>23.2</td>\n",
              "      <td>48.4</td>\n",
              "      <td>20.6</td>\n",
              "      <td>16.7</td>\n",
              "      <td>101</td>\n",
              "      <td>0.723</td>\n",
              "      <td>263</td>\n",
              "      <td>0.506</td>\n",
              "      <td>158</td>\n",
              "      <td>0.335</td>\n",
              "      <td>0.505</td>\n",
              "      <td>0.535</td>\n",
              "      <td>10.0</td>\n",
              "      <td>3.2</td>\n",
              "      <td>7.7</td>\n",
              "      <td>2.2</td>\n",
              "      <td>13.9</td>\n",
              "      <td>0.92</td>\n",
              "      <td>0.28</td>\n",
              "      <td>1.86</td>\n",
              "      <td>7.1</td>\n",
              "      <td>99.1</td>\n",
              "      <td>105.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>492</th>\n",
              "      <td>Coby White</td>\n",
              "      <td>Chi</td>\n",
              "      <td>G</td>\n",
              "      <td>20.07</td>\n",
              "      <td>65</td>\n",
              "      <td>25.8</td>\n",
              "      <td>53.7</td>\n",
              "      <td>24.4</td>\n",
              "      <td>11.4</td>\n",
              "      <td>129</td>\n",
              "      <td>0.791</td>\n",
              "      <td>416</td>\n",
              "      <td>0.430</td>\n",
              "      <td>376</td>\n",
              "      <td>0.354</td>\n",
              "      <td>0.478</td>\n",
              "      <td>0.506</td>\n",
              "      <td>13.2</td>\n",
              "      <td>3.6</td>\n",
              "      <td>7.6</td>\n",
              "      <td>2.7</td>\n",
              "      <td>16.6</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.09</td>\n",
              "      <td>1.68</td>\n",
              "      <td>7.8</td>\n",
              "      <td>99.8</td>\n",
              "      <td>108.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>291</th>\n",
              "      <td>Nassir Little</td>\n",
              "      <td>Por</td>\n",
              "      <td>F-G</td>\n",
              "      <td>20.08</td>\n",
              "      <td>48</td>\n",
              "      <td>11.9</td>\n",
              "      <td>24.8</td>\n",
              "      <td>13.7</td>\n",
              "      <td>8.1</td>\n",
              "      <td>44</td>\n",
              "      <td>0.636</td>\n",
              "      <td>92</td>\n",
              "      <td>0.554</td>\n",
              "      <td>59</td>\n",
              "      <td>0.237</td>\n",
              "      <td>0.477</td>\n",
              "      <td>0.505</td>\n",
              "      <td>3.6</td>\n",
              "      <td>2.3</td>\n",
              "      <td>9.8</td>\n",
              "      <td>0.5</td>\n",
              "      <td>5.1</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>5.4</td>\n",
              "      <td>107.8</td>\n",
              "      <td>105.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Name Team  Pos  ...  Versatility Index   ORTG   DRTG\n",
              "131   Sekou Doumbouya  Det    F  ...                4.3   90.3  105.2\n",
              "441    Alen Smailagic  Gol    F  ...                7.8  111.9  107.7\n",
              "503   Zion Williamson  Nor    F  ...                9.5  115.2  110.1\n",
              "26         RJ Barrett  Nyk  F-G  ...                7.5   94.5  109.3\n",
              "283      Jalen Lecque  Pho    G  ...                0.0   93.9  112.7\n",
              "32      Darius Bazley  Okc  F-G  ...                4.7   90.2  105.8\n",
              "202      Jaxson Hayes  Nor  C-F  ...                7.1  129.6  100.1\n",
              "396  Kevin Porter Jr.  Cle  G-F  ...                7.1   99.1  105.5\n",
              "492        Coby White  Chi    G  ...                7.8   99.8  108.3\n",
              "291     Nassir Little  Por  F-G  ...                5.4  107.8  105.2\n",
              "\n",
              "[10 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8cfR5uTfv7E",
        "colab_type": "text"
      },
      "source": [
        "Creating a new dataframe comprising solely of 'young' players:players below the age of 25"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PTWbiDPefqb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2f625d20-77a7-4ece-fc0c-ae27a47641ea"
      },
      "source": [
        "young=dftemp[dftemp.Age<25]\n",
        "young.shape"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(210, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1wdCZSCfOTS",
        "colab_type": "text"
      },
      "source": [
        "The following dataframe shows young players with the best PPG in the league"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nk_XNiy4e5lY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "41241d07-f40b-4b90-a67e-212ddb29d22a"
      },
      "source": [
        "young.loc[:,['Name','Team','PPG']].sort_values(by=['PPG'],ascending=False).head(10)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Team</th>\n",
              "      <th>PPG</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>510</th>\n",
              "      <td>Trae Young</td>\n",
              "      <td>Atl</td>\n",
              "      <td>29.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <td>Luka Doncic</td>\n",
              "      <td>Dal</td>\n",
              "      <td>28.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>462</th>\n",
              "      <td>Karl-Anthony Towns</td>\n",
              "      <td>Min</td>\n",
              "      <td>26.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>Devin Booker</td>\n",
              "      <td>Pho</td>\n",
              "      <td>26.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>233</th>\n",
              "      <td>Brandon Ingram</td>\n",
              "      <td>Nor</td>\n",
              "      <td>24.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>335</th>\n",
              "      <td>Donovan Mitchell</td>\n",
              "      <td>Uta</td>\n",
              "      <td>24.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>451</th>\n",
              "      <td>Jayson Tatum</td>\n",
              "      <td>Bos</td>\n",
              "      <td>23.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>503</th>\n",
              "      <td>Zion Williamson</td>\n",
              "      <td>Nor</td>\n",
              "      <td>23.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>John Collins</td>\n",
              "      <td>Atl</td>\n",
              "      <td>21.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>433</th>\n",
              "      <td>Collin Sexton</td>\n",
              "      <td>Cle</td>\n",
              "      <td>20.8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   Name Team   PPG\n",
              "510          Trae Young  Atl  29.6\n",
              "128         Luka Doncic  Dal  28.7\n",
              "462  Karl-Anthony Towns  Min  26.5\n",
              "52         Devin Booker  Pho  26.1\n",
              "233      Brandon Ingram  Nor  24.3\n",
              "335    Donovan Mitchell  Uta  24.2\n",
              "451        Jayson Tatum  Bos  23.6\n",
              "503     Zion Williamson  Nor  23.6\n",
              "103        John Collins  Atl  21.6\n",
              "433       Collin Sexton  Cle  20.8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEdrQtq_fgMK",
        "colab_type": "text"
      },
      "source": [
        "We next display the young players who spent most time on the court for their team, sorting them by minutes played."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSF7B5Zefq8f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "outputId": "d70bec73-4612-4688-c165-979a73143a59"
      },
      "source": [
        "young['Total Minutes']=young['MPG']*young['Minpercent']\n",
        "young.loc[:,['Name','Team','Total Minutes']].sort_values(by=['Total Minutes'],ascending=False).head(10)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Team</th>\n",
              "      <th>Total Minutes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>Devin Booker</td>\n",
              "      <td>Pho</td>\n",
              "      <td>2718.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>439</th>\n",
              "      <td>Ben Simmons</td>\n",
              "      <td>Phi</td>\n",
              "      <td>2667.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>510</th>\n",
              "      <td>Trae Young</td>\n",
              "      <td>Atl</td>\n",
              "      <td>2598.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>169</th>\n",
              "      <td>Shai Gilgeous-Alexander</td>\n",
              "      <td>Okc</td>\n",
              "      <td>2569.32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424</th>\n",
              "      <td>Domantas Sabonis</td>\n",
              "      <td>Ind</td>\n",
              "      <td>2523.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>451</th>\n",
              "      <td>Jayson Tatum</td>\n",
              "      <td>Bos</td>\n",
              "      <td>2494.66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>377</th>\n",
              "      <td>Kelly Oubre Jr.</td>\n",
              "      <td>Pho</td>\n",
              "      <td>2480.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>335</th>\n",
              "      <td>Donovan Mitchell</td>\n",
              "      <td>Uta</td>\n",
              "      <td>2463.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Bam Adebayo</td>\n",
              "      <td>Mia</td>\n",
              "      <td>2463.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>233</th>\n",
              "      <td>Brandon Ingram</td>\n",
              "      <td>Nor</td>\n",
              "      <td>2449.02</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        Name Team  Total Minutes\n",
              "52              Devin Booker  Pho        2718.33\n",
              "439              Ben Simmons  Phi        2667.10\n",
              "510               Trae Young  Atl        2598.08\n",
              "169  Shai Gilgeous-Alexander  Okc        2569.32\n",
              "424         Domantas Sabonis  Ind        2523.00\n",
              "451             Jayson Tatum  Bos        2494.66\n",
              "377          Kelly Oubre Jr.  Pho        2480.55\n",
              "335         Donovan Mitchell  Uta        2463.04\n",
              "1                Bam Adebayo  Mia        2463.04\n",
              "233           Brandon Ingram  Nor        2449.02"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Of30ThgFf-93",
        "colab_type": "text"
      },
      "source": [
        "Displaying the young players with maximum no. of assists per game"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smiaDBAgf3ld",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "39da9672-0741-4d9c-f376-cb0e948afb3f"
      },
      "source": [
        "young.loc[:,['Name','Team','APG']].sort_values(by=['APG'],ascending=False).head(10)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Team</th>\n",
              "      <th>APG</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>510</th>\n",
              "      <td>Trae Young</td>\n",
              "      <td>Atl</td>\n",
              "      <td>9.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <td>Luka Doncic</td>\n",
              "      <td>Dal</td>\n",
              "      <td>8.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>439</th>\n",
              "      <td>Ben Simmons</td>\n",
              "      <td>Phi</td>\n",
              "      <td>8.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Lonzo Ball</td>\n",
              "      <td>Nor</td>\n",
              "      <td>6.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>341</th>\n",
              "      <td>Ja Morant</td>\n",
              "      <td>Mem</td>\n",
              "      <td>6.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154</th>\n",
              "      <td>De'Aaron Fox</td>\n",
              "      <td>Sac</td>\n",
              "      <td>6.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>Devin Booker</td>\n",
              "      <td>Pho</td>\n",
              "      <td>6.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158</th>\n",
              "      <td>Markelle Fultz</td>\n",
              "      <td>Orl</td>\n",
              "      <td>5.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Bam Adebayo</td>\n",
              "      <td>Mia</td>\n",
              "      <td>5.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424</th>\n",
              "      <td>Domantas Sabonis</td>\n",
              "      <td>Ind</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Name Team  APG\n",
              "510        Trae Young  Atl  9.3\n",
              "128       Luka Doncic  Dal  8.7\n",
              "439       Ben Simmons  Phi  8.2\n",
              "22         Lonzo Ball  Nor  6.9\n",
              "341         Ja Morant  Mem  6.9\n",
              "154      De'Aaron Fox  Sac  6.7\n",
              "52       Devin Booker  Pho  6.6\n",
              "158    Markelle Fultz  Orl  5.2\n",
              "1         Bam Adebayo  Mia  5.1\n",
              "424  Domantas Sabonis  Ind  5.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DD9whIVgg5g2",
        "colab_type": "text"
      },
      "source": [
        "Displaying the young players with maximum no. of blocks per game"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_S0Ye-pgjlo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "751f55b1-e2c7-4671-82dc-53ccf4251c1a"
      },
      "source": [
        "young.loc[:,['Name','Team','BPG']].sort_values(by=['BPG'],ascending=False).head(10)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Team</th>\n",
              "      <th>BPG</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>235</th>\n",
              "      <td>Jonathan Isaac</td>\n",
              "      <td>Orl</td>\n",
              "      <td>2.44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>468</th>\n",
              "      <td>Myles Turner</td>\n",
              "      <td>Ind</td>\n",
              "      <td>2.18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400</th>\n",
              "      <td>Kristaps Porzingis</td>\n",
              "      <td>Dal</td>\n",
              "      <td>2.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>416</th>\n",
              "      <td>Mitchell Robinson</td>\n",
              "      <td>Nyk</td>\n",
              "      <td>1.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Deandre Ayton</td>\n",
              "      <td>Pho</td>\n",
              "      <td>1.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>237</th>\n",
              "      <td>Jaren Jackson Jr.</td>\n",
              "      <td>Mem</td>\n",
              "      <td>1.61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>John Collins</td>\n",
              "      <td>Atl</td>\n",
              "      <td>1.61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>392</th>\n",
              "      <td>Jakob Poeltl</td>\n",
              "      <td>San</td>\n",
              "      <td>1.47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Mo Bamba</td>\n",
              "      <td>Orl</td>\n",
              "      <td>1.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Jarrett Allen</td>\n",
              "      <td>Bro</td>\n",
              "      <td>1.33</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   Name Team   BPG\n",
              "235      Jonathan Isaac  Orl  2.44\n",
              "468        Myles Turner  Ind  2.18\n",
              "400  Kristaps Porzingis  Dal  2.08\n",
              "416   Mitchell Robinson  Nyk  1.95\n",
              "19        Deandre Ayton  Pho  1.70\n",
              "237   Jaren Jackson Jr.  Mem  1.61\n",
              "103        John Collins  Atl  1.61\n",
              "392        Jakob Poeltl  San  1.47\n",
              "23             Mo Bamba  Orl  1.40\n",
              "5         Jarrett Allen  Bro  1.33"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkuHteihhGyz",
        "colab_type": "text"
      },
      "source": [
        "Displaying the young players with maximum no. of rebounds per game"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGykOjv0hA0J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "6d020c12-cd72-4818-c232-e203752b08c1"
      },
      "source": [
        "young.loc[:,['Name','Team','RPG']].sort_values(by=['RPG'],ascending=False).head(10)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Team</th>\n",
              "      <th>RPG</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>424</th>\n",
              "      <td>Domantas Sabonis</td>\n",
              "      <td>Ind</td>\n",
              "      <td>12.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Deandre Ayton</td>\n",
              "      <td>Pho</td>\n",
              "      <td>12.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>462</th>\n",
              "      <td>Karl-Anthony Towns</td>\n",
              "      <td>Min</td>\n",
              "      <td>10.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Bam Adebayo</td>\n",
              "      <td>Mia</td>\n",
              "      <td>10.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>John Collins</td>\n",
              "      <td>Atl</td>\n",
              "      <td>10.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Jarrett Allen</td>\n",
              "      <td>Bro</td>\n",
              "      <td>9.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400</th>\n",
              "      <td>Kristaps Porzingis</td>\n",
              "      <td>Dal</td>\n",
              "      <td>9.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>Wendell Carter Jr.</td>\n",
              "      <td>Chi</td>\n",
              "      <td>9.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <td>Luka Doncic</td>\n",
              "      <td>Dal</td>\n",
              "      <td>9.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>439</th>\n",
              "      <td>Ben Simmons</td>\n",
              "      <td>Phi</td>\n",
              "      <td>7.9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   Name Team   RPG\n",
              "424    Domantas Sabonis  Ind  12.4\n",
              "19        Deandre Ayton  Pho  12.0\n",
              "462  Karl-Anthony Towns  Min  10.8\n",
              "1           Bam Adebayo  Mia  10.5\n",
              "103        John Collins  Atl  10.1\n",
              "5         Jarrett Allen  Bro   9.5\n",
              "400  Kristaps Porzingis  Dal   9.5\n",
              "84   Wendell Carter Jr.  Chi   9.4\n",
              "128         Luka Doncic  Dal   9.3\n",
              "439         Ben Simmons  Phi   7.9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmM2P2qghV47",
        "colab_type": "text"
      },
      "source": [
        "Displaying the young players with maximum no. of steals per game"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiKiE-loha_J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "6f1e3eaa-845f-4736-9790-d591a1121519"
      },
      "source": [
        "young.loc[:,['Name','Team','SPG']].sort_values(by=['SPG'],ascending=False).head(10)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Team</th>\n",
              "      <th>SPG</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>439</th>\n",
              "      <td>Ben Simmons</td>\n",
              "      <td>Phi</td>\n",
              "      <td>2.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>349</th>\n",
              "      <td>Dejounte Murray</td>\n",
              "      <td>San</td>\n",
              "      <td>1.72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>235</th>\n",
              "      <td>Jonathan Isaac</td>\n",
              "      <td>Orl</td>\n",
              "      <td>1.56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>Mikal Bridges</td>\n",
              "      <td>Pho</td>\n",
              "      <td>1.48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Lonzo Ball</td>\n",
              "      <td>Nor</td>\n",
              "      <td>1.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154</th>\n",
              "      <td>De'Aaron Fox</td>\n",
              "      <td>Sac</td>\n",
              "      <td>1.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>451</th>\n",
              "      <td>Jayson Tatum</td>\n",
              "      <td>Bos</td>\n",
              "      <td>1.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>OG Anunoby</td>\n",
              "      <td>Tor</td>\n",
              "      <td>1.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>459</th>\n",
              "      <td>Matisse Thybulle</td>\n",
              "      <td>Phi</td>\n",
              "      <td>1.39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>Donte DiVincenzo</td>\n",
              "      <td>Mil</td>\n",
              "      <td>1.37</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Name Team   SPG\n",
              "439       Ben Simmons  Phi  2.15\n",
              "349   Dejounte Murray  San  1.72\n",
              "235    Jonathan Isaac  Orl  1.56\n",
              "61      Mikal Bridges  Pho  1.48\n",
              "22         Lonzo Ball  Nor  1.45\n",
              "154      De'Aaron Fox  Sac  1.42\n",
              "451      Jayson Tatum  Bos  1.42\n",
              "15         OG Anunoby  Tor  1.41\n",
              "459  Matisse Thybulle  Phi  1.39\n",
              "127  Donte DiVincenzo  Mil  1.37"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k41fqXtIhxmP",
        "colab_type": "text"
      },
      "source": [
        "We now proceed to use our neural network trained on 2K ratings from the 2018 and 2019 seasons to determine the best young player in the current NBA season"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fnbspa5piIv2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "normal_young=mm_scaler.fit_transform(young.loc[:,cols].astype(float))\n",
        "normal_young = pd.DataFrame(normal_young, columns=cols)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9syA5K09iaYU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "resyoung=NN_model.predict(normal_young.loc[:,cols])"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "626g6aJOio7X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 702
        },
        "outputId": "c07121b3-bf9f-43c3-a016-ad0cfbef738c"
      },
      "source": [
        "young['Ratings']=resyoung\n",
        "young=young.sort_values(by=['Ratings'],ascending=False)\n",
        "young.head(10)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Team</th>\n",
              "      <th>Pos</th>\n",
              "      <th>Age</th>\n",
              "      <th>GP</th>\n",
              "      <th>MPG</th>\n",
              "      <th>Minpercent</th>\n",
              "      <th>USG</th>\n",
              "      <th>TOR</th>\n",
              "      <th>FTA</th>\n",
              "      <th>FTpercent</th>\n",
              "      <th>2PA</th>\n",
              "      <th>2Ppercent</th>\n",
              "      <th>3PA</th>\n",
              "      <th>3Ppercent</th>\n",
              "      <th>Effective Shooting percent</th>\n",
              "      <th>True shooting percent</th>\n",
              "      <th>PPG</th>\n",
              "      <th>RPG</th>\n",
              "      <th>TRB</th>\n",
              "      <th>APG</th>\n",
              "      <th>ASTpercent</th>\n",
              "      <th>SPG</th>\n",
              "      <th>BPG</th>\n",
              "      <th>TOPGTurnovers</th>\n",
              "      <th>Versatility Index</th>\n",
              "      <th>ORTG</th>\n",
              "      <th>DRTG</th>\n",
              "      <th>Total Minutes</th>\n",
              "      <th>Ratings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <td>Luka Doncic</td>\n",
              "      <td>Dal</td>\n",
              "      <td>G-F</td>\n",
              "      <td>21.03</td>\n",
              "      <td>54</td>\n",
              "      <td>33.3</td>\n",
              "      <td>69.4</td>\n",
              "      <td>37.0</td>\n",
              "      <td>14.7</td>\n",
              "      <td>491</td>\n",
              "      <td>0.752</td>\n",
              "      <td>619</td>\n",
              "      <td>0.575</td>\n",
              "      <td>491</td>\n",
              "      <td>0.318</td>\n",
              "      <td>0.532</td>\n",
              "      <td>0.584</td>\n",
              "      <td>28.7</td>\n",
              "      <td>9.3</td>\n",
              "      <td>14.6</td>\n",
              "      <td>8.7</td>\n",
              "      <td>45.3</td>\n",
              "      <td>1.06</td>\n",
              "      <td>0.19</td>\n",
              "      <td>4.22</td>\n",
              "      <td>15.9</td>\n",
              "      <td>115.6</td>\n",
              "      <td>102.9</td>\n",
              "      <td>2311.02</td>\n",
              "      <td>100.085464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>510</th>\n",
              "      <td>Trae Young</td>\n",
              "      <td>Atl</td>\n",
              "      <td>G</td>\n",
              "      <td>21.48</td>\n",
              "      <td>60</td>\n",
              "      <td>35.3</td>\n",
              "      <td>73.6</td>\n",
              "      <td>34.9</td>\n",
              "      <td>16.2</td>\n",
              "      <td>559</td>\n",
              "      <td>0.860</td>\n",
              "      <td>681</td>\n",
              "      <td>0.501</td>\n",
              "      <td>568</td>\n",
              "      <td>0.361</td>\n",
              "      <td>0.519</td>\n",
              "      <td>0.595</td>\n",
              "      <td>29.6</td>\n",
              "      <td>4.2</td>\n",
              "      <td>6.4</td>\n",
              "      <td>9.3</td>\n",
              "      <td>45.6</td>\n",
              "      <td>1.08</td>\n",
              "      <td>0.13</td>\n",
              "      <td>4.80</td>\n",
              "      <td>11.9</td>\n",
              "      <td>113.4</td>\n",
              "      <td>115.2</td>\n",
              "      <td>2598.08</td>\n",
              "      <td>97.050453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>462</th>\n",
              "      <td>Karl-Anthony Towns</td>\n",
              "      <td>Min</td>\n",
              "      <td>C-F</td>\n",
              "      <td>24.32</td>\n",
              "      <td>35</td>\n",
              "      <td>33.9</td>\n",
              "      <td>70.7</td>\n",
              "      <td>28.8</td>\n",
              "      <td>13.2</td>\n",
              "      <td>226</td>\n",
              "      <td>0.796</td>\n",
              "      <td>345</td>\n",
              "      <td>0.586</td>\n",
              "      <td>277</td>\n",
              "      <td>0.412</td>\n",
              "      <td>0.600</td>\n",
              "      <td>0.642</td>\n",
              "      <td>26.5</td>\n",
              "      <td>10.8</td>\n",
              "      <td>16.8</td>\n",
              "      <td>4.4</td>\n",
              "      <td>22.8</td>\n",
              "      <td>0.94</td>\n",
              "      <td>1.20</td>\n",
              "      <td>3.14</td>\n",
              "      <td>12.7</td>\n",
              "      <td>120.2</td>\n",
              "      <td>104.0</td>\n",
              "      <td>2396.73</td>\n",
              "      <td>96.728935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>439</th>\n",
              "      <td>Ben Simmons</td>\n",
              "      <td>Phi</td>\n",
              "      <td>F</td>\n",
              "      <td>23.64</td>\n",
              "      <td>54</td>\n",
              "      <td>35.8</td>\n",
              "      <td>74.5</td>\n",
              "      <td>20.9</td>\n",
              "      <td>20.5</td>\n",
              "      <td>284</td>\n",
              "      <td>0.627</td>\n",
              "      <td>612</td>\n",
              "      <td>0.587</td>\n",
              "      <td>6</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.586</td>\n",
              "      <td>0.607</td>\n",
              "      <td>16.7</td>\n",
              "      <td>7.9</td>\n",
              "      <td>12.1</td>\n",
              "      <td>8.2</td>\n",
              "      <td>34.8</td>\n",
              "      <td>2.15</td>\n",
              "      <td>0.59</td>\n",
              "      <td>3.56</td>\n",
              "      <td>11.5</td>\n",
              "      <td>115.8</td>\n",
              "      <td>100.3</td>\n",
              "      <td>2667.10</td>\n",
              "      <td>91.801369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>451</th>\n",
              "      <td>Jayson Tatum</td>\n",
              "      <td>Bos</td>\n",
              "      <td>F-G</td>\n",
              "      <td>22.03</td>\n",
              "      <td>59</td>\n",
              "      <td>34.6</td>\n",
              "      <td>72.1</td>\n",
              "      <td>28.6</td>\n",
              "      <td>9.6</td>\n",
              "      <td>279</td>\n",
              "      <td>0.806</td>\n",
              "      <td>694</td>\n",
              "      <td>0.478</td>\n",
              "      <td>420</td>\n",
              "      <td>0.398</td>\n",
              "      <td>0.523</td>\n",
              "      <td>0.562</td>\n",
              "      <td>23.6</td>\n",
              "      <td>7.1</td>\n",
              "      <td>11.0</td>\n",
              "      <td>2.9</td>\n",
              "      <td>14.0</td>\n",
              "      <td>1.42</td>\n",
              "      <td>0.90</td>\n",
              "      <td>2.22</td>\n",
              "      <td>9.1</td>\n",
              "      <td>110.3</td>\n",
              "      <td>103.3</td>\n",
              "      <td>2494.66</td>\n",
              "      <td>90.324974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Deandre Ayton</td>\n",
              "      <td>Pho</td>\n",
              "      <td>C</td>\n",
              "      <td>21.64</td>\n",
              "      <td>30</td>\n",
              "      <td>33.2</td>\n",
              "      <td>69.2</td>\n",
              "      <td>24.3</td>\n",
              "      <td>12.4</td>\n",
              "      <td>78</td>\n",
              "      <td>0.769</td>\n",
              "      <td>462</td>\n",
              "      <td>0.552</td>\n",
              "      <td>3</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.548</td>\n",
              "      <td>0.571</td>\n",
              "      <td>19.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>20.1</td>\n",
              "      <td>1.9</td>\n",
              "      <td>9.7</td>\n",
              "      <td>0.70</td>\n",
              "      <td>1.70</td>\n",
              "      <td>2.37</td>\n",
              "      <td>9.1</td>\n",
              "      <td>109.3</td>\n",
              "      <td>102.9</td>\n",
              "      <td>2297.44</td>\n",
              "      <td>90.134041</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400</th>\n",
              "      <td>Kristaps Porzingis</td>\n",
              "      <td>Dal</td>\n",
              "      <td>F-C</td>\n",
              "      <td>24.61</td>\n",
              "      <td>51</td>\n",
              "      <td>31.3</td>\n",
              "      <td>65.2</td>\n",
              "      <td>26.6</td>\n",
              "      <td>8.5</td>\n",
              "      <td>228</td>\n",
              "      <td>0.776</td>\n",
              "      <td>447</td>\n",
              "      <td>0.477</td>\n",
              "      <td>361</td>\n",
              "      <td>0.349</td>\n",
              "      <td>0.498</td>\n",
              "      <td>0.540</td>\n",
              "      <td>19.2</td>\n",
              "      <td>9.5</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.7</td>\n",
              "      <td>8.6</td>\n",
              "      <td>0.71</td>\n",
              "      <td>2.08</td>\n",
              "      <td>1.65</td>\n",
              "      <td>8.6</td>\n",
              "      <td>108.5</td>\n",
              "      <td>100.6</td>\n",
              "      <td>2040.76</td>\n",
              "      <td>89.887939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>Devin Booker</td>\n",
              "      <td>Pho</td>\n",
              "      <td>G</td>\n",
              "      <td>23.36</td>\n",
              "      <td>62</td>\n",
              "      <td>36.1</td>\n",
              "      <td>75.3</td>\n",
              "      <td>29.5</td>\n",
              "      <td>15.7</td>\n",
              "      <td>442</td>\n",
              "      <td>0.916</td>\n",
              "      <td>768</td>\n",
              "      <td>0.544</td>\n",
              "      <td>350</td>\n",
              "      <td>0.360</td>\n",
              "      <td>0.543</td>\n",
              "      <td>0.617</td>\n",
              "      <td>26.1</td>\n",
              "      <td>4.1</td>\n",
              "      <td>6.4</td>\n",
              "      <td>6.6</td>\n",
              "      <td>30.2</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.26</td>\n",
              "      <td>3.94</td>\n",
              "      <td>9.9</td>\n",
              "      <td>114.1</td>\n",
              "      <td>108.0</td>\n",
              "      <td>2718.33</td>\n",
              "      <td>89.662300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>John Collins</td>\n",
              "      <td>Atl</td>\n",
              "      <td>F</td>\n",
              "      <td>22.47</td>\n",
              "      <td>41</td>\n",
              "      <td>33.2</td>\n",
              "      <td>69.3</td>\n",
              "      <td>22.7</td>\n",
              "      <td>10.1</td>\n",
              "      <td>150</td>\n",
              "      <td>0.800</td>\n",
              "      <td>458</td>\n",
              "      <td>0.642</td>\n",
              "      <td>147</td>\n",
              "      <td>0.401</td>\n",
              "      <td>0.632</td>\n",
              "      <td>0.659</td>\n",
              "      <td>21.6</td>\n",
              "      <td>10.1</td>\n",
              "      <td>16.4</td>\n",
              "      <td>1.5</td>\n",
              "      <td>7.6</td>\n",
              "      <td>0.76</td>\n",
              "      <td>1.61</td>\n",
              "      <td>1.83</td>\n",
              "      <td>8.3</td>\n",
              "      <td>123.1</td>\n",
              "      <td>106.6</td>\n",
              "      <td>2300.76</td>\n",
              "      <td>89.294609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>233</th>\n",
              "      <td>Brandon Ingram</td>\n",
              "      <td>Nor</td>\n",
              "      <td>F</td>\n",
              "      <td>22.52</td>\n",
              "      <td>56</td>\n",
              "      <td>34.3</td>\n",
              "      <td>71.4</td>\n",
              "      <td>28.2</td>\n",
              "      <td>13.1</td>\n",
              "      <td>330</td>\n",
              "      <td>0.858</td>\n",
              "      <td>652</td>\n",
              "      <td>0.509</td>\n",
              "      <td>354</td>\n",
              "      <td>0.387</td>\n",
              "      <td>0.534</td>\n",
              "      <td>0.590</td>\n",
              "      <td>24.3</td>\n",
              "      <td>6.2</td>\n",
              "      <td>9.6</td>\n",
              "      <td>4.3</td>\n",
              "      <td>19.8</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.64</td>\n",
              "      <td>3.09</td>\n",
              "      <td>10.1</td>\n",
              "      <td>111.9</td>\n",
              "      <td>106.5</td>\n",
              "      <td>2449.02</td>\n",
              "      <td>89.239281</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   Name Team  Pos  ...   DRTG  Total Minutes     Ratings\n",
              "128         Luka Doncic  Dal  G-F  ...  102.9        2311.02  100.085464\n",
              "510          Trae Young  Atl    G  ...  115.2        2598.08   97.050453\n",
              "462  Karl-Anthony Towns  Min  C-F  ...  104.0        2396.73   96.728935\n",
              "439         Ben Simmons  Phi    F  ...  100.3        2667.10   91.801369\n",
              "451        Jayson Tatum  Bos  F-G  ...  103.3        2494.66   90.324974\n",
              "19        Deandre Ayton  Pho    C  ...  102.9        2297.44   90.134041\n",
              "400  Kristaps Porzingis  Dal  F-C  ...  100.6        2040.76   89.887939\n",
              "52         Devin Booker  Pho    G  ...  108.0        2718.33   89.662300\n",
              "103        John Collins  Atl    F  ...  106.6        2300.76   89.294609\n",
              "233      Brandon Ingram  Nor    F  ...  106.5        2449.02   89.239281\n",
              "\n",
              "[10 rows x 30 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BL24lKWmihH_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "young=young.loc[:,['Name','Ratings']].sort_values(by=['Ratings'],ascending=False)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdYQe4AcjFOu",
        "colab_type": "text"
      },
      "source": [
        "Our neural network determines Luka Doncic, last season's Rookie of the Year, to be the best young player in the NBA with Trae Young and Karl-Anthony Towns closing out the top 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2sJOTbai6GZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "dbb54152-8668-4810-8674-4fbaa7545519"
      },
      "source": [
        "young.loc[:,'Name'].head(10)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "128           Luka Doncic\n",
              "510            Trae Young\n",
              "462    Karl-Anthony Towns\n",
              "439           Ben Simmons\n",
              "451          Jayson Tatum\n",
              "19          Deandre Ayton\n",
              "400    Kristaps Porzingis\n",
              "52           Devin Booker\n",
              "103          John Collins\n",
              "233        Brandon Ingram\n",
              "Name: Name, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca-HVfkaj3L9",
        "colab_type": "text"
      },
      "source": [
        "We now go back to our regular dataset and continue our analysis. James Harden once again leads the league in PPG with 34.4 points per game. The league average is 8.97."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRwFzJzljqkA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "f2753e7f-a4b2-4cce-f64c-bd53bb5c94b0"
      },
      "source": [
        "sns.kdeplot(data=dftemp.PPG, shade=True)\n",
        "print(dftemp.PPG.mean())\n",
        "print(dftemp.PPG.max())\n",
        "print(dftemp[dftemp.PPG>=dftemp.PPG.max()].iloc[0,[0,1]])"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8.975024591537766\n",
            "34.4\n",
            "Name    James Harden\n",
            "Team             Hou\n",
            "Name: 191, dtype: object\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AtOsgZzkEBC",
        "colab_type": "text"
      },
      "source": [
        "Next, we see some basic stats on two pointers and three pointers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRKdvWMbkEev",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "c880cc1a-a558-4121-abee-665595025a1b"
      },
      "source": [
        "print(\"Total two pointers attempted:\")\n",
        "print(dftemp['2PA'].sum())\n",
        "df_temp=dftemp\n",
        "df_temp['2 pointers scored']=df_temp['2PA']*df_temp['2Ppercent']\n",
        "df_temp['3 pointers scored']=df_temp['3PA']*df_temp['3Ppercent']\n",
        "print(\"Total points through two pointers:\")\n",
        "print((df_temp['2 pointers scored'].sum())*2)\n",
        "print(\"Total three pointers attempted:\")\n",
        "print(dftemp['3PA'].sum())\n",
        "print(\"Total points through three pointers:\")\n",
        "print((df_temp['3 pointers scored'].sum())*3)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total two pointers attempted:\n",
            "104280\n",
            "Total points through two pointers:\n",
            "109031.68599999999\n",
            "Total three pointers attempted:\n",
            "64030\n",
            "Total points through three pointers:\n",
            "68730.85800000001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyUyPLwDkhDE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "09a96959-eb0e-47da-f453-4103f1f42e7a"
      },
      "source": [
        "print(\"Average two point accuracy:\")\n",
        "print(dftemp['2Ppercent'].mean())\n",
        "print(\"Average three point accuracy:\")\n",
        "print(dftemp['3Ppercent'].mean())\n",
        "print(\"Average points scored per two point attempt:\")\n",
        "print(dftemp['2Ppercent'].mean()*2)\n",
        "print(\"Average points scored per three point attempt:\")\n",
        "print(dftemp['3Ppercent'].mean()*3)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average two point accuracy:\n",
            "0.5041624999999998\n",
            "Average three point accuracy:\n",
            "0.3009729166666668\n",
            "Average points scored per two point attempt:\n",
            "1.0083249999999997\n",
            "Average points scored per three point attempt:\n",
            "0.9029187500000004\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_q-VpKbRkpnD",
        "colab_type": "text"
      },
      "source": [
        "The NBA is in the midst of a three point revolution. The number of three point shots attempted per game is far more than what it was only a decade ago. Still the averge points gained by a player per 2 point attemp still outweighs the avergae points per 3 point attempt because of the drop in accuracy. An accurate three point shooter can can create a big difference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnIhuYyNlXSg",
        "colab_type": "text"
      },
      "source": [
        "LeBron James leads the league in assists per game coming in at an impressive 10.6 assists with the mean at around 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_-RsgHmlLwK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "1a44507c-45c5-4138-cbb8-cedc8a7dd01a"
      },
      "source": [
        "sns.kdeplot(data=dftemp.APG, shade=True)\n",
        "print(dftemp.APG.mean())\n",
        "print(dftemp.APG.max())\n",
        "print(dftemp[dftemp.APG>=dftemp.APG.max()].iloc[0,[0,1]])"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.9782973161614785\n",
            "10.6\n",
            "Name    LeBron James\n",
            "Team             Lal\n",
            "Name: 243, dtype: object\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R75ynjH4li_u",
        "colab_type": "text"
      },
      "source": [
        "Ben Simmons of the 76ers leads the league in Steals Per Game."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJN0SBIuljU-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "2df15c30-b5af-40c8-ad3b-60f227853d97"
      },
      "source": [
        "sns.kdeplot(data=dftemp.SPG, shade=True)\n",
        "print(dftemp.SPG.mean())\n",
        "print(dftemp.SPG.max())\n",
        "print(dftemp[dftemp.SPG>=dftemp.SPG.max()].iloc[0,[0,1]])"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6276838175833436\n",
            "2.15\n",
            "Name    Ben Simmons\n",
            "Team            Phi\n",
            "Name: 439, dtype: object\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiWb1-R4l2Sj",
        "colab_type": "text"
      },
      "source": [
        "Hassan Whiteside of the Trailblazers recorded 3.07 blocks per game to lead the league in blocks per gae."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdUv3vZll2ei",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "822f9401-6847-4fcd-c347-a184bd8f45cc"
      },
      "source": [
        "sns.kdeplot(data=dftemp.BPG, shade=True)\n",
        "print(dftemp.BPG.mean())\n",
        "print(dftemp.BPG.max())\n",
        "print(dftemp[dftemp.BPG>=dftemp.BPG.max()].iloc[0,[0,1]])"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4089095543386162\n",
            "3.07\n",
            "Name    Hassan Whiteside\n",
            "Team                 Por\n",
            "Name: 494, dtype: object\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iC2rPN7memk",
        "colab_type": "text"
      },
      "source": [
        "Hassan Whiteside also led the league in rebounds per game, racking up 14.2 per game with a league average of 3.71."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5iUu5akmez7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "5fe64103-00c7-4ee7-fedb-3088aeef5627"
      },
      "source": [
        "sns.kdeplot(data=dftemp.RPG, shade=True)\n",
        "print(dftemp.RPG.mean())\n",
        "print(dftemp.RPG.max())\n",
        "print(dftemp[dftemp.RPG>=dftemp.RPG.max()].iloc[0,[0,1]])"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.712570165150989\n",
            "14.2\n",
            "Name    Hassan Whiteside\n",
            "Team                 Por\n",
            "Name: 494, dtype: object\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnswSZpymyJG",
        "colab_type": "text"
      },
      "source": [
        "George Hill specialised in 3 pointers as he achieved an accuracy of 0.48 while having attempts greater than the league's average."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZVUAc0PmyWG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "e3b665c6-d1f1-4512-e4b4-371d3afa073a"
      },
      "source": [
        "df_temp=dftemp[dftemp[\"3PA\"]>dftemp[\"3PA\"].mean()]\n",
        "sns.kdeplot(data=df_temp[\"3Ppercent\"], shade=True)\n",
        "print(df_temp[\"3Ppercent\"].mean())\n",
        "print(df_temp[\"3Ppercent\"].max())\n",
        "print(df_temp[df_temp[\"3Ppercent\"]>=df_temp[\"3Ppercent\"].max()].iloc[0,[0,1]])"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.3645263157894735\n",
            "0.48\n",
            "Name    George Hill\n",
            "Team            Mil\n",
            "Name: 212, dtype: object\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MT6mkJY9nE_Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "f4df50b2-60ff-40e2-ee5c-5b5af3d33ed6"
      },
      "source": [
        "df_temp.loc[:,['Name','Team','3Ppercent']].sort_values(by=['3Ppercent'],ascending=False).head(10)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Team</th>\n",
              "      <th>3Ppercent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>212</th>\n",
              "      <td>George Hill</td>\n",
              "      <td>Mil</td>\n",
              "      <td>0.480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114</th>\n",
              "      <td>Seth Curry</td>\n",
              "      <td>Dal</td>\n",
              "      <td>0.453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>408</th>\n",
              "      <td>JJ Redick</td>\n",
              "      <td>Nor</td>\n",
              "      <td>0.452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>413</th>\n",
              "      <td>Duncan Robinson</td>\n",
              "      <td>Mia</td>\n",
              "      <td>0.448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>318</th>\n",
              "      <td>Doug McDermott</td>\n",
              "      <td>Ind</td>\n",
              "      <td>0.445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>343</th>\n",
              "      <td>Marcus Morris Sr.</td>\n",
              "      <td>Nyk</td>\n",
              "      <td>0.439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>372</th>\n",
              "      <td>Kelly Olynyk</td>\n",
              "      <td>Mia</td>\n",
              "      <td>0.432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>217</th>\n",
              "      <td>Justin Holiday</td>\n",
              "      <td>Ind</td>\n",
              "      <td>0.424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>Davis Bertans</td>\n",
              "      <td>Was</td>\n",
              "      <td>0.424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>Nemanja Bjelica</td>\n",
              "      <td>Sac</td>\n",
              "      <td>0.422</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  Name Team  3Ppercent\n",
              "212        George Hill  Mil      0.480\n",
              "114         Seth Curry  Dal      0.453\n",
              "408          JJ Redick  Nor      0.452\n",
              "413    Duncan Robinson  Mia      0.448\n",
              "318     Doug McDermott  Ind      0.445\n",
              "343  Marcus Morris Sr.  Nyk      0.439\n",
              "372       Kelly Olynyk  Mia      0.432\n",
              "217     Justin Holiday  Ind      0.424\n",
              "39       Davis Bertans  Was      0.424\n",
              "44     Nemanja Bjelica  Sac      0.422"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFBRFZUTnS2s",
        "colab_type": "text"
      },
      "source": [
        "Determining which player was fouled the most."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzHvq2lXnIFd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "7da4573d-a9be-4707-bc40-96276cf27218"
      },
      "source": [
        "dftemp.loc[:,['Name','Team','FTA']].sort_values(by=['FTA'],ascending=False).head(10)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Team</th>\n",
              "      <th>FTA</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>191</th>\n",
              "      <td>James Harden</td>\n",
              "      <td>Hou</td>\n",
              "      <td>719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Giannis Antetokounmpo</td>\n",
              "      <td>Mil</td>\n",
              "      <td>570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>510</th>\n",
              "      <td>Trae Young</td>\n",
              "      <td>Atl</td>\n",
              "      <td>559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <td>Luka Doncic</td>\n",
              "      <td>Dal</td>\n",
              "      <td>491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>Jimmy Butler</td>\n",
              "      <td>Mia</td>\n",
              "      <td>490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>Bradley Beal</td>\n",
              "      <td>Was</td>\n",
              "      <td>458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>Anthony Davis</td>\n",
              "      <td>Lal</td>\n",
              "      <td>457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>Spencer Dinwiddie</td>\n",
              "      <td>Bro</td>\n",
              "      <td>446</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>Devin Booker</td>\n",
              "      <td>Pho</td>\n",
              "      <td>442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>290</th>\n",
              "      <td>Damian Lillard</td>\n",
              "      <td>Por</td>\n",
              "      <td>438</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      Name Team  FTA\n",
              "191           James Harden  Hou  719\n",
              "11   Giannis Antetokounmpo  Mil  570\n",
              "510             Trae Young  Atl  559\n",
              "128            Luka Doncic  Dal  491\n",
              "78            Jimmy Butler  Mia  490\n",
              "33            Bradley Beal  Was  458\n",
              "117          Anthony Davis  Lal  457\n",
              "126      Spencer Dinwiddie  Bro  446\n",
              "52            Devin Booker  Pho  442\n",
              "290         Damian Lillard  Por  438"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IJ61tCRnapH",
        "colab_type": "text"
      },
      "source": [
        "James Harden seems to be leading the pack by a considerable margin. The following chart sums it up better though."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMOvJvk-nbcI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "b1054baf-ace3-43a5-abe5-770daac723c8"
      },
      "source": [
        "ax1 = dftemp.plot.scatter(x='FTA',\n",
        "                      y='FTpercent',\n",
        "                      c='DarkBlue')"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QK0If8yfnj_h",
        "colab_type": "text"
      },
      "source": [
        "James Harden appears all the way to the end, far ahead of the pack, apparently drawing 149 more fouls than second placed Giannis Antetokounmpo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtifp1N4nqyL",
        "colab_type": "text"
      },
      "source": [
        "Graphing three point attempts by players"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eB2I2XmXng66",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "ab58d412-0fc4-48eb-e746-5af10521165a"
      },
      "source": [
        "dftemp.loc[:,['Name','Team','3PA']].sort_values(by=['3PA'],ascending=False).head(10)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Team</th>\n",
              "      <th>3PA</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>191</th>\n",
              "      <td>James Harden</td>\n",
              "      <td>Hou</td>\n",
              "      <td>769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>211</th>\n",
              "      <td>Buddy Hield</td>\n",
              "      <td>Sac</td>\n",
              "      <td>618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>Devonte' Graham</td>\n",
              "      <td>Cha</td>\n",
              "      <td>585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>290</th>\n",
              "      <td>Damian Lillard</td>\n",
              "      <td>Por</td>\n",
              "      <td>573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>510</th>\n",
              "      <td>Trae Young</td>\n",
              "      <td>Atl</td>\n",
              "      <td>568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>413</th>\n",
              "      <td>Duncan Robinson</td>\n",
              "      <td>Mia</td>\n",
              "      <td>543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <td>Luka Doncic</td>\n",
              "      <td>Dal</td>\n",
              "      <td>491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>279</th>\n",
              "      <td>Zach LaVine</td>\n",
              "      <td>Chi</td>\n",
              "      <td>484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>Bradley Beal</td>\n",
              "      <td>Was</td>\n",
              "      <td>481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>Davis Bertans</td>\n",
              "      <td>Was</td>\n",
              "      <td>472</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Name Team  3PA\n",
              "191     James Harden  Hou  769\n",
              "211      Buddy Hield  Sac  618\n",
              "174  Devonte' Graham  Cha  585\n",
              "290   Damian Lillard  Por  573\n",
              "510       Trae Young  Atl  568\n",
              "413  Duncan Robinson  Mia  543\n",
              "128      Luka Doncic  Dal  491\n",
              "279      Zach LaVine  Chi  484\n",
              "33      Bradley Beal  Was  481\n",
              "39     Davis Bertans  Was  472"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnhMOcifnycj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "6db7ce1f-003b-45a1-e54d-22d5cc3d012f"
      },
      "source": [
        "ax1 = dftemp.plot.scatter(x='3PA',\n",
        "                      y='3Ppercent',\n",
        "                      c='DarkBlue')"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LU_IeIjrn3ok",
        "colab_type": "text"
      },
      "source": [
        "James Harden, here too takes far more 3 pointers than any other player but doesn't boast of a similar stat in terms of accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Nu8U9k5oDZe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "46cb291f-1903-46d6-ab86-3fc95993c496"
      },
      "source": [
        "temp=dftemp\n",
        "temp['Minutes']=temp['GP']*temp['MPG']\n",
        "temp=temp[temp['Minutes']>temp.Minutes.mean()]\n",
        "temp.loc[:,['Name','Team','True shooting percent']].sort_values(by=['True shooting percent'],ascending=False).head(10)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Team</th>\n",
              "      <th>True shooting percent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>416</th>\n",
              "      <td>Mitchell Robinson</td>\n",
              "      <td>Nyk</td>\n",
              "      <td>0.726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>170</th>\n",
              "      <td>Rudy Gobert</td>\n",
              "      <td>Uta</td>\n",
              "      <td>0.700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>224</th>\n",
              "      <td>Dwight Howard</td>\n",
              "      <td>Lal</td>\n",
              "      <td>0.696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>219</th>\n",
              "      <td>Richaun Holmes</td>\n",
              "      <td>Sac</td>\n",
              "      <td>0.683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>258</th>\n",
              "      <td>DeAndre Jordan</td>\n",
              "      <td>Bro</td>\n",
              "      <td>0.682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>413</th>\n",
              "      <td>Duncan Robinson</td>\n",
              "      <td>Mia</td>\n",
              "      <td>0.678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>401</th>\n",
              "      <td>Dwight Powell</td>\n",
              "      <td>Dal</td>\n",
              "      <td>0.677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>Brandon Clarke</td>\n",
              "      <td>Mem</td>\n",
              "      <td>0.673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>212</th>\n",
              "      <td>George Hill</td>\n",
              "      <td>Mil</td>\n",
              "      <td>0.668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>John Collins</td>\n",
              "      <td>Atl</td>\n",
              "      <td>0.659</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  Name Team  True shooting percent\n",
              "416  Mitchell Robinson  Nyk                  0.726\n",
              "170        Rudy Gobert  Uta                  0.700\n",
              "224      Dwight Howard  Lal                  0.696\n",
              "219     Richaun Holmes  Sac                  0.683\n",
              "258     DeAndre Jordan  Bro                  0.682\n",
              "413    Duncan Robinson  Mia                  0.678\n",
              "401      Dwight Powell  Dal                  0.677\n",
              "96      Brandon Clarke  Mem                  0.673\n",
              "212        George Hill  Mil                  0.668\n",
              "103       John Collins  Atl                  0.659"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gODZvrGtoHr3",
        "colab_type": "text"
      },
      "source": [
        "Finally, we proceed towards our main objective, using the trained neural network to determine the MVP of the league."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDs3tVF9ICdc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df3n = mm_scaler.fit_transform(df3.loc[:,cols].astype(float))\n",
        "\n",
        "df3n = pd.DataFrame(df3n, columns=cols)"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XY6R48xR9meO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "a78459f8-6c31-4be2-a0f0-6cf04403d7c9"
      },
      "source": [
        "df3n.head()"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PPG</th>\n",
              "      <th>TOPGTurnovers</th>\n",
              "      <th>FTA</th>\n",
              "      <th>MPG</th>\n",
              "      <th>2PA</th>\n",
              "      <th>USG</th>\n",
              "      <th>Versatility Index</th>\n",
              "      <th>APG</th>\n",
              "      <th>RPG</th>\n",
              "      <th>SPG</th>\n",
              "      <th>3PA</th>\n",
              "      <th>BPG</th>\n",
              "      <th>GP</th>\n",
              "      <th>True shooting percent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.316860</td>\n",
              "      <td>0.312500</td>\n",
              "      <td>0.254520</td>\n",
              "      <td>0.714697</td>\n",
              "      <td>0.439759</td>\n",
              "      <td>0.356688</td>\n",
              "      <td>0.540698</td>\n",
              "      <td>0.226415</td>\n",
              "      <td>0.661972</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.003901</td>\n",
              "      <td>0.364821</td>\n",
              "      <td>0.407143</td>\n",
              "      <td>0.779639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.470930</td>\n",
              "      <td>0.589583</td>\n",
              "      <td>0.475661</td>\n",
              "      <td>0.927954</td>\n",
              "      <td>0.707831</td>\n",
              "      <td>0.471338</td>\n",
              "      <td>0.645349</td>\n",
              "      <td>0.481132</td>\n",
              "      <td>0.739437</td>\n",
              "      <td>0.558140</td>\n",
              "      <td>0.016905</td>\n",
              "      <td>0.426710</td>\n",
              "      <td>0.457143</td>\n",
              "      <td>0.779639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.549419</td>\n",
              "      <td>0.291667</td>\n",
              "      <td>0.265647</td>\n",
              "      <td>0.890490</td>\n",
              "      <td>0.637550</td>\n",
              "      <td>0.560510</td>\n",
              "      <td>0.488372</td>\n",
              "      <td>0.226415</td>\n",
              "      <td>0.521127</td>\n",
              "      <td>0.316279</td>\n",
              "      <td>0.204161</td>\n",
              "      <td>0.534202</td>\n",
              "      <td>0.371429</td>\n",
              "      <td>0.734536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.148256</td>\n",
              "      <td>0.204167</td>\n",
              "      <td>0.038943</td>\n",
              "      <td>0.288184</td>\n",
              "      <td>0.108434</td>\n",
              "      <td>0.535032</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.169811</td>\n",
              "      <td>0.140845</td>\n",
              "      <td>0.125581</td>\n",
              "      <td>0.152146</td>\n",
              "      <td>0.055375</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.568299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.215116</td>\n",
              "      <td>0.160417</td>\n",
              "      <td>0.048679</td>\n",
              "      <td>0.414986</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.378981</td>\n",
              "      <td>0.395349</td>\n",
              "      <td>0.132075</td>\n",
              "      <td>0.154930</td>\n",
              "      <td>0.106977</td>\n",
              "      <td>0.118336</td>\n",
              "      <td>0.009772</td>\n",
              "      <td>0.207143</td>\n",
              "      <td>0.743557</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        PPG  TOPGTurnovers       FTA  ...       BPG        GP  True shooting percent\n",
              "0  0.316860       0.312500  0.254520  ...  0.364821  0.407143               0.779639\n",
              "1  0.470930       0.589583  0.475661  ...  0.426710  0.457143               0.779639\n",
              "2  0.549419       0.291667  0.265647  ...  0.534202  0.371429               0.734536\n",
              "3  0.148256       0.204167  0.038943  ...  0.055375  0.285714               0.568299\n",
              "4  0.215116       0.160417  0.048679  ...  0.009772  0.207143               0.743557\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20EJ7HvoDUnm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "res=NN_model.predict(df3n.loc[:,cols])"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVKHTCMEFAdR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df3['Ratings']=res"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szMFhNALGhtk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dftemp=df3"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rJTIHhAGj-Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "81137d5b-9d52-4628-f7ee-6a6d7feade14"
      },
      "source": [
        "dftemp.sort_values(by=['Ratings'],ascending=False).head()"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Team</th>\n",
              "      <th>Pos</th>\n",
              "      <th>Age</th>\n",
              "      <th>GP</th>\n",
              "      <th>MPG</th>\n",
              "      <th>Minpercent</th>\n",
              "      <th>USG</th>\n",
              "      <th>TOR</th>\n",
              "      <th>FTA</th>\n",
              "      <th>FTpercent</th>\n",
              "      <th>2PA</th>\n",
              "      <th>2Ppercent</th>\n",
              "      <th>3PA</th>\n",
              "      <th>3Ppercent</th>\n",
              "      <th>Effective Shooting percent</th>\n",
              "      <th>True shooting percent</th>\n",
              "      <th>PPG</th>\n",
              "      <th>RPG</th>\n",
              "      <th>TRB</th>\n",
              "      <th>APG</th>\n",
              "      <th>ASTpercent</th>\n",
              "      <th>SPG</th>\n",
              "      <th>BPG</th>\n",
              "      <th>TOPGTurnovers</th>\n",
              "      <th>Versatility Index</th>\n",
              "      <th>ORTG</th>\n",
              "      <th>DRTG</th>\n",
              "      <th>2 pointers scored</th>\n",
              "      <th>3 pointers scored</th>\n",
              "      <th>Minutes</th>\n",
              "      <th>Ratings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>191</th>\n",
              "      <td>James Harden</td>\n",
              "      <td>Hou</td>\n",
              "      <td>G</td>\n",
              "      <td>30.54</td>\n",
              "      <td>61</td>\n",
              "      <td>36.7</td>\n",
              "      <td>76.5</td>\n",
              "      <td>36.4</td>\n",
              "      <td>13.7</td>\n",
              "      <td>719</td>\n",
              "      <td>0.861</td>\n",
              "      <td>617</td>\n",
              "      <td>0.538</td>\n",
              "      <td>769</td>\n",
              "      <td>0.352</td>\n",
              "      <td>0.533</td>\n",
              "      <td>0.616</td>\n",
              "      <td>34.4</td>\n",
              "      <td>6.3</td>\n",
              "      <td>9.1</td>\n",
              "      <td>7.4</td>\n",
              "      <td>34.4</td>\n",
              "      <td>1.74</td>\n",
              "      <td>0.87</td>\n",
              "      <td>4.43</td>\n",
              "      <td>12.8</td>\n",
              "      <td>119.7</td>\n",
              "      <td>103.6</td>\n",
              "      <td>331.946</td>\n",
              "      <td>270.688</td>\n",
              "      <td>2238.7</td>\n",
              "      <td>98.981972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <td>Luka Doncic</td>\n",
              "      <td>Dal</td>\n",
              "      <td>G-F</td>\n",
              "      <td>21.03</td>\n",
              "      <td>54</td>\n",
              "      <td>33.3</td>\n",
              "      <td>69.4</td>\n",
              "      <td>37.0</td>\n",
              "      <td>14.7</td>\n",
              "      <td>491</td>\n",
              "      <td>0.752</td>\n",
              "      <td>619</td>\n",
              "      <td>0.575</td>\n",
              "      <td>491</td>\n",
              "      <td>0.318</td>\n",
              "      <td>0.532</td>\n",
              "      <td>0.584</td>\n",
              "      <td>28.7</td>\n",
              "      <td>9.3</td>\n",
              "      <td>14.6</td>\n",
              "      <td>8.7</td>\n",
              "      <td>45.3</td>\n",
              "      <td>1.06</td>\n",
              "      <td>0.19</td>\n",
              "      <td>4.22</td>\n",
              "      <td>15.9</td>\n",
              "      <td>115.6</td>\n",
              "      <td>102.9</td>\n",
              "      <td>355.925</td>\n",
              "      <td>156.138</td>\n",
              "      <td>1798.2</td>\n",
              "      <td>96.427612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Giannis Antetokounmpo</td>\n",
              "      <td>Mil</td>\n",
              "      <td>F</td>\n",
              "      <td>25.26</td>\n",
              "      <td>57</td>\n",
              "      <td>30.9</td>\n",
              "      <td>64.4</td>\n",
              "      <td>37.4</td>\n",
              "      <td>13.1</td>\n",
              "      <td>570</td>\n",
              "      <td>0.633</td>\n",
              "      <td>868</td>\n",
              "      <td>0.622</td>\n",
              "      <td>271</td>\n",
              "      <td>0.306</td>\n",
              "      <td>0.583</td>\n",
              "      <td>0.608</td>\n",
              "      <td>29.6</td>\n",
              "      <td>13.7</td>\n",
              "      <td>21.9</td>\n",
              "      <td>5.8</td>\n",
              "      <td>33.9</td>\n",
              "      <td>1.05</td>\n",
              "      <td>1.02</td>\n",
              "      <td>3.67</td>\n",
              "      <td>17.2</td>\n",
              "      <td>115.8</td>\n",
              "      <td>89.7</td>\n",
              "      <td>539.896</td>\n",
              "      <td>82.926</td>\n",
              "      <td>1761.3</td>\n",
              "      <td>95.557617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>243</th>\n",
              "      <td>LeBron James</td>\n",
              "      <td>Lal</td>\n",
              "      <td>F</td>\n",
              "      <td>35.20</td>\n",
              "      <td>60</td>\n",
              "      <td>34.9</td>\n",
              "      <td>72.7</td>\n",
              "      <td>31.6</td>\n",
              "      <td>15.3</td>\n",
              "      <td>343</td>\n",
              "      <td>0.697</td>\n",
              "      <td>795</td>\n",
              "      <td>0.570</td>\n",
              "      <td>381</td>\n",
              "      <td>0.349</td>\n",
              "      <td>0.555</td>\n",
              "      <td>0.582</td>\n",
              "      <td>25.7</td>\n",
              "      <td>7.9</td>\n",
              "      <td>12.2</td>\n",
              "      <td>10.6</td>\n",
              "      <td>49.7</td>\n",
              "      <td>1.23</td>\n",
              "      <td>0.50</td>\n",
              "      <td>3.98</td>\n",
              "      <td>14.8</td>\n",
              "      <td>116.3</td>\n",
              "      <td>103.2</td>\n",
              "      <td>453.150</td>\n",
              "      <td>132.969</td>\n",
              "      <td>2094.0</td>\n",
              "      <td>95.487732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>234</th>\n",
              "      <td>Kyrie Irving</td>\n",
              "      <td>Bro</td>\n",
              "      <td>G</td>\n",
              "      <td>27.97</td>\n",
              "      <td>20</td>\n",
              "      <td>32.9</td>\n",
              "      <td>68.5</td>\n",
              "      <td>32.6</td>\n",
              "      <td>10.1</td>\n",
              "      <td>102</td>\n",
              "      <td>0.922</td>\n",
              "      <td>274</td>\n",
              "      <td>0.522</td>\n",
              "      <td>142</td>\n",
              "      <td>0.394</td>\n",
              "      <td>0.546</td>\n",
              "      <td>0.595</td>\n",
              "      <td>27.4</td>\n",
              "      <td>5.2</td>\n",
              "      <td>8.1</td>\n",
              "      <td>6.4</td>\n",
              "      <td>37.4</td>\n",
              "      <td>1.35</td>\n",
              "      <td>0.50</td>\n",
              "      <td>2.60</td>\n",
              "      <td>11.8</td>\n",
              "      <td>118.7</td>\n",
              "      <td>103.3</td>\n",
              "      <td>143.028</td>\n",
              "      <td>55.948</td>\n",
              "      <td>658.0</td>\n",
              "      <td>94.666824</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      Name Team  Pos  ...  3 pointers scored  Minutes    Ratings\n",
              "191           James Harden  Hou    G  ...            270.688   2238.7  98.981972\n",
              "128            Luka Doncic  Dal  G-F  ...            156.138   1798.2  96.427612\n",
              "11   Giannis Antetokounmpo  Mil    F  ...             82.926   1761.3  95.557617\n",
              "243           LeBron James  Lal    F  ...            132.969   2094.0  95.487732\n",
              "234           Kyrie Irving  Bro    G  ...             55.948    658.0  94.666824\n",
              "\n",
              "[5 rows x 32 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgtrD1JvFxVL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df3=df3.loc[:,['Name','Ratings']].sort_values(by=['Ratings'],ascending=False)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Hc1QaxKK2tH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "outputId": "6240bb7c-1a25-4644-e066-5247d1262630"
      },
      "source": [
        "df3.head(20)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Ratings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>191</th>\n",
              "      <td>James Harden</td>\n",
              "      <td>98.981972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <td>Luka Doncic</td>\n",
              "      <td>96.427612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Giannis Antetokounmpo</td>\n",
              "      <td>95.557617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>243</th>\n",
              "      <td>LeBron James</td>\n",
              "      <td>95.487732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>234</th>\n",
              "      <td>Kyrie Irving</td>\n",
              "      <td>94.666824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>462</th>\n",
              "      <td>Karl-Anthony Towns</td>\n",
              "      <td>93.816978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>290</th>\n",
              "      <td>Damian Lillard</td>\n",
              "      <td>93.589333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>510</th>\n",
              "      <td>Trae Young</td>\n",
              "      <td>93.580475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>Stephen Curry</td>\n",
              "      <td>92.677101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>491</th>\n",
              "      <td>Russell Westbrook</td>\n",
              "      <td>92.462212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>Bradley Beal</td>\n",
              "      <td>91.740059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>Anthony Davis</td>\n",
              "      <td>91.687462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>287</th>\n",
              "      <td>Kawhi Leonard</td>\n",
              "      <td>91.326164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>296</th>\n",
              "      <td>Kyle Lowry</td>\n",
              "      <td>90.416321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>Joel Embiid</td>\n",
              "      <td>90.352509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>439</th>\n",
              "      <td>Ben Simmons</td>\n",
              "      <td>89.584717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>254</th>\n",
              "      <td>Nikola Jokic</td>\n",
              "      <td>89.093956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>Jimmy Butler</td>\n",
              "      <td>88.569817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>279</th>\n",
              "      <td>Zach LaVine</td>\n",
              "      <td>88.538887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>216</th>\n",
              "      <td>Jrue Holiday</td>\n",
              "      <td>88.457108</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      Name    Ratings\n",
              "191           James Harden  98.981972\n",
              "128            Luka Doncic  96.427612\n",
              "11   Giannis Antetokounmpo  95.557617\n",
              "243           LeBron James  95.487732\n",
              "234           Kyrie Irving  94.666824\n",
              "462     Karl-Anthony Towns  93.816978\n",
              "290         Damian Lillard  93.589333\n",
              "510             Trae Young  93.580475\n",
              "115          Stephen Curry  92.677101\n",
              "491      Russell Westbrook  92.462212\n",
              "33            Bradley Beal  91.740059\n",
              "117          Anthony Davis  91.687462\n",
              "287          Kawhi Leonard  91.326164\n",
              "296             Kyle Lowry  90.416321\n",
              "140            Joel Embiid  90.352509\n",
              "439            Ben Simmons  89.584717\n",
              "254           Nikola Jokic  89.093956\n",
              "78            Jimmy Butler  88.569817\n",
              "279            Zach LaVine  88.538887\n",
              "216           Jrue Holiday  88.457108"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVuMUyHwo1py",
        "colab_type": "text"
      },
      "source": [
        "Finally!\n",
        "James Harden, Luka Doncic and Giannis Antetokounmpo are the players that the neural network determined to be the 'best' in the league."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlQ2oQiCKVGE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3fea99fb-6baa-4fff-83f4-49db66a22099"
      },
      "source": [
        "# df3.to_csv('nbares.csv')\n",
        "# !cp nbares.csv \"drive/My Drive/\""
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: cannot create regular file 'drive/My Drive/': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}